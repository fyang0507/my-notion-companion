{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514f47d6-1811-45bc-ba95-e2c9d6ac99f3",
   "metadata": {},
   "source": [
    "# Demo: using NotionChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687426f4-14b4-4d40-97f9-447ac9baf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd608f1-a66a-4f6e-987a-1224c63dd397",
   "metadata": {},
   "source": [
    "load `.env` to environment variables, used for LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "078e0401-7050-4f98-b60c-d2bb2d330a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b886e533-157c-493f-9871-9c0599bb4bc2",
   "metadata": {},
   "source": [
    "Load configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ebdc5df-377d-4463-943e-218bd6402c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a828a95-041d-46e6-a874-2c7818be50fd",
   "metadata": {},
   "source": [
    "initialize llm and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6ba61d-e198-4ccb-af9d-0ce45515b488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/fred/Documents/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.06 MiB, ( 4095.12 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4095.05 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 7168\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   896.00 MiB, ( 4992.69 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =   104.05 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1976.02 MiB, ( 6968.70 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  1976.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   314.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 4\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'huggingfaceh4_zephyr-7b-beta'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+_CONFIGS['model_mapping'][_CONFIGS['model_name']],\n",
    "    name=_CONFIGS['model_name'], \n",
    "    **_CONFIGS['llm']\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _CONFIGS['model_name'], \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b98722-e8ac-4449-85b3-e9e27fde6fa1",
   "metadata": {},
   "source": [
    "initialize NotionChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0861913d-350e-4b76-a391-0249866dd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-13 16:37:04.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.utils\u001b[0m:\u001b[36mload_notion_documents\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mLoad data from existing offline copy.\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:04.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_metadata_filter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m18\u001b[0m - \u001b[1mSetting metadata fuzzy match threshold to: 0.8.\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:04.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.query_analyzer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mInitialize Query Analyzer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from my_notion_companion.notion_chatbot import NotionChatBot\n",
    "\n",
    "c = NotionChatBot(llm, tokenizer, '../.config.toml', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3529305-58a2-4b8f-8c12-6af49d789834",
   "metadata": {},
   "source": [
    "Invoke the chatbot. The first query will be used to search in lexical and semantic databases to retrieve relevant documents. The chatbot will then answer the question based on the retrieved docuemtns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05185a43-aab9-4889-9c17-36ed836ec3fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-13 16:37:04.857\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mTry lexical search.\u001b[0m\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       2.31 ms /    24 runs   (    0.10 ms per token, 10376.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8863.30 ms /   759 tokens (   11.68 ms per token,    85.63 tokens per second)\n",
      "llama_print_timings:        eval time =     809.82 ms /    23 runs   (   35.21 ms per token,    28.40 tokens per second)\n",
      "llama_print_timings:       total time =    9716.46 ms /   782 tokens\n",
      "\u001b[32m2024-03-13 16:37:14.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.query_analyzer\u001b[0m:\u001b[36mclean_output\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mQuery Analyzer output: 关键词：谁曾在步行者队效力|搜索范围：写作\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:14.755\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.query_analyzer\u001b[0m:\u001b[36mparse_output\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m\n",
      "Query Analyzer output\n",
      "keyword: ['谁曾在步行者队效力']\n",
      "search domains:['写作']\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:14.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.retriever\u001b[0m:\u001b[36m_filter_documents\u001b[0m:\u001b[36m110\u001b[0m - \u001b[1mfilter found by query analyzer: ['写作']\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:15.499\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_metadata_filter\u001b[0m:\u001b[36mfilter_multiple_criteria\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1mRemaining doc:  0.306\u001b[0m\n",
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/4m/z0fvcltx31xcv13t79qsnb8r0000gn/T/jieba.cache\n",
      "Loading model cost 0.347 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "\u001b[32m2024-03-13 16:37:16.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1m1 docs found via lexical search. Try semantic search.\u001b[0m\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "Metadata key author not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key author not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "\u001b[32m2024-03-13 16:37:44.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1m4 docs found via semantic search. Use LLM to check relevance.\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       0.47 ms /     4 runs   (    0.12 ms per token,  8492.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3623.58 ms /   769 tokens (    4.71 ms per token,   212.22 tokens per second)\n",
      "llama_print_timings:        eval time =     114.52 ms /     3 runs   (   38.17 ms per token,    26.20 tokens per second)\n",
      "llama_print_timings:       total time =    3749.07 ms /   772 tokens\n",
      "\u001b[32m2024-03-13 16:37:48.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:817e455c4d4446fbb35e5395b333db76', 'author': None, 'name': '2017-MAY-01 巡礼之年 - 瑞士', 'source': '写作', 'tags': '日常记趣, 游记', 'date_start': '20170501', 'date_end': '20170501'}\n",
      "\n",
      "（原本是 北京 出版集团某个旅行图书编辑的约稿，不过后来也不...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:48.064\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     4 runs   (    0.11 ms per token,  9345.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4805.15 ms /  1116 tokens (    4.31 ms per token,   232.25 tokens per second)\n",
      "llama_print_timings:        eval time =     122.08 ms /     3 runs   (   40.69 ms per token,    24.58 tokens per second)\n",
      "llama_print_timings:       total time =    4936.24 ms /  1119 tokens\n",
      "\u001b[32m2024-03-13 16:37:53.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:4c708d0ceaa749a383e20d675c465729', 'author': '【美】约瑟夫·海勒', 'name': '第二十二条军规 【美】约瑟夫·海勒', 'source': '读书笔记（文学）', 'tags': '小说', 'date_start': '20140204', 'date_end': None}\n",
      "\n",
      "“什么时候，长官？”\n",
      "“我在问你。你回答。”\n",
      "“是，长官。我...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:53.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       6.69 ms /    64 runs   (    0.10 ms per token,  9569.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4213.04 ms /   964 tokens (    4.37 ms per token,   228.81 tokens per second)\n",
      "llama_print_timings:        eval time =    2434.50 ms /    63 runs   (   38.64 ms per token,    25.88 tokens per second)\n",
      "llama_print_timings:       total time =    6753.81 ms /  1027 tokens\n",
      "\u001b[32m2024-03-13 16:37:59.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:491947c812c741ba94795561f0d89aba', 'author': None, 'name': '嘘之默然新纪元版3.33：来自新世界', 'source': '写作', 'tags': '嘘之默然', 'date_start': '20150626', 'date_end': '20150708'}\n",
      "\n",
      "我并非无端举出这两种人的故事。我希望，从这两种对待生活的态度...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-13 16:37:59.765\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mconclusion: 相关，因为讨论了两种人的态度和行为，并且提供了旅行和计划的比喻，可以帮助我们学习如何生存和处理生活中的困难和变化。\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       0.40 ms /     4 runs   (    0.10 ms per token,  9900.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2435.31 ms /   575 tokens (    4.24 ms per token,   236.11 tokens per second)\n",
      "llama_print_timings:        eval time =     118.88 ms /     3 runs   (   39.63 ms per token,    25.23 tokens per second)\n",
      "llama_print_timings:       total time =    2561.21 ms /   578 tokens\n",
      "\u001b[32m2024-03-13 16:38:02.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:3a1eb995135e41d792e321e392f71ba6', 'author': '【日】三岛由纪夫', 'name': '假面自白 【日】三岛由纪夫', 'source': '读书笔记（文学）', 'tags': '小说', 'date_start': '20160324', 'date_end': None}\n",
      "\n",
      "这时候，不知家里的大人是否直感到乍看是像往常一样迂回游行的这...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-13 16:38:02.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "\u001b[32m2024-03-13 16:38:02.332\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mRetrieved relevant docs:\n",
      "\n",
      "{'name': '嘘之默然♯♯♯致死的疾病，然后...', 'tags': '嘘之默然', 'id': '62dd822e-1014-4404-bfb8-668d55f1ca32', 'source': '写作', 'date_start': 20200822, 'date_end': 20201208}\n",
      "\n",
      "大卫·韦斯特是个令人敬佩的竞争者，同时也是，作为詹姆斯球队曾...\n",
      "------------------------------\n",
      "{'id': 'doc:notiondb:491947c812c741ba94795561f0d89aba', 'author': None, 'name': '嘘之默然新纪元版3.33：来自新世界', 'source': '写作', 'tags': '嘘之默然', 'date_start': '20150626', 'date_end': '20150708'}\n",
      "\n",
      "我并非无端举出这两种人的故事。我希望，从这两种对待生活的态度...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-13 16:38:02.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m81\u001b[0m - \u001b[1mInitialize Conversational RAG.\u001b[0m\n",
      "\u001b[32m2024-03-13 16:38:02.333\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mSending query to conversatonal RAG: 谁曾在步行者队效力？从“写作”中找答案。\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       8.39 ms /    94 runs   (    0.09 ms per token, 11202.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8610.23 ms /  1937 tokens (    4.45 ms per token,   224.96 tokens per second)\n",
      "llama_print_timings:        eval time =    3727.00 ms /    93 runs   (   40.08 ms per token,    24.95 tokens per second)\n",
      "llama_print_timings:       total time =   12507.90 ms /  2030 tokens\n",
      "\u001b[32m2024-03-13 16:38:14.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mReceived response: 文档1 提到了大卫·韦斯特曾经在步行者队效力，他以一手16尺的毫无破绽的中投、强硬的挡拆和护框、加之敏锐的球场嗅觉、同时还有高圆圆在东决给热火内线造成了巨大打击。\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'文档1 提到了大卫·韦斯特曾经在步行者队效力，他以一手16尺的毫无破绽的中投、强硬的挡拆和护框、加之敏锐的球场嗅觉、同时还有高圆圆在东决给热火内线造成了巨大打击。'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"谁曾在步行者队效力？从“写作”中找答案。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9afc0-0f75-4403-a076-5d4004eef957",
   "metadata": {},
   "source": [
    "The chatbot will have memory so we can have follow up conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb796c6-963d-4d81-8d9f-dda803efd9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-13 16:38:14.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mSending query to conversatonal RAG: 文档中有多少段落来自“写作”？\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       3.94 ms /    44 runs   (    0.09 ms per token, 11161.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     222.82 ms /    32 tokens (    6.96 ms per token,   143.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1743.22 ms /    43 runs   (   40.54 ms per token,    24.67 tokens per second)\n",
      "llama_print_timings:       total time =    2042.67 ms /    75 tokens\n",
      "\u001b[32m2024-03-13 16:38:16.973\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mReceived response: 只有文档1 中有来自 \"写作\" 的段落，所以不能确定整个资料库中有多少段落来自 \"写作\"。\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'只有文档1 中有来自 \"写作\" 的段落，所以不能确定整个资料库中有多少段落来自 \"写作\"。'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"文档中有多少段落来自“写作”？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c36dc0-e462-4e20-830f-2bf146a024f4",
   "metadata": {},
   "source": [
    "The chatbot also has information from the metadata of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1327a79-b19f-497a-a531-7e91b664f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-13 16:38:16.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1mSending query to conversatonal RAG: 这些文档创建的时间是什么时候？\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8864.86 ms\n",
      "llama_print_timings:      sample time =       5.33 ms /    61 runs   (    0.09 ms per token, 11442.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     197.06 ms /    32 tokens (    6.16 ms per token,   162.39 tokens per second)\n",
      "llama_print_timings:        eval time =    2453.89 ms /    60 runs   (   40.90 ms per token,    24.45 tokens per second)\n",
      "llama_print_timings:       total time =    2753.99 ms /    92 tokens\n",
      "\u001b[32m2024-03-13 16:38:19.756\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.conversational_rag\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mReceived response: 文档2 中提到了旅行时间范围为 2020 年 8 月至 2020 年 12 月之间。 不过，没有提供其他文档的创建时间。\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'文档2 中提到了旅行时间范围为 2020 年 8 月至 2020 年 12 月之间。 不过，没有提供其他文档的创建时间。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"这些文档创建的时间是什么时候？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a639329-ccb5-406a-921a-25819c636793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
