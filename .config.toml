# model path
model_path = '/Users/fred/Documents/models'

# model name
model_name = 'HuggingFaceH4/zephyr-7b-beta'

# model mapping
model_mapping.'baichuan-inc/Baichuan-7B' = 'baichuan2-7b-chat.Q4_K_S.gguf'
model_mapping.'hfl/chinese-alpaca-2-7b' = 'chinese-alpaca-2-7b-q4_0.gguf'
model_mapping.'Qwen/Qwen-7B-Chat' = 'Qwen-7B-Chat.Q4_K_M.gguf'
model_mapping.'Qwen/Qwen1.5-7B-Chat' = 'qwen1_5-7b-chat-q4_0.gguf'
model_mapping.'01-ai/Yi-6B-Chat' = 'yi-chat-6b.Q4_K_M.gguf'
model_mapping.'BAAI/AquilaChat2-7B-16K' = 'AquilaChat2-7B-16K.Q4_0.gguf'
model_mapping.'HuggingFaceH4/zephyr-7b-beta' = 'zephyr-7b-beta.Q4_K_M.gguf'

# LLM params
# ref: https://python.langchain.com/docs/guides/local_llms
# ref: https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/llamacpp.py
llm.n_gpu_layers = 32 # number of layer to be loaded in GPU. can check n_layers to see the total layers of llm
llm.n_batch = 2048 # batch size, number of tokens the model should process in parallel, should be between 1 and n_ctx, depend on the amount of RAM of Apple Silicon Chip.
llm.n_ctx = 7168 # context size, default is 2048, zephyr (base model mistrial 7b) supports 8K contexts
llm.temperature = 0.0
llm.f16_kv = true # Metal only supports True. Used to be a bug, see https://github.com/langchain-ai/langchain/pull/3320#issue-1679133618
# customized
llm.conversation.k_rounds = 1 # number of rounds of conversations to keep in memory, -1 as keep everything

# BM25 retriever
# no. of top docs to be returns
# ref: https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/retrievers/bm25.py
bm25.k = 5

# directories
path.tokens = '../.tokens.toml'
path.notion_dbs = '../.notion_databases.toml'
path.docs = '../data/notion.pkl'
path.test_cases = '../data/test_cases.txt'

# notion data pull
force_repull = false

# embedding model
embedding_model = "sentence-transformers/distiluse-base-multilingual-cased-v1"

# template and few-shot examples
template.query_analyzer = '../templates/query_analyzer_template.toml'
template.conversatoinal_rag = '../templates/conversational_rag_template.toml'
template.document_match_checker = '../templates/document_match_checker_template.toml'

# splitter
# 1 Chinese characters = 2 english character
# most paragraphs/sections are within 500 Chinese words/chars
splitter.chunk_size = 500
splitter.chunk_overlap = 50

# connection to redis standalone at localhost, db 0, no password
redis_url = "redis://localhost:6379"
index_name = "notiondb"

# vectorstore schema (required by Redis)
[[redis_schema.text]]
name = "author"
[[redis_schema.text]]
name = "id"
[[redis_schema.text]]
name = "name"
[[redis_schema.text]]
name = "source"
[[redis_schema.text]]
name = "tags"

[[redis_schema.numeric]]
name = "date_start"
[[redis_schema.numeric]]
name = "date_end"
