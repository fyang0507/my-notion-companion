{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3043cc34-fbc5-4253-a044-306ed1bf169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f64a4b-e3c4-4425-b5c4-e29cac977644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76569a6b-6a60-48f5-99ae-14f5a288bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05f623d4-0eda-4293-8610-5a2e1201613f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/fred/Documents/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.06 MiB, ( 4095.12 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4095.05 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 7168\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   896.00 MiB, ( 4992.69 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   896.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  896.00 MiB, K (f16):  448.00 MiB, V (f16):  448.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =   104.05 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1976.02 MiB, ( 6968.70 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  1976.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   314.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 4\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'huggingfaceh4_zephyr-7b-beta'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+_CONFIGS['model_mapping'][_CONFIGS['model_name']],\n",
    "    name=_CONFIGS['model_name'], \n",
    "    **_CONFIGS['llm']\n",
    ")\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _CONFIGS['model_name'], \n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03dcac4-cf0d-453d-927a-99f4a10883cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7168"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd598352-413c-4ff8-8c82-08b2f5356d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-09 21:01:52.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_filter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mSetting metadata fuzzy match threshold to: 0.8.\u001b[0m\n",
      "\u001b[32m2024-03-09 21:01:52.010\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.query_analyzer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mInitialize Query Analyzer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from my_notion_companion.notion_chatbot import NotionChatBot\n",
    "\n",
    "c = NotionChatBot(llm, tokenizer, '../.config.toml', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edbe4e07-ca44-406f-b3af-da0b6ab9851e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-09 21:02:16.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mTry lexical search.\u001b[0m\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =      14.65 ms /   112 runs   (    0.13 ms per token,  7645.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6803.21 ms /   742 tokens (    9.17 ms per token,   109.07 tokens per second)\n",
      "llama_print_timings:        eval time =    3998.08 ms /   111 runs   (   36.02 ms per token,    27.76 tokens per second)\n",
      "llama_print_timings:       total time =   11069.54 ms /   853 tokens\n",
      "\u001b[32m2024-03-09 21:02:27.638\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mretriever\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m101\u001b[0m - \u001b[31m\u001b[1mFailed to construct query for the input: what is openAI?, returning raw input.\u001b[0m\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/4m/z0fvcltx31xcv13t79qsnb8r0000gn/T/jieba.cache\n",
      "Loading model cost 0.412 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.50 ms /     4 runs   (    0.12 ms per token,  8064.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3828.45 ms /   808 tokens (    4.74 ms per token,   211.05 tokens per second)\n",
      "llama_print_timings:        eval time =     105.54 ms /     3 runs   (   35.18 ms per token,    28.43 tokens per second)\n",
      "llama_print_timings:       total time =    3944.72 ms /   811 tokens\n",
      "\u001b[32m2024-03-09 21:02:33.059\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'tags': '职场', 'author': 'John Lucht', 'name': 'Rites of Passage', 'id': '9ec1529d-87b8-4168-ba61-652c535091d9', 'source': '笔记（非文学）', 'date_start': 20210515}\n",
      "\n",
      "Q: how do you feel about your ...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:33.060\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     4 runs   (    0.16 ms per token,  6182.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =     619.47 ms /   117 tokens (    5.29 ms per token,   188.87 tokens per second)\n",
      "llama_print_timings:        eval time =     106.42 ms /     3 runs   (   35.47 ms per token,    28.19 tokens per second)\n",
      "llama_print_timings:       total time =     735.22 ms /   120 tokens\n",
      "\u001b[32m2024-03-09 21:02:33.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'tags': '职场', 'author': 'John Lucht', 'name': 'Rites of Passage', 'id': '9ec1529d-87b8-4168-ba61-652c535091d9', 'source': '笔记（非文学）', 'date_start': 20210515}\n",
      "\n",
      "Try saying this:\n",
      "\"So frankly, ...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:33.802\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.43 ms /     4 runs   (    0.11 ms per token,  9259.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     319.51 ms /    41 tokens (    7.79 ms per token,   128.32 tokens per second)\n",
      "llama_print_timings:        eval time =     104.13 ms /     3 runs   (   34.71 ms per token,    28.81 tokens per second)\n",
      "llama_print_timings:       total time =     431.12 ms /    44 tokens\n",
      "\u001b[32m2024-03-09 21:02:34.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'author': 'Martin Hägglund', 'tags': '人文', 'name': 'This Life: secular faith and spiritual freedom', 'id': '92051deb-aa0e-4a1f-a6a7-e5ee71efb805', 'source': '读书笔记（文学）', 'date_start': 20230331}\n",
      "\n",
      "The anxiety of death is an exp...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:34.239\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.51 ms /     4 runs   (    0.13 ms per token,  7782.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.12 ms /   126 tokens (    4.75 ms per token,   210.66 tokens per second)\n",
      "llama_print_timings:        eval time =     109.35 ms /     3 runs   (   36.45 ms per token,    27.44 tokens per second)\n",
      "llama_print_timings:       total time =     715.36 ms /   129 tokens\n",
      "\u001b[32m2024-03-09 21:02:34.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'author': 'Martin Hägglund', 'tags': '人文', 'name': 'This Life: secular faith and spiritual freedom', 'id': '92051deb-aa0e-4a1f-a6a7-e5ee71efb805', 'source': '读书笔记（文学）', 'date_start': 20230331}\n",
      "\n",
      "Nietzsche famously said “God i...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:34.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.49 ms /     4 runs   (    0.12 ms per token,  8097.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =     598.24 ms /   104 tokens (    5.75 ms per token,   173.84 tokens per second)\n",
      "llama_print_timings:        eval time =     104.11 ms /     3 runs   (   34.70 ms per token,    28.82 tokens per second)\n",
      "llama_print_timings:       total time =     710.15 ms /   107 tokens\n",
      "\u001b[32m2024-03-09 21:02:35.674\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'author': 'Martin Hägglund', 'tags': '人文', 'name': 'This Life: secular faith and spiritual freedom', 'id': '92051deb-aa0e-4a1f-a6a7-e5ee71efb805', 'source': '读书笔记（文学）', 'date_start': 20230331}\n",
      "\n",
      "To own your life is not to own...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:35.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "\u001b[32m2024-03-09 21:02:35.675\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1m0 docs found via lexical search. Try semantic search.\u001b[0m\n",
      "score_threshold is deprecated. Use distance_threshold instead.score_threshold should only be used in similarity_search_with_relevance_scores.score_threshold will be removed in a future release.\n",
      "Metadata key author not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key author not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.48 ms /     4 runs   (    0.12 ms per token,  8421.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2740.05 ms /   527 tokens (    5.20 ms per token,   192.33 tokens per second)\n",
      "llama_print_timings:        eval time =     116.00 ms /     3 runs   (   38.67 ms per token,    25.86 tokens per second)\n",
      "llama_print_timings:       total time =    2868.36 ms /   530 tokens\n",
      "\u001b[32m2024-03-09 21:03:04.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:d405bf860e614255afe516ef2453daf5', 'author': None, 'name': '2019-JAN-12 二〇一九指导纲领 オペレーション SGA', 'source': '写作', 'tags': '年度报告', 'date_start': '20190112', 'date_end': '20190112'}\n",
      "\n",
      "恋爱之后，生活却并未就此展开。\n",
      "当然，以我的立场来看，这两件...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:04.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.53 ms /     4 runs   (    0.13 ms per token,  7604.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3416.19 ms /   779 tokens (    4.39 ms per token,   228.03 tokens per second)\n",
      "llama_print_timings:        eval time =     128.35 ms /     3 runs   (   42.78 ms per token,    23.37 tokens per second)\n",
      "llama_print_timings:       total time =    3553.92 ms /   782 tokens\n",
      "\u001b[32m2024-03-09 21:03:07.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:1ab61bc762bd427ebb793cc90500a871', 'author': None, 'name': '我们生活的故事♯1', 'source': '写作', 'tags': '我们生活的故事', 'date_start': '20211106', 'date_end': '20211128'}\n",
      "\n",
      "（六）\n",
      "\n",
      "十多年前，苹果凭借iPhone的惊雷震撼刚刚兴起的...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:07.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       0.46 ms /     4 runs   (    0.11 ms per token,  8771.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3824.50 ms /   893 tokens (    4.28 ms per token,   233.49 tokens per second)\n",
      "llama_print_timings:        eval time =     128.37 ms /     3 runs   (   42.79 ms per token,    23.37 tokens per second)\n",
      "llama_print_timings:       total time =    3961.97 ms /   896 tokens\n",
      "\u001b[32m2024-03-09 21:03:11.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:a647d1045dee405f81ac645350c3eb99', 'author': '托马斯', 'name': '托马斯风光调色', 'source': '笔记（非文学）', 'tags': '摄影', 'date_start': '20200525', 'date_end': None}\n",
      "\n",
      "ACR使用技巧\n",
      "以打开智能对象的方式打开ACR\n",
      "按住shif...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:11.559\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 不相关\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       8.60 ms /    68 runs   (    0.13 ms per token,  7906.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4236.47 ms /   953 tokens (    4.45 ms per token,   224.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2593.36 ms /    67 runs   (   38.71 ms per token,    25.84 tokens per second)\n",
      "llama_print_timings:       total time =    6965.53 ms /  1020 tokens\n",
      "\u001b[32m2024-03-09 21:03:18.531\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m43\u001b[0m - \u001b[1mcompare relevance with doc:\n",
      "\n",
      "{'id': 'doc:notiondb:c786d0d52fe8438494dce25e56a9e88e', 'author': 'Sean Ellis & Morgan Brown', 'name': '增长黑客', 'source': '笔记（非文学）', 'tags': '科技, 数据科学, 创业', 'date_start': '20210926', 'date_end': None}\n",
      "\n",
      "除问卷以外，开展更多更细致的采访和实地调研（从互联网上走下来...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:18.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdocument_match_checker\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m44\u001b[0m - \u001b[1mconclusion: 相关，因为分析师需要分析用户对产品的使用和行为，以确定增长杠杆和核心增长指标，并优化用户体验，使其可以尽快发现产品的核心价值。\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:18.532\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mRetrieved relevant docs:\n",
      "\n",
      "{'id': 'doc:notiondb:c786d0d52fe8438494dce25e56a9e88e', 'author': 'Sean Ellis & Morgan Brown', 'name': '增长黑客', 'source': '笔记（非文学）', 'tags': '科技, 数据科学, 创业', 'date_start': '20210926', 'date_end': None}\n",
      "\n",
      "除问卷以外，开展更多更细致的采访和实地调研（从互联网上走下来...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "\u001b[32m2024-03-09 21:03:18.533\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mInitialize Conversational RAG.\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =      13.46 ms /   147 runs   (    0.09 ms per token, 10922.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5135.23 ms /  1215 tokens (    4.23 ms per token,   236.60 tokens per second)\n",
      "llama_print_timings:        eval time =    5412.19 ms /   146 runs   (   37.07 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =   10793.48 ms /  1361 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': \"OpenAI is a research organization founded by Elon Musk and others in 2015. Its mission is to advance digital intelligence in a way that is safe and beneficial for humanity. OpenAI conducts research in areas such as artificial intelligence, machine learning, and robotics, with the goal of developing new technologies that can help solve some of the world's most pressing problems. The organization is funded by a variety of sources, including Musk's own investment, as well as grants and partnerships with companies like Microsoft and Amazon. OpenAI's work is open source and freely available to the public, in line with its commitment to advancing scientific knowledge and promoting collaboration in the field of AI research.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"what is openAI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e150555-0bc7-4a92-bd78-90171259d83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-09 21:06:44.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36mclear\u001b[0m:\u001b[36m39\u001b[0m - \u001b[1mClear retrieved documents. Please re-enter the prompt.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "c.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cc87dc4-b8e8-4aa2-a937-c5abf1df91f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-09 21:06:53.863\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m46\u001b[0m - \u001b[1mTry lexical search.\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    6804.68 ms\n",
      "llama_print_timings:      sample time =       3.79 ms /    40 runs   (    0.09 ms per token, 10542.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4830.47 ms /   726 tokens (    6.65 ms per token,   150.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1361.00 ms /    39 runs   (   34.90 ms per token,    28.66 tokens per second)\n",
      "llama_print_timings:       total time =    6260.81 ms /   765 tokens\n",
      "\u001b[32m2024-03-09 21:07:00.133\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mretriever\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m101\u001b[0m - \u001b[31m\u001b[1mFailed to construct query for the input: tell me more about its founder, returning raw input.\u001b[0m\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtell me more about its founder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/GenAIProjects/my-notion-companion/playground/../my_notion_companion/notion_chatbot.py:48\u001b[0m, in \u001b[0;36mNotionChatBot.invoke\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     46\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry lexical search.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m docs_retrieved \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretriever_lexical\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[0;32m---> 48\u001b[0m docs_filtered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch_checker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs_retrieved\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(docs_filtered) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     51\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(docs_filtered)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m docs found via lexical search. Try semantic search.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-Personal/GenAIProjects/my-notion-companion/my_notion_companion/document_match_checker.py:40\u001b[0m, in \u001b[0;36mDocumentMatchChecker.invoke\u001b[0;34m(self, docs, query)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[1;32m     39\u001b[0m     user_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_user_content(doc, query)\n\u001b[0;32m---> 40\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m     43\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompare relevance with doc:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpeek_docs([doc])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/runnables/base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:273\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    270\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    271\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 273\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    284\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:568\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    567\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m     ]\n\u001b[0;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 592\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    601\u001b[0m         )\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/language_models/llms.py:1177\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m   1176\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1180\u001b[0m     )\n\u001b[1;32m   1181\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m   1182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:288\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombined_text_output\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_community/llms/llamacpp.py:341\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[0;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    340\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 341\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGenerationChunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/llama.py:978\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    976\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    977\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 978\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    991\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_eos\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompletion_tokens\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/llama.py:663\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 663\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_tokens:\n\u001b[1;32m    665\u001b[0m         token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    666\u001b[0m             top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    667\u001b[0m             top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    681\u001b[0m             idx\u001b[38;5;241m=\u001b[39msample_idx,\n\u001b[1;32m    682\u001b[0m         )\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/llama.py:503\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    499\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[1;32m    501\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[1;32m    502\u001b[0m )\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/_internals.py:305\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/llama_cpp.py:1636\u001b[0m, in \u001b[0;36mllama_decode\u001b[0;34m(ctx, batch)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Positive return values does not mean a fatal error, but rather a warning.\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;124;03m    0 - success\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;124;03m    1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\u001b[39;00m\n\u001b[1;32m   1635\u001b[0m \u001b[38;5;124;03m    < 0 - error\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c.invoke(\"tell me more about its founder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4f216f4-a46b-4eea-a663-6d8db8197cb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 10019dbb-3639-4bc4-a6f4-a499a3092ff4\n",
       "system: 你是一个乐于助人的AI助手。在对话开头，你会得到一些文档。这些文档包含了文档的内容和元数据两部分。请仅根据这些文档回答问题。如果你不知道答案，只需说不知道即可。如果你没有得到任何文档，请谨慎根据情景简洁回答。请使用与提问和资料相同的语言进行回答。\n",
       "\n",
       "资料：\n",
       "\n",
       "\n",
       "文档1\n",
       "\n",
       "内容：\n",
       "除问卷以外，开展更多更细致的采访和实地调研（从互联网上走下来），才可以真正与客户和潜在客户交谈。 Etsy派遣员工参与全国各地的工艺品展销会，这推动了Etsy早期的开发和社区形成。Tinder锁定了大学的兄弟会和姐妹会，并通过实地去校园演讲，打造了当地的约会网络。\n",
       "跟踪活跃用户\n",
       "分析师应该寻找那些频繁使用或购买产品的用户，发现他们与其他普通用户之间的区别。可以根据用户的属性进行分类（地区、年龄、性别、职业、移动设备等），找出超级用户的活跃与某个产品特征之间的关联。Netflix就是通过数据挖掘发现政治题材的电视剧和凯文·史派西的电影非常受欢迎，从而推出了《纸牌屋》。\n",
       "重新定位\n",
       "在发现用户（可能出人意料的）对产品的使用后，团队需要反思是否需要对产品进行重新定位。Yelp最初的开发重心并不是它的用户评论功能，Instagram也是从一个复杂的社交应用精简而成的产品，甚至YouTube最初是一个视频约会网站，但创始人发现人们上传视频资料并不是为了寻找约会对象。\n",
       "引导用户进入“啊哈时刻”\n",
       "以上所有的试验和分析都是为了聚焦于“啊哈时刻”。当我们发现这个神奇时刻后，团队应该把所有精力放在优化用户体验上，使得新用户可以尽快发现该产品的核心价值。 啊哈时刻对后期的所有增长都至关重要，因此必须付出大量的资源。Facebook、Twitter和Pinterest为此建立了专门的团队，将新用户体验从主产品从独立出来当做一个新产品看待。\n",
       "确定增长杠杆\n",
       "在确定产品的“啊哈时刻”之后，下一步是明确增长战略。在这一步我们需要明确增长杠杆，整合数据和深入分析，并出具报表跟踪相关进展。\n",
       "增长公式和北极星指标\n",
       "首先以“基本增长等式（fundamental growth equation）”开始分析公司成长的驱动因素。\n",
       "eBay的增长等式为：发布物品的卖家数 x 发布物品的数量 x 买家数量 x 成功交易数量 = 总商品增加数量 亚马逊的增长等式为：垂直扩张 x 平均垂直市场产品库存 x 产品流量 x 购买转化率 x 平均购买价值 x 重复购买行为 = 收入增长\n",
       "增长等式可能并无法囊括更多的影响利润的因素，比如库存周转，研发投入，材料成本等等，但正因这种简洁才能使我们聚焦。\n",
       "在该等式的基础上，我们继续确定核心增长指标，即用户对产品核心价值体验最相关的行为。\n",
       "\n",
       "元数据：\n",
       "{'id': 'doc:notiondb:c786d0d52fe8438494dce25e56a9e88e', 'author': 'Sean Ellis & Morgan Brown', 'name': '增长黑客', 'source': '笔记（非文学）', 'tags': '科技, 数据科学, 创业', 'date_start': '20210926', 'date_end': None}\n",
       "user: what is openAI?\n",
       "assistant: OpenAI is a research organization founded by Elon Musk and others in 2015. Its mission is to advance digital intelligence in a way that is safe and beneficial for humanity. OpenAI conducts research in areas such as artificial intelligence, machine learning, and robotics, with the goal of developing new technologies that can help solve some of the world's most pressing problems. The organization is funded by a variety of sources, including Musk's own investment, as well as grants and partnerships with companies like Microsoft and Amazon. OpenAI's work is open source and freely available to the public, in line with its commitment to advancing scientific knowledge and promoting collaboration in the field of AI research."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.conversatoinal_rag.conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80322424-997d-40e5-a60a-19ba354b1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.conversatoinal_rag.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c120cae6-28b2-4b96-a22f-f2ee81ae6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation id: 9095a6db-8eef-4181-9006-0b343297f541\n",
       "system: 你是一个乐于助人的AI助手。在对话开头，你会得到一些文档。这些文档包含了文档的内容和元数据两部分。请仅根据这些文档回答问题。如果你不知道答案，只需说不知道即可。如果你没有得到任何文档，请谨慎根据情景简洁回答。请使用与提问和资料相同的语言进行回答。\n",
       "\n",
       "资料：\n",
       "\n",
       "\n",
       "文档1\n",
       "\n",
       "内容：\n",
       "除问卷以外，开展更多更细致的采访和实地调研（从互联网上走下来），才可以真正与客户和潜在客户交谈。 Etsy派遣员工参与全国各地的工艺品展销会，这推动了Etsy早期的开发和社区形成。Tinder锁定了大学的兄弟会和姐妹会，并通过实地去校园演讲，打造了当地的约会网络。\n",
       "跟踪活跃用户\n",
       "分析师应该寻找那些频繁使用或购买产品的用户，发现他们与其他普通用户之间的区别。可以根据用户的属性进行分类（地区、年龄、性别、职业、移动设备等），找出超级用户的活跃与某个产品特征之间的关联。Netflix就是通过数据挖掘发现政治题材的电视剧和凯文·史派西的电影非常受欢迎，从而推出了《纸牌屋》。\n",
       "重新定位\n",
       "在发现用户（可能出人意料的）对产品的使用后，团队需要反思是否需要对产品进行重新定位。Yelp最初的开发重心并不是它的用户评论功能，Instagram也是从一个复杂的社交应用精简而成的产品，甚至YouTube最初是一个视频约会网站，但创始人发现人们上传视频资料并不是为了寻找约会对象。\n",
       "引导用户进入“啊哈时刻”\n",
       "以上所有的试验和分析都是为了聚焦于“啊哈时刻”。当我们发现这个神奇时刻后，团队应该把所有精力放在优化用户体验上，使得新用户可以尽快发现该产品的核心价值。 啊哈时刻对后期的所有增长都至关重要，因此必须付出大量的资源。Facebook、Twitter和Pinterest为此建立了专门的团队，将新用户体验从主产品从独立出来当做一个新产品看待。\n",
       "确定增长杠杆\n",
       "在确定产品的“啊哈时刻”之后，下一步是明确增长战略。在这一步我们需要明确增长杠杆，整合数据和深入分析，并出具报表跟踪相关进展。\n",
       "增长公式和北极星指标\n",
       "首先以“基本增长等式（fundamental growth equation）”开始分析公司成长的驱动因素。\n",
       "eBay的增长等式为：发布物品的卖家数 x 发布物品的数量 x 买家数量 x 成功交易数量 = 总商品增加数量 亚马逊的增长等式为：垂直扩张 x 平均垂直市场产品库存 x 产品流量 x 购买转化率 x 平均购买价值 x 重复购买行为 = 收入增长\n",
       "增长等式可能并无法囊括更多的影响利润的因素，比如库存周转，研发投入，材料成本等等，但正因这种简洁才能使我们聚焦。\n",
       "在该等式的基础上，我们继续确定核心增长指标，即用户对产品核心价值体验最相关的行为。\n",
       "\n",
       "元数据：\n",
       "{'id': 'doc:notiondb:c786d0d52fe8438494dce25e56a9e88e', 'author': 'Sean Ellis & Morgan Brown', 'name': '增长黑客', 'source': '笔记（非文学）', 'tags': '科技, 数据科学, 创业', 'date_start': '20210926', 'date_end': None}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.conversatoinal_rag.conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6ca36b-5487-468b-ac33-e836a2ac0a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
