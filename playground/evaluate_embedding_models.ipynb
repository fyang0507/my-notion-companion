{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4d221c-be00-456d-955e-64f9d9898cc7",
   "metadata": {},
   "source": [
    "# Evaluate embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a6158-ed87-4317-995c-ad239b500d9a",
   "metadata": {},
   "source": [
    "## load and format test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bdd4433-0a5d-4f97-a98e-abcbbb4d08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d02cdc-03a5-436b-9c3b-5f683ad2033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_cases.txt') as f:\n",
    "    test_cases_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a3d413-eba3-4344-8b85-ac7d14f76be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['问：什么是我国第一部编年史著作？\\n',\n",
       " '\\n',\n",
       " '答：《左传》。\\n',\n",
       " '\\n',\n",
       " '资料：附：《左传》是我国第一部编年史著作。\\n',\n",
       " '\\n',\n",
       " '问：什么是我国第一部编年国别史？\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases_raw[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd3b7818-0498-4ece-b773-81802d85fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f3e3ef-c4ed-429f-9f20-982990be4809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'q': '什么是我国第一部编年史著作？', 'a': '《左传》。', 'ref': ['附：《左传》是我国第一部编年史著作。\\n']},\n",
       " {'q': '什么是我国第一部编年国别史？', 'a': '《国语》。', 'ref': ['附：《国语》是我国第一部编年国别史。\\n']},\n",
       " {'q': '“寡人之于国也”下一句是什么？来自哪里？',\n",
       "  'a': '“寡人之于国也”下一句是“尽心焉耳矣”。这个句子来自《孟子》。',\n",
       "  'ref': ['梁惠王曰：“寡人之于国也，尽心焉耳矣。河内凶，则移其民于河东，移其粟于河内；河东凶亦然。察邻国之政，无如寡人之用心者。邻国之民不加少，寡人之民不加多，何也？”',\n",
       "   '《寡人之于国也》（孟子）\\n']}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_cases_preprocessing(raw_texts: str) -> Dict[str, str]:\n",
    "    # combining into a single string, remove all \"\\n\" in between\n",
    "    texts_split = \"\".join(raw_texts[0::2]).split('问：')[1:]\n",
    "    # separate by keyword 答, 资料\n",
    "    texts_split = [re.split(r'\\n答：|\\n资料：', x) for x in texts_split]\n",
    "    # remove all \\xa0 in between\n",
    "    texts_split = [[x.replace('\\xa0', '') for x in sublist] for sublist in texts_split]\n",
    "    # format into a dict with q, a, and ref keys\n",
    "    test_cases = [{'q': x[0], 'a': x[1], 'ref': x[2:]} for x in texts_split]\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "test_cases = test_cases_preprocessing(test_cases_raw)\n",
    "test_cases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce20332-f979-4709-bd85-aee1e631a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031c41f-e17f-46a4-89e7-b84c01a4cc1b",
   "metadata": {},
   "source": [
    "# Evaluate embedding models\n",
    "\n",
    "* We want our `q` questions to be as close to `ref` refernced documents as possible, as `ref` are text chunks directly retrieved from the documents that are to be put into vector databases.\n",
    "* Evaluated options from SentenceTransformer registries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91dbe9d-0348-4b4e-865e-334b28b823cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # not free\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21982fcb-21fd-4c42-8166-7580142460d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fed7b8-0a4b-4158-a104-ce3dfe37d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import file_utils\n",
    "# print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a3e5c-1b34-4c6c-a3b3-1e4cc91a2bad",
   "metadata": {},
   "source": [
    "Model name | Provider | Model size (#pamras) | Model Size (disk) | Download past month | Highlights | Time Load/Inference (online compute) | Mean difference paired & unpaired Q & Ref | HF Link |\n",
    "--|--|--|--|--|--|--|--|--|\n",
    "intfloat/multilingual-e5-large | Microsoft | 560M | 2.2G | 93K |24 layers and the embedding size is 1024| 5.0s/1920s | 0.062 |https://huggingface.co/intfloat/multilingual-e5-large|\n",
    "intfloat/multilingual-e5-base| Microsoft | 278M | 1.1G | 42K |12 layers and the embedding size is 768| 3.4s/531s| 0.063 | https://huggingface.co/intfloat/multilingual-e5-base|\n",
    "sentence-transformers/LaBSE | Google | | 1.9G | 88K | the embedding size is 768 | 5.7s/620s | 0.19 | https://huggingface.co/sentence-transformers/LaBSE|\n",
    "maidalun1020/bce-embedding-base_v1 | NetEase-Youdao |  279M | 1.1G | 111K | optimized for RAG | 3.0s/495s | 0.23 | https://huggingface.co/maidalun1020/bce-embedding-base_v1\n",
    "BAAI/bge-large-zh-v1.5|Beijing Academy of Artificial Intelligence| 326M | 1.3G | 22K | | 1.6s/1730s| 0.26 |  https://huggingface.co/BAAI/bge-large-zh-v1.5#usage|\n",
    "uer/sbert-base-chinese-nli| Tencent | | 409M  | 8K | 12 layers and the embedding size is 768 | 0.6s/1350s | 0.22 | https://huggingface.co/uer/sbert-base-chinese-nli |\n",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2| Sentence Transformer | | 449M | 38K | 384 embedding size | 1.4s/392s | 0.25 | https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |\n",
    "sentence-transformers/distiluse-base-multilingual-cased-v1 | Sentence Transformer | | 539M | 31K | 768 embedding size | 1.3s/163s | 0.28 | https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1 |\n",
    "sentence-transformers/distiluse-base-multilingual-cased-v2 | Sentence Transformer | | 539M | 43K | 768 enbedding size | 1.2s/164s | 0.25 | https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 |\n",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2 | Sentence Transformer | | 1.1G | 24K | 768 embedding size | 2.7s/463s | 0.21 | https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc75478-4dde-4ad1-b094-4c8f4710cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# validate GPU/Metal acceleration on Mac (no action requires, should be enabled with environment build)\n",
    "# https://developer.apple.com/metal/pytorch/\n",
    "# The output should show: tensor([1.], device='mps:0')\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85414a88-8c36-41fc-aa47-74500acb73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all embeedings supported in sentence-transformers library\n",
    "# https://huggingface.co/models?library=sentence-transformers\n",
    "\n",
    "# cached model objects in ~/.cache/torch/sentence_transformers\n",
    "\n",
    "sentence_transformer_model_lists = [\n",
    "    # Microsoft\n",
    "    'intfloat/multilingual-e5-large',\n",
    "    'intfloat/multilingual-e5-base',\n",
    "\n",
    "    # Google\n",
    "    'sentence-transformers/LaBSE',\n",
    "\n",
    "    # Chinese companies\n",
    "    'maidalun1020/bce-embedding-base_v1', # this requires direct download through Git LFS from HF as it is gated https://huggingface.co/maidalun1020/bce-embedding-base_v1/tree/main\n",
    "    'BAAI/bge-large-zh-v1.5',\n",
    "    'uer/sbert-base-chinese-nli',\n",
    "\n",
    "    # Sentence Transformer native\n",
    "    # https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
    "    'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
    "    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "]\n",
    "\n",
    "# # download the models\n",
    "# for model_name in sentence_transformer_model_lists:\n",
    "#     print(model_name)\n",
    "#     _ = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b82a9ce-3f38-4784-8cfc-cfa2efd642c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [x['q'] for x in test_cases] + [x['a'] for x in test_cases]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b2e58c-4d62-4263-98c7-2df63e608529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3060b7bc-1610-44de-8fd3-b2688bd094cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 09:31:03.790\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprofile_docs\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mtotal Chinese characters: 3820, total docs: 74\u001b[0m\n",
      "\u001b[32m2024-01-21 09:31:03.791\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprofile_docs\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mMin/Mean/Max characters per doc: 3, 51.62, 523\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def profile_docs(docs: List[str]):\n",
    "    characters = [len(x) for x in docs]\n",
    "    len_docs = len(docs)\n",
    "    logger.info(f\"total Chinese characters: {sum(characters)}, total docs: {len_docs}\")\n",
    "    logger.info(f\"Min/Mean/Max characters per doc: {min(characters)}, {np.mean(characters):.2f}, {max(characters)}\")\n",
    "\n",
    "profile_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0a706b-7213-45bb-a496-5b192b321ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '/Users/fred/.cache/torch/sentence_transformers/'\n",
    "\n",
    "def _get_model_path(model_dir, model_name):\n",
    "    return model_dir+model_name.replace('/', '_')+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2abacf-edf4-45e4-aa16-3c64df38837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the querying time for each embedding model using questions in the test cases\n",
    "\n",
    "# def timer_embedding_model(model_name, sentences, model_dir) -> None:\n",
    "#     time_start = time.time()\n",
    "#     model = SentenceTransformer.load(_get_model_path(model_dir, model_name))\n",
    "#     time_end = time.time()\n",
    "#     logger.info(f'Time taken loading {model_name}: {time_end - time_start:.2f}s')\n",
    "    \n",
    "#     time_start = time.time()\n",
    "#     model.encode(sentences)\n",
    "#     time_end = time.time()\n",
    "#     logger.info(f\"Time taken for {model_name}: {time_end - time_start:.2f}s\")\n",
    "\n",
    "#     print()\n",
    "\n",
    "# for model in sentence_transformer_model_lists:\n",
    "#     timer_embedding_model(model, docs, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b657d-7ef1-4c6a-af7a-6b21f879e615",
   "metadata": {},
   "source": [
    "\"contrastive loss\" measure for test cases\n",
    "\n",
    "* paired `ref` and `q` should have high similarity\n",
    "* unpaired `ref` and `q` should have low similarity\n",
    "* scores can be computed as the difference between the two as \"contrastive\" measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13b2353-b105-4be8-ba51-b40b66583aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _flatten_list(ll: List[List[str]]) -> List[str]:\n",
    "    return [item for sublist in ll for item in sublist]\n",
    "\n",
    "_flatten_list([[1,2], [3, 4, 5], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf08fd7f-9de8-497c-8cf9-ebc4c6a097ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SentenceTransformer('sentence-transformers/distilu4se-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c6bcfc-7a93-4f59-83a5-009da69c6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 09:31:08.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading intfloat/multilingual-e5-large: 4.95s\u001b[0m\n",
      "\u001b[32m2024-01-21 10:03:09.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by intfloat/multilingual-e5-large: 1920.41s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 10:03:12.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading intfloat/multilingual-e5-base: 3.42s\u001b[0m\n",
      "\u001b[32m2024-01-21 10:12:04.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by intfloat/multilingual-e5-base: 531.77s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 10:12:10.018\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading sentence-transformers/LaBSE: 5.66s\u001b[0m\n",
      "\u001b[32m2024-01-21 10:22:29.862\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by sentence-transformers/LaBSE: 619.84s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 10:22:32.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading maidalun1020/bce-embedding-base_v1: 2.96s\u001b[0m\n",
      "\u001b[32m2024-01-21 10:30:48.336\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by maidalun1020/bce-embedding-base_v1: 495.52s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 10:30:49.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading BAAI/bge-large-zh-v1.5: 1.61s\u001b[0m\n",
      "\u001b[32m2024-01-21 10:59:39.496\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by BAAI/bge-large-zh-v1.5: 1729.55s\u001b[0m\n",
      "No sentence-transformers model found with name /Users/fred/.cache/torch/sentence_transformers/uer_sbert-base-chinese-nli/. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 10:59:40.109\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading uer/sbert-base-chinese-nli: 0.61s\u001b[0m\n",
      "\u001b[32m2024-01-21 11:22:09.834\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by uer/sbert-base-chinese-nli: 1349.72s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 11:22:11.204\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 1.37s\u001b[0m\n",
      "\u001b[32m2024-01-21 11:28:43.177\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 391.97s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 11:28:44.487\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading sentence-transformers/distiluse-base-multilingual-cased-v1: 1.31s\u001b[0m\n",
      "\u001b[32m2024-01-21 11:31:27.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by sentence-transformers/distiluse-base-multilingual-cased-v1: 163.21s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 11:31:28.916\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading sentence-transformers/distiluse-base-multilingual-cased-v2: 1.21s\u001b[0m\n",
      "\u001b[32m2024-01-21 11:34:13.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by sentence-transformers/distiluse-base-multilingual-cased-v2: 164.47s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-21 11:34:16.101\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTime taken loading sentence-transformers/paraphrase-multilingual-mpnet-base-v2: 2.71s\u001b[0m\n",
      "\u001b[32m2024-01-21 11:41:59.225\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mTime taken to score by sentence-transformers/paraphrase-multilingual-mpnet-base-v2: 463.12s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 16h 53min 27s, sys: 23min 6s, total: 17h 16min 34s\n",
      "Wall time: 2h 10min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sentence_transformers import util as st_utils\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for model_name in sentence_transformer_model_lists:\n",
    "    \n",
    "    time_start = time.time()\n",
    "    model = SentenceTransformer.load(_get_model_path(MODEL_DIR, model_name))\n",
    "    time_end = time.time()\n",
    "    logger.info(f'Time taken loading {model_name}: {time_end - time_start:.2f}s')\n",
    "    \n",
    "    score_list = list()\n",
    "    \n",
    "    time_start = time.time()\n",
    "    for qa in test_cases:\n",
    "        \n",
    "        query = qa['q']\n",
    "        docs_paired = qa['ref']\n",
    "        docs_unpaired = _flatten_list([x['ref'] for x in test_cases if x is not qa])\n",
    "\n",
    "        # skip if empty records found\n",
    "        if query == \"\" or docs_paired == []:\n",
    "            continue\n",
    "        \n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        docs_paired_embeddings = model.encode(docs_paired, convert_to_tensor=True)\n",
    "        docs_unpaired_embeddings = model.encode(docs_unpaired, convert_to_tensor=True)\n",
    "        \n",
    "        cos_scores_paired = st_utils.cos_sim(query_embedding, docs_paired_embeddings)[0] # cos_sim is used in multi-dim computation, so it returns nested result [[]]\n",
    "        cos_scored_unpaired = st_utils.cos_sim(query_embedding, docs_unpaired_embeddings)[0]\n",
    "        score_list.append({'score_paired': cos_scores_paired.numpy(), 'score_unpaired': cos_scored_unpaired.numpy()})\n",
    "\n",
    "    time_end = time.time()\n",
    "    logger.info(f'Time taken to score by {model_name}: {time_end-time_start:.2f}s')\n",
    "    print()\n",
    "    \n",
    "    scores[model_name] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76f7574f-4e80-49b0-a33b-179fc8cfc2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save\n",
    "# use np.load('../data/embedding_models_evaluation.npy', allow_pickle=True)\n",
    "np.save('../data/embedding_models_evaluation.npy', scores, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20698ac2-d1c6-448e-a958-7b46cabfbc84",
   "metadata": {},
   "source": [
    "### Process scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72c9ffc3-906c-476a-8fe4-ba32099a7618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'intfloat/multilingual-e5-large': 0.0616228,\n",
       " 'intfloat/multilingual-e5-base': 0.062966585,\n",
       " 'sentence-transformers/LaBSE': 0.18936068,\n",
       " 'maidalun1020/bce-embedding-base_v1': 0.23052517,\n",
       " 'BAAI/bge-large-zh-v1.5': 0.25686648,\n",
       " 'uer/sbert-base-chinese-nli': 0.21703584,\n",
       " 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2': 0.24728146,\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v1': 0.2788659,\n",
       " 'sentence-transformers/distiluse-base-multilingual-cased-v2': 0.24307375,\n",
       " 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2': 0.21112707}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aggregate_scores(scores: Dict[str, List[Dict[str, np.array]]]):\n",
    "\n",
    "    result = dict()\n",
    "    for model_name, score in scores.items():\n",
    "        # inner loop aggregation to average all scores of examples for each q\n",
    "        # result looks like [{'score_paired': arr, 'score_unpaired': arr}, {same fotmat}, {same}, ...]\n",
    "        score_agg = [{'score_paired' : np.mean(x['score_paired']), 'score_unpaired': np.mean(x['score_unpaired'])} for x in score]\n",
    "        \n",
    "        # outer loop aggregation to average all scores of all qs\n",
    "        score_paired = np.mean([x['score_paired'] for x in score_agg])\n",
    "        score_unpaired = np.mean([x['score_unpaired'] for x in score_agg])\n",
    "\n",
    "        # final result per model\n",
    "        diff = score_paired - score_unpaired\n",
    "        result[model_name] = diff\n",
    "\n",
    "    return result\n",
    "\n",
    "aggregate_scores(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a18b33-7b91-4dd6-80f6-e3f44182dda3",
   "metadata": {},
   "source": [
    "Final recommendation: `sentence-transformers/distiluse-base-multilingual-cased-v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06c3e58-b11f-45a3-b961-82c624a29b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
