{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4d221c-be00-456d-955e-64f9d9898cc7",
   "metadata": {},
   "source": [
    "# Testing embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a6158-ed87-4317-995c-ad239b500d9a",
   "metadata": {},
   "source": [
    "## load and format test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1bdd4433-0a5d-4f97-a98e-abcbbb4d08f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68d02cdc-03a5-436b-9c3b-5f683ad2033d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/test_cases.txt') as f:\n",
    "    test_cases_raw = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9a3d413-eba3-4344-8b85-ac7d14f76be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['问：什么是我国第一部编年史著作？\\n',\n",
       " '\\n',\n",
       " '答：《左传》。\\n',\n",
       " '\\n',\n",
       " '资料：附：《左传》是我国第一部编年史著作。\\n',\n",
       " '\\n',\n",
       " '问：什么是我国第一部编年国别史？\\n']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cases_raw[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd3b7818-0498-4ece-b773-81802d85fe46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2f3e3ef-c4ed-429f-9f20-982990be4809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'q': '什么是我国第一部编年史著作？', 'a': '《左传》。', 'ref': ['附：《左传》是我国第一部编年史著作。\\n']},\n",
       " {'q': '什么是我国第一部编年国别史？', 'a': '《国语》。', 'ref': ['附：《国语》是我国第一部编年国别史。\\n']},\n",
       " {'q': '“寡人之于国也”下一句是什么？来自哪里？',\n",
       "  'a': '“寡人之于国也”下一句是“尽心焉耳矣”。这个句子来自《孟子》。',\n",
       "  'ref': ['梁惠王曰：“寡人之于国也，尽心焉耳矣。河内凶，则移其民于河东，移其粟于河内；河东凶亦然。察邻国之政，无如寡人之用心者。邻国之民不加少，寡人之民不加多，何也？”',\n",
       "   '《寡人之于国也》（孟子）\\n']}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_cases_preprocessing(raw_texts: str) -> Dict[str, str]:\n",
    "    # combining into a single string, remove all \"\\n\" in between\n",
    "    texts_split = \"\".join(raw_texts[0::2]).split('问：')[1:]\n",
    "    # separate by keyword 答, 资料\n",
    "    texts_split = [re.split(r'\\n答：|\\n资料：', x) for x in texts_split]\n",
    "    # remove all \\xa0 in between\n",
    "    texts_split = [[x.replace('\\xa0', '') for x in sublist] for sublist in texts_split]\n",
    "    # format into a dict with q, a, and ref keys\n",
    "    test_cases = [{'q': x[0], 'a': x[1], 'ref': x[2:]} for x in texts_split]\n",
    "    \n",
    "    return test_cases\n",
    "\n",
    "test_cases = test_cases_preprocessing(test_cases_raw)\n",
    "test_cases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce20332-f979-4709-bd85-aee1e631a76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2031c41f-e17f-46a4-89e7-b84c01a4cc1b",
   "metadata": {},
   "source": [
    "# Evaluate embedding models\n",
    "\n",
    "* We want our `q` questions to be as close to `ref` refernced documents as possible, as `ref` are text chunks directly retrieved from the documents that are to be put into vector databases.\n",
    "\n",
    "* langchain_community.embeddings\n",
    "    * from langchain_openai import OpenAIEmbeddings\n",
    "    * SentenceTransformer registries\n",
    "    * from langchain_community.embeddings import OllamaEmbeddings\n",
    "    * from langchain_community.embeddings import LlamaCppEmbeddings\n",
    "    * from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "    * from langchain_google_vertexai import VertexAIEmbeddings\n",
    "    * from langchain_community.embeddings import CohereEmbeddings\n",
    "    * from langchain_community.embeddings import QianfanEmbeddingsEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91dbe9d-0348-4b4e-865e-334b28b823cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # not free\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21982fcb-21fd-4c42-8166-7580142460d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fed7b8-0a4b-4158-a104-ce3dfe37d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import file_utils\n",
    "# print(file_utils.default_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a3e5c-1b34-4c6c-a3b3-1e4cc91a2bad",
   "metadata": {},
   "source": [
    "Model name | Provider | Model size (#pamras) | Model Size (disk) | Download past month | Highlights | Time Load/Inference | HF Link |\n",
    "--|--|--|--|--|--|--|--|\n",
    "intfloat/multilingual-e5-large | Microsoft | 560M | 2.2G | 93K |24 layers and the embedding size is 1024| 4.2s/55.2s |https://huggingface.co/intfloat/multilingual-e5-large|\n",
    "intfloat/multilingual-e5-base| Microsoft | 278M | 1.1G | 42K |12 layers and the embedding size is 768| 2.7s/19.6s| https://huggingface.co/intfloat/multilingual-e5-base|\n",
    "sentence-transformers/LaBSE | Google | | 1.9G | 88K | the embedding size is 768 | 4.7s/14.9s | https://huggingface.co/sentence-transformers/LaBSE|\n",
    "maidalun1020/bce-embedding-base_v1 | NetEase-Youdao |  279M | XG | 111K | optimized for RAG | 2.7s/20.5s | https://huggingface.co/maidalun1020/bce-embedding-base_v1\n",
    "BAAI/bge-large-zh-v1.5|Beijing Academy of Artificial Intelligence| 326M | 1.3G | 22K | | 1.4s/76.7s| https://huggingface.co/BAAI/bge-large-zh-v1.5#usage|\n",
    "uer/sbert-base-chinese-nli| Tencent | | 409M  | 8K | 12 layers and the embedding size is 768 | 0.6s/25.0s | https://huggingface.co/uer/sbert-base-chinese-nli |\n",
    "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2| Sentence Transformer | | 449M | 38K | 384 embedding size | 1.4s/11.8s | https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2 |\n",
    "sentence-transformers/distiluse-base-multilingual-cased-v1 | Sentence Transformer | | 539M | 31K | 768 embedding size | 1.2s/7.1s | https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1 |\n",
    "sentence-transformers/distiluse-base-multilingual-cased-v2 | Sentence Transformer | | 539M | 43K | 768 enbedding size | 1.2s/5.7s | https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2 |\n",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2 | Sentence Transformer | | 1.1G | 24K | 768 embedding size | 2.6s/11.0s | https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc75478-4dde-4ad1-b094-4c8f4710cd75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# validate GPU/Metal acceleration on Mac (no action requires, should be enabled with environment build)\n",
    "# https://developer.apple.com/metal/pytorch/\n",
    "# The output should show: tensor([1.], device='mps:0')\n",
    "\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85414a88-8c36-41fc-aa47-74500acb73c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all embeedings supported in sentence-transformers library\n",
    "# https://huggingface.co/models?library=sentence-transformers\n",
    "\n",
    "# cached model objects in ~/.cache/torch/sentence_transformers\n",
    "\n",
    "sentence_transformer_model_lists = [\n",
    "    # Microsoft\n",
    "    'intfloat/multilingual-e5-large',\n",
    "    'intfloat/multilingual-e5-base',\n",
    "\n",
    "    # Google\n",
    "    'sentence-transformers/LaBSE',\n",
    "\n",
    "    # Chinese companies\n",
    "    'maidalun1020/bce-embedding-base_v1', # this requires direct download through Git LFS from HF as it is gated https://huggingface.co/maidalun1020/bce-embedding-base_v1/tree/main\n",
    "    'BAAI/bge-large-zh-v1.5',\n",
    "    'uer/sbert-base-chinese-nli',\n",
    "\n",
    "    # Sentence Transformer native\n",
    "    # https://www.sbert.net/docs/pretrained_models.html#multi-lingual-models\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
    "    'sentence-transformers/distiluse-base-multilingual-cased-v1',\n",
    "    'sentence-transformers/distiluse-base-multilingual-cased-v2',\n",
    "    'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "]\n",
    "\n",
    "# # download the models\n",
    "# for model_name in sentence_transformer_model_lists:\n",
    "#     print(model_name)\n",
    "#     _ = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b82a9ce-3f38-4784-8cfc-cfa2efd642c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [x['q'] for x in test_cases] + [x['a'] for x in test_cases]\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3060b7bc-1610-44de-8fd3-b2688bd094cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:36:46.997\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprofile_docs\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mtotal Chinese characters: 3746, total docs: 74\u001b[0m\n",
      "\u001b[32m2024-01-20 22:36:46.998\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprofile_docs\u001b[0m:\u001b[36m7\u001b[0m - \u001b[1mMin/Mean/Max characters per doc: 2, 50.62, 522\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def profile_docs(docs: List[str]):\n",
    "    characters = [len(x) for x in docs]\n",
    "    len_docs = len(docs)\n",
    "    logger.info(f\"total Chinese characters: {sum(characters)}, total docs: {len_docs}\")\n",
    "    logger.info(f\"Min/Mean/Max characters per doc: {min(characters)}, {np.mean(characters):.2f}, {max(characters)}\")\n",
    "\n",
    "profile_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b0a706b-7213-45bb-a496-5b192b321ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '/Users/fred/.cache/torch/sentence_transformers/'\n",
    "\n",
    "def _get_model_path(model_dir, model_name):\n",
    "    return model_dir+model_name.replace('/', '_')+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4b2e58c-4d62-4263-98c7-2df63e608529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "import time\n",
    "\n",
    "def timer_embedding_model(model_name, sentences, model_dir) -> None:\n",
    "    time_start = time.time()\n",
    "    model = SentenceTransformer.load(_get_model_path(model_dir, model_name))\n",
    "    time_end = time.time()\n",
    "    logger.info(f'Time taken loading {model_name}: {time_end - time_start:.2f}s')\n",
    "    \n",
    "    time_start = time.time()\n",
    "    model.encode(sentences)\n",
    "    time_end = time.time()\n",
    "    logger.info(f\"Time taken for {model_name}: {time_end - time_start:.2f}s\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f2abacf-edf4-45e4-aa16-3c64df38837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:36:51.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading intfloat/multilingual-e5-large: 4.33s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:37:49.244\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for intfloat/multilingual-e5-large: 57.91s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:37:52.141\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading intfloat/multilingual-e5-base: 2.70s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:38:11.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for intfloat/multilingual-e5-base: 19.27s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:38:16.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading sentence-transformers/LaBSE: 4.59s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:38:31.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for sentence-transformers/LaBSE: 15.25s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:38:34.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading maidalun1020/bce-embedding-base_v1: 2.68s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:38:54.369\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for maidalun1020/bce-embedding-base_v1: 20.20s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:38:55.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading BAAI/bge-large-zh-v1.5: 1.41s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:40:13.134\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for BAAI/bge-large-zh-v1.5: 77.20s\u001b[0m\n",
      "No sentence-transformers model found with name /Users/fred/.cache/torch/sentence_transformers/uer_sbert-base-chinese-nli/. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:40:13.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading uer/sbert-base-chinese-nli: 0.66s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:40:38.920\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for uer/sbert-base-chinese-nli: 25.10s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:40:40.298\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 1.36s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:40:51.449\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2: 11.15s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:40:52.768\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading sentence-transformers/distiluse-base-multilingual-cased-v1: 1.16s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:40:58.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for sentence-transformers/distiluse-base-multilingual-cased-v1: 5.63s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:40:59.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading sentence-transformers/distiluse-base-multilingual-cased-v2: 1.17s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:41:05.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for sentence-transformers/distiluse-base-multilingual-cased-v2: 5.82s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:41:08.045\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m8\u001b[0m - \u001b[1mTime taken loading sentence-transformers/paraphrase-multilingual-mpnet-base-v2: 2.61s\u001b[0m\n",
      "\u001b[32m2024-01-20 22:41:19.547\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtimer_embedding_model\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mTime taken for sentence-transformers/paraphrase-multilingual-mpnet-base-v2: 11.50s\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the querying time for each embedding model using questions in the test cases\n",
    "\n",
    "for model in sentence_transformer_model_lists:\n",
    "    timer_embedding_model(model, docs, MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "96c6bcfc-7a93-4f59-83a5-009da69c6b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-01-20 22:59:13.936\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m26\u001b[0m - \u001b[1mTime taken to score by intfloat/multilingual-e5-large: 58.66s\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util as st_utils\n",
    "\n",
    "scores = dict()\n",
    "\n",
    "for model_name in sentence_transformer_model_lists:\n",
    "    model = SentenceTransformer.load(_get_model_path(MODEL_DIR, model_name))\n",
    "    score_list = list()\n",
    "    \n",
    "    time_start = time.time()\n",
    "    for qa in test_cases:\n",
    "        \n",
    "        query = qa['q']\n",
    "        docs = qa['ref']\n",
    "\n",
    "        # skip if empty records found\n",
    "        if query == \"\" or docs == []:\n",
    "            continue\n",
    "        \n",
    "        docs_embeddings = model.encode(docs, convert_to_tensor=True)\n",
    "        query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        cos_scores = st_utils.cos_sim(query_embedding, docs_embeddings)[0] # cos_sim is used in multi-dim computation, so it returns nested result [[]]\n",
    "        score_list.append(cos_scores.numpy())\n",
    "\n",
    "    time_end = time.time()\n",
    "    logger.info(f'Time taken to score by {model_name}: {time_end-time_start:.2f}s')\n",
    "    \n",
    "    scores[model_name] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f7574f-4e80-49b0-a33b-179fc8cfc2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
