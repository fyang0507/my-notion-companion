{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687426f4-14b4-4d40-97f9-447ac9baf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a071a8-cac2-470f-8d96-f73feb1708d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078e0401-7050-4f98-b60c-d2bb2d330a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebdc5df-377d-4463-943e-218bd6402c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a6ba61d-e198-4ccb-af9d-0ce45515b488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/fred/Documents/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4095.06 MiB, ( 4095.12 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4095.05 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   512.00 MiB, ( 4608.69 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    80.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1184.02 MiB, ( 5792.70 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  1184.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   314.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 4\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'huggingfaceh4_zephyr-7b-beta'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+_CONFIGS['model_mapping'][_CONFIGS['model_name']],\n",
    "    name=_CONFIGS['model_name'], \n",
    "    **_CONFIGS['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0861913d-350e-4b76-a391-0249866dd0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-03-07 20:35:08.145\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.document_filter\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mSetting metadata fuzzy match threshold to: 0.8.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from my_notion_companion.notion_chatbot import NotionChatBot\n",
    "\n",
    "c = NotionChatBot(llm, '../.config.toml', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66a8c113-3c9b-4962-ae71-ea72bf6a487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       1.67 ms /    20 runs   (    0.08 ms per token, 12004.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4181.32 ms /   903 tokens (    4.63 ms per token,   215.96 tokens per second)\n",
      "llama_print_timings:        eval time =     665.90 ms /    19 runs   (   35.05 ms per token,    28.53 tokens per second)\n",
      "llama_print_timings:       total time =    4878.51 ms /   922 tokens\n",
      "\u001b[32m2024-03-07 20:35:14.104\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.query_analyzer\u001b[0m:\u001b[36mparse_output\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1m\n",
      "Query Analyzer output\n",
      "keyword: ['寡人之于国也']\n",
      "search domains:['无']\u001b[0m\n",
      "\u001b[32m2024-03-07 20:35:14.105\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mretriever\u001b[0m:\u001b[36m_filter_documents\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mNo filters found by query analyzer.\u001b[0m\n",
      "\u001b[32m2024-03-07 20:35:15.107\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmy_notion_companion.notion_chatbot\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1m{'author': None, 'tags': '古文', 'name': '高中古诗文', 'id': '0c77a756-a81d-486d-83de-2de6fb7e9e5d', 'source': '读书笔记（文学）', 'date_start': 20130812}\n",
      "国之父兄请曰：“昔者夫差耻吾君于诸侯之国，今越国亦节（克制）...\n",
      "------------------------------\n",
      "{'author': None, 'tags': '古文', 'name': '高中古诗文', 'id': '0c77a756-a81d-486d-83de-2de6fb7e9e5d', 'source': '读书笔记（文学）', 'date_start': 20130812}\n",
      "《寡人之于国也》（孟子）\n",
      "梁惠王曰：“寡人之于国也，尽心焉耳...\n",
      "------------------------------\n",
      "{'author': None, 'tags': '古文', 'name': '高中古诗文', 'id': '0c77a756-a81d-486d-83de-2de6fb7e9e5d', 'source': '读书笔记（文学）', 'date_start': 20130812}\n",
      "勾践之地，南至于句无，北至于御儿，东至于鄞（yín），西至于...\n",
      "------------------------------\n",
      "{'author': None, 'tags': '古文', 'name': '高中古诗文', 'id': '0c77a756-a81d-486d-83de-2de6fb7e9e5d', 'source': '读书笔记（文学）', 'date_start': 20130812}\n",
      "《勾践灭吴》（国语）\n",
      "越王勾践栖于会稽之上，乃号令于三军曰：...\n",
      "------------------------------\n",
      "{'author': None, 'tags': '古文', 'name': '高中古诗文', 'id': '0c77a756-a81d-486d-83de-2de6fb7e9e5d', 'source': '读书笔记（文学）', 'date_start': 20130812}\n",
      "高一上\n",
      "课文\n",
      "\n",
      "《烛之武退秦师》（左传）\n",
      "晋侯、秦伯围郑，以...\n",
      "------------------------------\n",
      "\u001b[0m\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       5.05 ms /    57 runs   (    0.09 ms per token, 11291.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15077.29 ms /  3317 tokens (    4.55 ms per token,   220.00 tokens per second)\n",
      "llama_print_timings:        eval time =    2569.69 ms /    56 runs   (   45.89 ms per token,    21.79 tokens per second)\n",
      "llama_print_timings:       total time =   17758.96 ms /  3373 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '\"尽心焉耳矣\" 这是 \"寡人之于国也\" 这段文字的结尾语句，来自《孟子》的《高处论》第二十章。'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"“寡人之于国也”下一句是什么？来自哪里？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef775ce0-23c9-46e3-bf29-ac3fa02bc532",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =      13.38 ms /   149 runs   (    0.09 ms per token, 11132.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     487.97 ms /    38 tokens (   12.84 ms per token,    77.87 tokens per second)\n",
      "llama_print_timings:        eval time =    6891.15 ms /   148 runs   (   46.56 ms per token,    21.48 tokens per second)\n",
      "llama_print_timings:       total time =    7659.87 ms /   186 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '晋侯、秦伯围郑，以其无礼于晋，且贰于楚也。晋军函陵，秦军氾（fán）南。佚（yì）之狐言于郑伯曰：“国危矣，若使烛之武见秦君，师必退。”公从之。辞曰：“臣之壮也，犹不如人；今老矣，无能为也已。”'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"背诵《烛之武退秦师》前5句话。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d06323c7-8216-435b-8fce-0341e140511b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       9.95 ms /   113 runs   (    0.09 ms per token, 11361.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     334.72 ms /    23 tokens (   14.55 ms per token,    68.71 tokens per second)\n",
      "llama_print_timings:        eval time =    5277.06 ms /   112 runs   (   47.12 ms per token,    21.22 tokens per second)\n",
      "llama_print_timings:       total time =    5828.31 ms /   135 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '夜缒（zhuì）而出，见秦伯，曰：“秦、晋围郑，郑既知亡矣。若亡郑而有益于君，敢以烦执事。” 这就是后面三句话，来自《左传》的《秦春秋》第二十八章的《烛之武退秦师》。'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"后面三句呢？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddb796c6-963d-4d81-8d9f-dda803efd9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       4.68 ms /    55 runs   (    0.09 ms per token, 11749.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3644.25 ms /   729 tokens (    5.00 ms per token,   200.04 tokens per second)\n",
      "llama_print_timings:        eval time =    2644.11 ms /    54 runs   (   48.97 ms per token,    20.42 tokens per second)\n",
      "llama_print_timings:       total time =    6400.61 ms /   783 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '根据提供的文档，我可以看出有四段落来自读书笔记（文学），其中包括《高中古诗文》这个标签下的四篇文章。'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"文档中有多少段落来自读书笔记？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1327a79-b19f-497a-a531-7e91b664f39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       3.73 ms /    43 runs   (    0.09 ms per token, 11518.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3164.93 ms /   629 tokens (    5.03 ms per token,   198.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2024.03 ms /    42 runs   (   48.19 ms per token,    20.75 tokens per second)\n",
      "llama_print_timings:       total time =    5270.71 ms /   671 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant', 'content': '根据提供的文档中的元数据，我可以看出所有四篇文章都创建在2013年8月12日上午。'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"这些文档创建的时间是什么时候？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3015033e-f53f-4d7b-a356-e930340573ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =       8.98 ms /   103 runs   (    0.09 ms per token, 11468.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2849.59 ms /   559 tokens (    5.10 ms per token,   196.17 tokens per second)\n",
      "llama_print_timings:        eval time =    4917.83 ms /   102 runs   (   48.21 ms per token,    20.74 tokens per second)\n",
      "llama_print_timings:       total time =    7960.13 ms /   661 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '我是一个智能语言处理模型，我可以帮助您回答您提出的问题并为您提供相应的信息。我不具有身体或者感知能力，我只能通过语言来与您交互。我不能为您完成任何实际操作或者为您提供任何物质帮助，我只能为您提供信息和建议。'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"你是谁？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa94db36-0703-4ba8-a0de-361cc77c992c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9497.49 ms\n",
      "llama_print_timings:      sample time =      13.75 ms /   158 runs   (    0.09 ms per token, 11487.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2749.28 ms /   522 tokens (    5.27 ms per token,   189.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7567.45 ms /   157 runs   (   48.20 ms per token,    20.75 tokens per second)\n",
      "llama_print_timings:       total time =   10623.00 ms /   679 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': '如果您是在唱或者引述《孤帆远影》这首歌曲，那么下一句可能是：\"孤帆远影碧空尽，烟雾茫茫中，我只能等待，直到我的心肝荡然去\"。然而，如果您是在引述其他文字或者作品中的这段语句，那么下一句可能会完全不同。请提供更多上下文或者完整的句子，以帮助我更好地理解并回答您的问题。'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"孤帆远影碧空尽下一句是？\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
