{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2228b50e-bde6-4811-90d1-3e5ffba0020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eabb57f-21d8-4925-a7f6-ac5a797cc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c016251c-280c-4f27-9241-0a59f3e3a7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef978eb-ad78-4ef6-bc74-d28f626807fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 387 tensors from /Users/fred/Documents/models/qwen1_5-7b-chat-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.20 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3963.36 MiB, ( 3963.42 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4297.21 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3963.36 MiB\n",
      "...................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2048.00 MiB, ( 6012.98 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    80.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1120.02 MiB, ( 7133.00 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  1120.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1251.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 4\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '2', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '4096', 'qwen2.attention.head_count_kv': '32', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '32', 'general.architecture': 'qwen2', 'qwen2.block_count': '32', 'qwen2.feed_forward_length': '11008', 'general.name': 'Qwen1.5-7B-Chat-AWQ-fp16'}\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import tomllib\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)\n",
    "\n",
    "with open('../.tokens.toml', 'rb') as f:\n",
    "    _TOKENS = tomllib.load(f)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+_CONFIGS['model_mapping'][_CONFIGS['model_name']],\n",
    "    name=_CONFIGS['model_name'], \n",
    "    **_CONFIGS['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa3445c7-b4da-4287-9fda-8ae8f84764a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    _CONFIGS[\"model_name\"], trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7286314-0821-4a9d-a34c-0caf800a0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open(\"../templates/query_constructor_template.toml\", \"rb\") as f:\n",
    "    templates = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f01ac4-9fd2-4e9b-9a3c-cf85e2368ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_notion_companion.few_shot_constructor import FewShotTemplateConstructor\n",
    "\n",
    "few_shot = FewShotTemplateConstructor(tokenizer, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2403e90-924b-4b49-a8e5-ab0dbbe68015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nYou are a helpful AI assistant, your only role is to find the keywords from a given text. These keywords will be used for subsequent search in a search engine. DO NOT answer any questions. Only extract keywords. Please strictly follow the response pattern from conversation history.\\n<|im_end|>\\n<|im_start|>user\\n<< text >>\\n中华人民共和国成立时间是?<|im_end|>\\n<|im_start|>assistant\\n<< search keywords >>\\n中华人民共和国成立时间<|im_end|>\\n<|im_start|>user\\n<< text >>\\n李白和杜甫相识吗？他们是怎么认识的？请从资料库中找到答案。<|im_end|>\\n<|im_start|>assistant\\n<< search keywords >>\\n李白和杜甫的相识<|im_end|>\\n<|im_start|>user\\n<< text >>\\n“鹅鹅鹅，曲项向天歌”是哪位诗人所作？何时所作？请只给出诗人名字。<|im_end|>\\n<|im_start|>assistant\\n<< search keywords >>\\n鹅鹅鹅，曲项向天歌<|im_end|>\\n<|im_start|>user\\n<< text >>\\n新裤子乐队最出名的专辑是什么时候发售的？请简单介绍该乐队。<|im_end|>\\n<|im_start|>assistant\\n<< search keywords >>\\n新裤子乐队<|im_end|>\\n<|im_start|>user\\n<< text >>\\n新裤子乐队最出名的专辑是什么时候发售的？请简单介绍该乐队。<|im_end|>\\n<|im_start|>assistant\\n<< search keywords >>\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = few_shot.invoke(\"新裤子乐队最出名的专辑是什么时候发售的？请简单介绍该乐队。\")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab912a08-108a-4409-8ad0-e07b8fc2be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "c = RunnableLambda(few_shot.invoke) | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0048714d-713e-4dbe-9cfa-38f9f9289038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      11.32 ms /    34 runs   (    0.33 ms per token,  3003.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3153.73 ms /   303 tokens (   10.41 ms per token,    96.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1223.03 ms /    33 runs   (   37.06 ms per token,    26.98 tokens per second)\n",
      "llama_print_timings:       total time =    4606.74 ms /   336 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'鹅鹅鹅，曲项向天歌\\n\\n诗人名字：李白\\n\\n时间：唐朝，具体创作时间无法确定，因为这是李白的代表作之一。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.invoke(\"“鹅鹅鹅，曲项向天歌”是哪位诗人所作？何时所作？请只给出诗人名字。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e4361c0-bfcc-4814-a195-09c940365e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'q': '什么是我国第一部编年史著作？', 'a': '《左传》。', 'docs': ['附：《左传》是我国第一部编年史著作。\\n']},\n",
       " {'q': '什么是我国第一部编年国别史？', 'a': '《国语》。', 'docs': ['附：《国语》是我国第一部编年国别史。\\n']},\n",
       " {'q': '“寡人之于国也”下一句是什么？来自哪里？',\n",
       "  'a': '“寡人之于国也”下一句是“尽心焉耳矣”。这个句子来自《孟子》。',\n",
       "  'docs': ['梁惠王曰：“寡人之于国也，尽心焉耳矣。河内凶，则移其民于河东，移其粟于河内；河东凶亦然。察邻国之政，无如寡人之用心者。邻国之民不加少，寡人之民不加多，何也？”',\n",
       "   '《寡人之于国也》（孟子）\\n']}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import load_test_cases\n",
    "\n",
    "test_cases = load_test_cases('../data/test_cases.txt')\n",
    "test_cases[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86928eb7-5873-4dec-a759-04ff6a5f6707",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from my_notion_companion.query_constructor import QueryConstructor\n",
    "\n",
    "qc = QueryConstructor(llm, _CONFIGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5821494e-d12b-46be-b8d4-73103a3eefa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       2.49 ms /     7 runs   (    0.36 ms per token,  2811.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3952.80 ms /   325 tokens (   12.16 ms per token,    82.22 tokens per second)\n",
      "llama_print_timings:        eval time =     202.33 ms /     6 runs   (   33.72 ms per token,    29.65 tokens per second)\n",
      "llama_print_timings:       total time =    4211.21 ms /   331 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是我国第一部编年史著作？\n",
      "我国第一部编年史著作\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       2.98 ms /     9 runs   (    0.33 ms per token,  3017.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     255.51 ms /    25 tokens (   10.22 ms per token,    97.84 tokens per second)\n",
      "llama_print_timings:        eval time =     267.79 ms /     8 runs   (   33.47 ms per token,    29.87 tokens per second)\n",
      "llama_print_timings:       total time =     581.69 ms /    33 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "什么是我国第一部编年国别史？\n",
      "我国第一部编年国别史著作\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      14.96 ms /    45 runs   (    0.33 ms per token,  3008.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.25 ms /    31 tokens (    9.43 ms per token,   106.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1467.44 ms /    44 runs   (   33.35 ms per token,    29.98 tokens per second)\n",
      "llama_print_timings:       total time =    2051.67 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“寡人之于国也”下一句是什么？来自哪里？\n",
      "“寡人之于国也”下一句是“尽心焉耳矣”。这句话出自《孟子·梁惠王上》。\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      46.32 ms /   140 runs   (    0.33 ms per token,  3022.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     431.98 ms /    36 tokens (   12.00 ms per token,    83.34 tokens per second)\n",
      "llama_print_timings:        eval time =    4693.53 ms /   139 runs   (   33.77 ms per token,    29.62 tokens per second)\n",
      "llama_print_timings:       total time =    6012.78 ms /   175 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "庄子写过哪些文章？请根据“高中古诗文”中收录的作品回答。\n",
      "庄子文章\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       1.98 ms /     6 runs   (    0.33 ms per token,  3034.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =     428.78 ms /    33 tokens (   12.99 ms per token,    76.96 tokens per second)\n",
      "llama_print_timings:        eval time =     170.41 ms /     5 runs   (   34.08 ms per token,    29.34 tokens per second)\n",
      "llama_print_timings:       total time =     636.29 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“一陂春水绕花身”中“陂”的读音是什么？\n",
      "陂 (pí)\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      31.15 ms /    92 runs   (    0.34 ms per token,  2953.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     421.52 ms /    45 tokens (    9.37 ms per token,   106.76 tokens per second)\n",
      "llama_print_timings:        eval time =    3097.47 ms /    91 runs   (   34.04 ms per token,    29.38 tokens per second)\n",
      "llama_print_timings:       total time =    4088.62 ms /   136 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "补充对联：“此地有崇山峻岭茂林修竹”，请根据“高中古诗文”中收录的作品回答。\n",
      "崇山峻岭 茂林修竹\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      22.47 ms /    68 runs   (    0.33 ms per token,  3026.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     253.52 ms /    24 tokens (   10.56 ms per token,    94.67 tokens per second)\n",
      "llama_print_timings:        eval time =    2277.55 ms /    67 runs   (   33.99 ms per token,    29.42 tokens per second)\n",
      "llama_print_timings:       total time =    2947.39 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苏轼是如何评价韩愈的？\n",
      "苏轼评价韩愈\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       2.35 ms /     7 runs   (    0.34 ms per token,  2981.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =     277.93 ms /    28 tokens (    9.93 ms per token,   100.75 tokens per second)\n",
      "llama_print_timings:        eval time =     214.78 ms /     6 runs   (   35.80 ms per token,    27.94 tokens per second)\n",
      "llama_print_timings:       total time =     537.13 ms /    34 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“治大国若烹小鲜”是谁说的？\n",
      "治大国若烹小鲜\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       3.06 ms /     9 runs   (    0.34 ms per token,  2945.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     280.47 ms /    29 tokens (    9.67 ms per token,   103.40 tokens per second)\n",
      "llama_print_timings:        eval time =     271.31 ms /     8 runs   (   33.91 ms per token,    29.49 tokens per second)\n",
      "llama_print_timings:       total time =     606.70 ms /    37 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "据推测，《卫风•氓》是否没有经过修改？\n",
      "《卫风•氓》修改情况\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       6.27 ms /    19 runs   (    0.33 ms per token,  3028.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     263.45 ms /    26 tokens (   10.13 ms per token,    98.69 tokens per second)\n",
      "llama_print_timings:        eval time =     620.26 ms /    18 runs   (   34.46 ms per token,    29.02 tokens per second)\n",
      "llama_print_timings:       total time =    1000.88 ms /    44 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《短歌行》中前两句是什么？\n",
      "《短歌行》前两句\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       3.87 ms /    12 runs   (    0.32 ms per token,  3103.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     447.02 ms /    36 tokens (   12.42 ms per token,    80.53 tokens per second)\n",
      "llama_print_timings:        eval time =     383.24 ms /    11 runs   (   34.84 ms per token,    28.70 tokens per second)\n",
      "llama_print_timings:       total time =     906.78 ms /    47 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《春江花月夜》收录在哪里？请根据“高中古诗文”回答。\n",
      "《春江花月夜》高中古诗文\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      19.57 ms /    58 runs   (    0.34 ms per token,  2963.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =     420.86 ms /    34 tokens (   12.38 ms per token,    80.79 tokens per second)\n",
      "llama_print_timings:        eval time =    1965.47 ms /    57 runs   (   34.48 ms per token,    29.00 tokens per second)\n",
      "llama_print_timings:       total time =    2753.48 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《黄州快哉亭记》中宋玉是怎么讽刺楚襄王的？\n",
      "宋玉讽刺楚襄王的场景在《黄州快哉亭记》中。宋玉通过描述楚王游乐的“快哉亭”周围景色，暗中批评楚王的奢侈游乐没有实质意义，是对楚襄王的一种委婉的讽刺。\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      14.31 ms /    42 runs   (    0.34 ms per token,  2935.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =     273.71 ms /    28 tokens (    9.78 ms per token,   102.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1416.63 ms /    41 runs   (   34.55 ms per token,    28.94 tokens per second)\n",
      "llama_print_timings:       total time =    1949.27 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李白针对《逍遥游》写过一首什么诗？\n",
      "李白 逍遥游\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       2.02 ms /     6 runs   (    0.34 ms per token,  2967.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =     451.65 ms /    40 tokens (   11.29 ms per token,    88.56 tokens per second)\n",
      "llama_print_timings:        eval time =     176.25 ms /     5 runs   (   35.25 ms per token,    28.37 tokens per second)\n",
      "llama_print_timings:       total time =     665.14 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哪一本小说谈到了“美的一次性”的话题？请根据“读书笔记（文学）”回答。\n",
      "美的一次性小说\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      23.57 ms /    71 runs   (    0.33 ms per token,  3012.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     424.91 ms /    40 tokens (   10.62 ms per token,    94.14 tokens per second)\n",
      "llama_print_timings:        eval time =    2451.56 ms /    70 runs   (   35.02 ms per token,    28.55 tokens per second)\n",
      "llama_print_timings:       total time =    3332.90 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“我们之所以突然变得残暴”的后面是什么？请从三岛由纪夫的小说中寻找答案。\n",
      "我们之所以突然变得残暴，后面是什么\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       8.87 ms /    27 runs   (    0.33 ms per token,  3045.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =     465.36 ms /    43 tokens (   10.82 ms per token,    92.40 tokens per second)\n",
      "llama_print_timings:        eval time =     918.46 ms /    26 runs   (   35.33 ms per token,    28.31 tokens per second)\n",
      "llama_print_timings:       total time =    1552.01 ms /    69 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\n",
      "每个人都以为他自己至少有一种主要的美德\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      21.37 ms /    65 runs   (    0.33 ms per token,  3041.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     426.22 ms /    46 tokens (    9.27 ms per token,   107.93 tokens per second)\n",
      "llama_print_timings:        eval time =    2261.49 ms /    64 runs   (   35.34 ms per token,    28.30 tokens per second)\n",
      "llama_print_timings:       total time =    3119.39 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "为什么钱钟书说“忠厚老实人的恶毒，像饭里的沙砾或者出骨鱼片里未净的刺”？\n",
      "钱钟书 对忠厚老实人的评价\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      22.64 ms /    68 runs   (    0.33 ms per token,  3004.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =     578.86 ms /    47 tokens (   12.32 ms per token,    81.19 tokens per second)\n",
      "llama_print_timings:        eval time =    2374.14 ms /    67 runs   (   35.43 ms per token,    28.22 tokens per second)\n",
      "llama_print_timings:       total time =    3380.34 ms /   114 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是什么让昆德拉说“我要对你说出我一生中最悲愁的发现：受迫害者并不比迫害者更高贵”？\n",
      "昆德拉 受迫害者 平等高贵\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       1.33 ms /     4 runs   (    0.33 ms per token,  3003.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     274.72 ms /    28 tokens (    9.81 ms per token,   101.92 tokens per second)\n",
      "llama_print_timings:        eval time =     109.51 ms /     3 runs   (   36.50 ms per token,    27.39 tokens per second)\n",
      "llama_print_timings:       total time =     409.48 ms /    31 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "哪本小说中提到了“细密画”？\n",
      "细密画\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       3.64 ms /    11 runs   (    0.33 ms per token,  3024.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =     441.59 ms /    41 tokens (   10.77 ms per token,    92.85 tokens per second)\n",
      "llama_print_timings:        eval time =     357.41 ms /    10 runs   (   35.74 ms per token,    27.98 tokens per second)\n",
      "llama_print_timings:       total time =     867.30 ms /    51 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请补充“火车代表黑夜与毁灭”的后一句。请从“读书笔记（文学）”中找到答案。\n",
      "火车代表黑夜与毁灭，后一句是什么？\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       2.30 ms /     7 runs   (    0.33 ms per token,  3042.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     269.60 ms /    27 tokens (    9.99 ms per token,   100.15 tokens per second)\n",
      "llama_print_timings:        eval time =     213.48 ms /     6 runs   (   35.58 ms per token,    28.11 tokens per second)\n",
      "llama_print_timings:       total time =     528.09 ms /    33 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "《天使与昆虫》是哪位作家的作品？\n",
      "《天使与昆虫》作家\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       3.65 ms /    11 runs   (    0.33 ms per token,  3011.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     425.24 ms /    33 tokens (   12.89 ms per token,    77.60 tokens per second)\n",
      "llama_print_timings:        eval time =     365.47 ms /    10 runs   (   36.55 ms per token,    27.36 tokens per second)\n",
      "llama_print_timings:       total time =     861.28 ms /    43 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请从辛波斯卡的诗歌中找出一首以“雪”为主题的。\n",
      "辛波斯卡 雪 主题诗歌\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       5.70 ms /    17 runs   (    0.34 ms per token,  2985.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =     282.37 ms /    29 tokens (    9.74 ms per token,   102.70 tokens per second)\n",
      "llama_print_timings:        eval time =     576.91 ms /    16 runs   (   36.06 ms per token,    27.73 tokens per second)\n",
      "llama_print_timings:       total time =     968.18 ms /    45 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请补充完整以下句子：“铁如意，指挥倜傥”。\n",
      "铁如意，指挥倜傥\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       1.62 ms /     5 runs   (    0.32 ms per token,  3080.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =     258.41 ms /    25 tokens (   10.34 ms per token,    96.74 tokens per second)\n",
      "llama_print_timings:        eval time =     151.85 ms /     4 runs   (   37.96 ms per token,    26.34 tokens per second)\n",
      "llama_print_timings:       total time =     442.46 ms /    29 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "人生有哪“四不捡”？\n",
      "人生四不捡\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      30.76 ms /    94 runs   (    0.33 ms per token,  3055.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     295.31 ms /    31 tokens (    9.53 ms per token,   104.97 tokens per second)\n",
      "llama_print_timings:        eval time =    3385.39 ms /    93 runs   (   36.40 ms per token,    27.47 tokens per second)\n",
      "llama_print_timings:       total time =    4278.19 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请全文摘录海子的《春天，十个海子》。\n",
      "海子 春天，十个海子\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      12.86 ms /    39 runs   (    0.33 ms per token,  3033.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     264.79 ms /    26 tokens (   10.18 ms per token,    98.19 tokens per second)\n",
      "llama_print_timings:        eval time =    1381.44 ms /    38 runs   (   36.35 ms per token,    27.51 tokens per second)\n",
      "llama_print_timings:       total time =    1887.48 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本多是哪本小说中出现的人物？\n",
      "本多\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      14.35 ms /    43 runs   (    0.33 ms per token,  2996.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =     421.44 ms /    33 tokens (   12.77 ms per token,    78.30 tokens per second)\n",
      "llama_print_timings:        eval time =    1534.66 ms /    42 runs   (   36.54 ms per token,    27.37 tokens per second)\n",
      "llama_print_timings:       total time =    2226.71 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“接受神的恩惠降生的人”下一句是什么？来自哪里？\n",
      "接受神的恩惠降生的人 下一句是什么\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      17.34 ms /    53 runs   (    0.33 ms per token,  3056.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.38 ms /    31 tokens (    9.43 ms per token,   106.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1902.76 ms /    52 runs   (   36.59 ms per token,    27.33 tokens per second)\n",
      "llama_print_timings:       total time =    2528.86 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“西伯利亚癔病”是哪本小说中出现的概念？\n",
      "西伯利亚癔病\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      12.83 ms /    39 runs   (    0.33 ms per token,  3040.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     427.68 ms /    41 tokens (   10.43 ms per token,    95.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1406.28 ms /    38 runs   (   37.01 ms per token,    27.02 tokens per second)\n",
      "llama_print_timings:       total time =    2080.19 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请列举一个关于“深秋初冬”的描写片段。请从“读书笔记（文学）”中寻找。\n",
      "深秋初冬\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      17.09 ms /    50 runs   (    0.34 ms per token,  2926.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     306.19 ms /    32 tokens (    9.57 ms per token,   104.51 tokens per second)\n",
      "llama_print_timings:        eval time =    1844.72 ms /    49 runs   (   37.65 ms per token,    26.56 tokens per second)\n",
      "llama_print_timings:       total time =    2492.40 ms /    81 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请摘录“猛忆西湖，当年一梦难忘”的全诗词。\n",
      "猛忆西湖，当年一梦难忘\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      55.60 ms /   166 runs   (    0.33 ms per token,  2985.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =     333.97 ms /    30 tokens (   11.13 ms per token,    89.83 tokens per second)\n",
      "llama_print_timings:        eval time =    6176.53 ms /   165 runs   (   37.43 ms per token,    26.71 tokens per second)\n",
      "llama_print_timings:       total time =    7599.50 ms /   195 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“胁肩媚笑，病于夏畦”是什么意思？\n",
      "胁肩媚笑，病于夏畦\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =       6.21 ms /    19 runs   (    0.33 ms per token,  3058.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1027.93 ms /    35 tokens (   29.37 ms per token,    34.05 tokens per second)\n",
      "llama_print_timings:        eval time =     673.53 ms /    18 runs   (   37.42 ms per token,    26.72 tokens per second)\n",
      "llama_print_timings:       total time =    1820.88 ms /    53 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在《道连·葛雷的画像》中王尔德是如何形容英国人的。\n",
      "《道连·葛雷的画像》 王尔德 英国人形容\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      16.62 ms /    50 runs   (    0.33 ms per token,  3008.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =     270.63 ms /    27 tokens (   10.02 ms per token,    99.77 tokens per second)\n",
      "llama_print_timings:        eval time =    1825.33 ms /    49 runs   (   37.25 ms per token,    26.84 tokens per second)\n",
      "llama_print_timings:       total time =    2407.95 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“子非吾友也”的出处是哪里？\n",
      "子非吾友也\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      19.42 ms /    58 runs   (    0.33 ms per token,  2986.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     259.64 ms /    25 tokens (   10.39 ms per token,    96.29 tokens per second)\n",
      "llama_print_timings:        eval time =    2132.05 ms /    57 runs   (   37.40 ms per token,    26.73 tokens per second)\n",
      "llama_print_timings:       total time =    2756.77 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "昆德拉是否有关于纽约的描写？\n",
      "昆德拉 纽约描写\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      17.79 ms /    54 runs   (    0.33 ms per token,  3034.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     437.11 ms /    45 tokens (    9.71 ms per token,   102.95 tokens per second)\n",
      "llama_print_timings:        eval time =    1996.46 ms /    53 runs   (   37.67 ms per token,    26.55 tokens per second)\n",
      "llama_print_timings:       total time =    2772.63 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“我相信某一天，在某个人的生命中，他头顶天空中所有云朵的形状将会片刻地与他相似。”出自哪里？\n",
      "我相信某一天，在某个人的生命中，他头顶天空中所有云朵的形状将会片刻地与他相似。\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      16.26 ms /    49 runs   (    0.33 ms per token,  3013.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     434.26 ms /    45 tokens (    9.65 ms per token,   103.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1828.43 ms /    48 runs   (   38.09 ms per token,    26.25 tokens per second)\n",
      "llama_print_timings:       total time =    2573.50 ms /    93 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有一个诗人曾经写过类似“有人把国家带进沟里，但这些人说统治是一门艺术”的话。请找到出处。\n",
      "有人把国家带进沟里，但这些人说统治是一门艺术\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    3154.48 ms\n",
      "llama_print_timings:      sample time =      63.73 ms /   191 runs   (    0.33 ms per token,  2996.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     430.88 ms /    37 tokens (   11.65 ms per token,    85.87 tokens per second)\n",
      "llama_print_timings:        eval time =    7267.34 ms /   190 runs   (   38.25 ms per token,    26.14 tokens per second)\n",
      "llama_print_timings:       total time =    8925.65 ms /   227 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "心学和理学的区别是什么？请从《中国哲学简史》中找到相关资料。\n",
      "心学 理学 区别 《中国哲学简史》\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for case in test_cases:\n",
    "    r = qc.invoke(case['q'])\n",
    "    print(case['q'])\n",
    "    print(r)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3abc70-7406-4fca-bc20-f0b781f4aa55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
