{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152891d4-9bb8-4cc1-abf1-3207a625cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "140baf02-574d-4a1f-adea-1c4e58364026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2572370-bc5b-48d9-ad92-5c383595915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../.tokens.toml', 'rb') as f:\n",
    "    _TOKENS = tomllib.load(f)\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183ddf4a-ae95-4d76-9bc3-a2e49dcffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=_TOKENS['huggingface'], \n",
    "    model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    ")\n",
    "\n",
    "vs_chroma = Chroma(persist_directory='../database/vs_chroma', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1aea029-97f6-4474-8a48-c4f4133dafc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # chroma applies filter before semantic sesarch\n",
    "# vs_chroma.similarity_search_with_score(\n",
    "#     '谁说过陌生贵己？', \n",
    "#     filter={\n",
    "#         'author': '【中】冯友兰',\n",
    "#     },\n",
    "#     k=2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b18bcdc6-6c77-4357-b1b3-27e1e6138d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author', 'date_end', 'date_start', 'id', 'name', 'source', 'tags'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = vs_chroma.get(include=[\"metadatas\"])\n",
    "\n",
    "metadata_set = set()\n",
    "\n",
    "for x in metadata['metadatas']:\n",
    "    metadata_set = metadata_set.union(list(x.keys()))\n",
    "\n",
    "metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca8535b6-429d-4092-98e8-933034489267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': {'description': '本篇文章的作者', 'type': 'string'},\n",
       " 'date_start': {'description': '文章被创建的时间，格式是YYYY-MM-DD', 'type': 'string'},\n",
       " 'date_end': {'description': '文章被完成的时间，格式是YYYY-MM-DD', 'type': 'string'},\n",
       " 'id': {'description': '文章的id', 'type': 'string'},\n",
       " 'name': {'description': '文章的名字', 'type': 'string'},\n",
       " 'source': {'description': '文章的来源，这里的文章取自若干不同数据库', 'type': 'string'},\n",
       " 'tags': {'description': '文章的标签，可能代表它的风格、题材、来源，或者系列', 'type': 'string'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = _CONFIGS['attributes']\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd28f7e-1891-4218-b1dd-2aef112e848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure there's no more undocumented metadata\n",
    "assert metadata_set.union(metadata.keys()) == metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e371368b-92d2-47ad-abf8-4155aacfa2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author', 'date_end', 'date_start', 'id', 'name', 'source', 'tags'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6da61c-8dcf-4c01-b8a0-e84b5db43332",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 387 tensors from /Users/fred/Documents/models/qwen1_5-7b-chat-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llama_model_loader: - kv   2:                          qwen2.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     qwen2.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  qwen2.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 qwen2.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              qwen2.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv   9:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  10:                qwen2.use_parallel_residual bool             = true\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  13:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  18:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 2\n",
      "llama_model_loader: - type  f32:  161 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.20 GiB (4.67 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen1.5-7B-Chat-AWQ-fp16\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   108.66 MiB, (  108.72 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4297.21 MiB\n",
      "llm_load_tensors:      Metal buffer size =   108.66 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1984.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    64.00 MiB, (  174.28 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.02 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  174.30 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   316.81 MiB, (  491.09 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   316.80 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   335.23 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.file_type': '2', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.bos_token_id': '151643', 'tokenizer.ggml.padding_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151645', 'tokenizer.ggml.model': 'gpt2', 'qwen2.use_parallel_residual': 'true', 'qwen2.rope.freq_base': '1000000.000000', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'qwen2.embedding_length': '4096', 'qwen2.attention.head_count_kv': '32', 'qwen2.context_length': '32768', 'qwen2.attention.head_count': '32', 'general.architecture': 'qwen2', 'qwen2.block_count': '32', 'qwen2.feed_forward_length': '11008', 'general.name': 'Qwen1.5-7B-Chat-AWQ-fp16'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+'qwen1_5-7b-chat-q4_0.gguf',\n",
    "    name='Qwen/Qwen1.5-7B-Chat', \n",
    "    **_CONFIGS['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60d99d07-c280-445f-97f3-7fb570f2a9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AttributeInfo(name='author', description='本篇文章的作者', type='string'),\n",
       " AttributeInfo(name='date_start', description='文章被创建的时间，格式是YYYY-MM-DD', type='string'),\n",
       " AttributeInfo(name='date_end', description='文章被完成的时间，格式是YYYY-MM-DD', type='string'),\n",
       " AttributeInfo(name='id', description='文章的id', type='string'),\n",
       " AttributeInfo(name='name', description='文章的名字', type='string'),\n",
       " AttributeInfo(name='source', description='文章的来源，这里的文章取自若干不同数据库', type='string'),\n",
       " AttributeInfo(name='tags', description='文章的标签，可能代表它的风格、题材、来源，或者系列', type='string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "attribute_info = list()\n",
    "\n",
    "for k, v in metadata.items():\n",
    "    attribute_info.append(\n",
    "        AttributeInfo(\n",
    "            name=k,\n",
    "            description=v['description'],\n",
    "            type=v['type']\n",
    "        )\n",
    "    )\n",
    "\n",
    "attribute_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ac868-e21f-4579-9a5a-8e29bf7eb629",
   "metadata": {},
   "source": [
    "# Construct customized self-query retriever\n",
    "\n",
    "Q: Why not using the standard?\n",
    "A: The standard SelfQueryRetriever Class provides a standard prompt template that uses few-show examples to tell llm how to construct structured query (examples can be found in [langchain.chains.query_constructor.prompt](https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/query_constructor/prompt.py). Most examples inside uses EQ (=) comparator, which isn't suitable for our use cases (mostly fuzzy matches). Therefore, we will reconstruct the self-query retriever using a customized few-shot prompt teamplate.\n",
    "\n",
    "Q: why do we copied the `get_query_constructor_prompt` provided?\n",
    "A: Its original dependency `construct_examples` will decode json using ASCII by default, which won't support Chinese, we'll need to overwrite the two functions\n",
    "\n",
    "```python\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm,\n",
    "    vectorstore=vs_chroma,\n",
    "    document_contents='Articles and excerpts.',\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")\n",
    "```\n",
    "\n",
    "References: \n",
    "https://python.langchain.com/docs/modules/data_connection/retrievers/self_query/#constructing-from-scratch-with-lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd19092d-33cf-41da-9c3a-41a3e6035089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'structured_request': {'filter': 'or(like(\"source\", \"笑死\"), in(\"source\", '\n",
      "                                  '\"笑死\"), like(\"tags\", \"笑死\"), in(\"tags\", '\n",
      "                                  '\"笑死\"))',\n",
      "                        'query': '人生有几个不捡'},\n",
      " 'user_query': '人生有几个不捡？仅从“笑死”中找答案。'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "with open('../self_query_examples.toml', 'rb') as f:\n",
    "    self_query_examples = tomllib.load(f)\n",
    "\n",
    "pprint(self_query_examples['example'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c91b4a9c-13fe-4611-82d6-0df32b656a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../self_query_template_chinese.txt', 'r') as f:\n",
    "    self_query_template = \"\\n\".join(f.readlines())\n",
    "\n",
    "# with open('../self_query_template.txt', 'r') as f:\n",
    "#     self_query_template = \"\\n\".join(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82aabfae-d016-443f-a28a-d6694dcfbc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Union, Tuple\n",
    "import json\n",
    "from langchain.chains.query_constructor.base import _format_attribute_info, get_query_constructor_prompt\n",
    "from langchain_core.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.chains.query_constructor.prompt import USER_SPECIFIED_EXAMPLE_PROMPT, SUFFIX_WITHOUT_DATA_SOURCE\n",
    "\n",
    "def _format_attribute_info(info: Sequence[Union[AttributeInfo, dict]]) -> str:\n",
    "    info_dicts = {}\n",
    "    for i in info:\n",
    "        i_dict = dict(i)\n",
    "        info_dicts[i_dict.pop(\"name\")] = i_dict\n",
    "    # return json.dumps(info_dicts, indent=4, ensure_ascii=False).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "    return info_dicts\n",
    "                                                                       \n",
    "def construct_examples(input_output_pairs: Sequence[Tuple[str, dict]]) -> List[dict]:\n",
    "    \"\"\"Construct examples from input-output pairs.\n",
    "\n",
    "    Adapted from: https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/query_constructor/base.py\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    for i, (_input, output) in enumerate(input_output_pairs):\n",
    "        structured_request = (\n",
    "            json.dumps(output, indent=4, ensure_ascii=False).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        )\n",
    "        example = {\n",
    "            \"i\": i + 1,\n",
    "            \"user_query\": _input,\n",
    "            \"structured_request\": structured_request,\n",
    "        }\n",
    "        examples.append(example)\n",
    "    return examples\n",
    "\n",
    "examples = construct_examples(\n",
    "    [(x['user_query'], x['structured_request']) for x in self_query_examples['example']]\n",
    ")\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=list(examples),\n",
    "    example_prompt=USER_SPECIFIED_EXAMPLE_PROMPT,\n",
    "    input_variables=[\"query\"],\n",
    "    # suffix=\"\",\n",
    "    suffix=SUFFIX_WITHOUT_DATA_SOURCE.format(i=len(examples) + 1),\n",
    "    prefix=self_query_template.format(\n",
    "        content_and_attributes=json.dumps({\n",
    "            'content': '文章',\n",
    "            'attributes': _format_attribute_info(attribute_info)\n",
    "        }, indent=4, ensure_ascii=False).replace(\"{\", \"{{\").replace(\"}\", \"}}\"),\n",
    "        attributes_set=str(list(metadata_set))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dc8b64d1-237f-4972-b395-64a3c731c62a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的目标是将用户的查询结构化，以匹配下面提供的请求模式。\n",
      "\n",
      "\n",
      "\n",
      "<< 结构化请求模式 >>\n",
      "\n",
      "在回复时，请使用一个Markdown代码片段，其中包含一个按照以下模式格式化的JSON对象：\n",
      "\n",
      "\n",
      "\n",
      "```json\n",
      "\n",
      "{\n",
      "\n",
      "    \"query\": string \\ 用于与文档内容进行比较的文本字符串\n",
      "\n",
      "    \"filter\": string \\ 用于过滤文档的逻辑条件语句\n",
      "\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "\n",
      "\n",
      "查询字符串应仅包含与文档内容匹配的文本。在查询中不应提及任何过滤条件。\n",
      "\n",
      "\n",
      "\n",
      "逻辑条件语句由一个或多个比较和逻辑操作语句组成。\n",
      "\n",
      "\n",
      "\n",
      "比较语句采用以下形式：`comp(attr, val)`：\n",
      "\n",
      "- comp（eq | ne | gt | gte | lt | lte | contain | like | in | nin）：比较器\n",
      "\n",
      "- attr（字符串）：要应用比较的属性名称\n",
      "\n",
      "- val（字符串）：比较值\n",
      "\n",
      "\n",
      "\n",
      "逻辑操作语句采用以下形式 op(statement1, statement2, ...)：\n",
      "\n",
      "\n",
      "\n",
      "- op（and | or | not）：逻辑运算符\n",
      "\n",
      "- statement1，statement2，...（比较语句或逻辑操作语句）：要应用操作的一个或多个语句\n",
      "\n",
      "\n",
      "\n",
      "确保仅使用上述比较器和逻辑运算符，不使用其他任何内容。\n",
      "\n",
      "确保过滤器仅引用数据源中存在的属性。\n",
      "\n",
      "确保过滤器仅使用带有其函数名称的属性名称（如果对其应用了函数）。\n",
      "\n",
      "确保过滤器仅在处理日期数据类型值时使用 YYYY-MM-DD 格式。\n",
      "\n",
      "确保过滤器考虑到属性的描述，并仅进行与存储的数据类型相符的比较。可用的属性有：['author', 'name', 'tags', 'source', 'date_end', 'date_start', 'id']。禁止擅自添加其他属性。\n",
      "\n",
      "除非是非确定，否则不要使用 eq 进行比较，鼓励多使用 like 或 in 的语句进行模糊匹配，并尽量多地将模糊匹配应用到所有属性中。\n",
      "\n",
      "仅在需要时使用过滤器。如果没有可应用的过滤器，请为过滤器值返回 \"NO_FILTER\"。\n",
      "\n",
      "\n",
      "\n",
      "<< 数据源 >>\n",
      "\n",
      "```json\n",
      "\n",
      "{\n",
      "    \"content\": \"文章\",\n",
      "    \"attributes\": {\n",
      "        \"author\": {\n",
      "            \"description\": \"本篇文章的作者\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"date_start\": {\n",
      "            \"description\": \"文章被创建的时间，格式是YYYY-MM-DD\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"date_end\": {\n",
      "            \"description\": \"文章被完成的时间，格式是YYYY-MM-DD\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"id\": {\n",
      "            \"description\": \"文章的id\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"name\": {\n",
      "            \"description\": \"文章的名字\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"source\": {\n",
      "            \"description\": \"文章的来源，这里的文章取自若干不同数据库\",\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        \"tags\": {\n",
      "            \"description\": \"文章的标签，可能代表它的风格、题材、来源，或者系列\",\n",
      "            \"type\": \"string\"\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "```\n",
      "\n",
      "<< Example 1. >>\n",
      "User Query:\n",
      "人生有几个不捡？仅从“笑死”中找答案。\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"人生有几个不捡\",\n",
      "    \"filter\": \"or(like(\\\"source\\\", \\\"笑死\\\"), in(\\\"source\\\", \\\"笑死\\\"), like(\\\"tags\\\", \\\"笑死\\\"), in(\\\"tags\\\", \\\"笑死\\\"))\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 2. >>\n",
      "User Query:\n",
      "请从三岛由纪夫的小说找到下文相关片段：“我们之所以突然变得残暴”的后面是什么？\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"我们之所以突然变得残暴\",\n",
      "    \"filter\": \"or(like(\\\"tags\\\", \\\"三岛由纪夫\\\"), in(\\\"tags\\\", \\\"三岛由纪夫\\\"), like(\\\"author\\\", \\\"三岛由纪夫\\\"), in(\\\"author\\\", \\\"三岛由纪夫\\\"), like(\\\"tags\\\", \\\"小说\\\"), in(\\\"tags\\\", \\\"小说\\\"))\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 3. >>\n",
      "User Query:\n",
      "《天使与昆虫》是哪位作家的作品？\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"\",\n",
      "    \"filter\": \"or(like(\\\"tags\\\", \\\"天使与昆虫\\\"), in(\\\"tags\\\", \\\"天使与昆虫\\\"), like(\\\"name\\\", \\\"天使与昆虫\\\"), in(\\\"name\\\", \\\"天使与昆虫\\\"), like(\\\"source\\\", \\\"天使与昆虫\\\"), in(\\\"source\\\", \\\"天使与昆虫\\\"))\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 4. >>\n",
      "User Query:\n",
      "我在2019年到2022年间写过多少篇文章？\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"\",\n",
      "    \"filter\": \"and(eq(\\\"source\\\", \\\"写作\\\"), gt(\\\"date_start\\\", \\\"2019-01-01\\\"), lt(\\\"date_end\\\", \\\"2022-12-31\\\"))\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 5. >>\n",
      "User Query:\n",
      "美国有多少个州？\n",
      "\n",
      "Structured Request:\n",
      "```json\n",
      "{\n",
      "    \"query\": \"\",\n",
      "    \"filter\": \"NO_FILTER\"\n",
      "}\n",
      "```\n",
      "\n",
      "\n",
      "<< Example 6. >>\n",
      "User Query:\n",
      "\u001b[33;1m\u001b[1;3m{query}\u001b[0m\n",
      "\n",
      "Structured Request:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0efb04c6-ad8e-4fac-bbbb-a6f03edb1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import StructuredQueryOutputParser\n",
    "output_parser = StructuredQueryOutputParser.from_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c825baf0-34b2-43af-a0a6-f6a116ce7481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"base\",\n",
      "    \"StringPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"text\": \"你的目标是将用户的查询结构化，以匹配下面提供的请求模式。\\n\\n\\n\\n<< 结构化请求模式 >>\\n\\n在回复时，请使用一个Markdown代码片段，其中包含一个按照以下模式格式化的JSON对象：\\n\\n\\n\\n```json\\n\\n{\\n\\n    \\\"query\\\": string \\\\ 用于与文档内容进行比较的文本字符串\\n\\n    \\\"filter\\\": string \\\\ 用于过滤文档的逻辑条件语句\\n\\n}\\n\\n```\\n\\n\\n\\n查询字符串应仅包含与文档内容匹配的文本。在查询中不应提及任何过滤条件。\\n\\n\\n\\n逻辑条件语句由一个或多个比较和逻辑操作语句组成。\\n\\n\\n\\n比较语句采用以下形式：`comp(attr, val)`：\\n\\n- comp（eq | ne | gt | gte | lt | lte | contain | like | in | nin）：比较器\\n\\n- attr（字符串）：要应用比较的属性名称\\n\\n- val（字符串）：比较值\\n\\n\\n\\n逻辑操作语句采用以下形式 op(statement1, statement2, ...)：\\n\\n\\n\\n- op（and | or | not）：逻辑运算符\\n\\n- statement1，statement2，...（比较语句或逻辑操作语句）：要应用操作的一个或多个语句\\n\\n\\n\\n确保仅使用上述比较器和逻辑运算符，不使用其他任何内容。\\n\\n确保过滤器仅引用数据源中存在的属性。\\n\\n确保过滤器仅使用带有其函数名称的属性名称（如果对其应用了函数）。\\n\\n确保过滤器仅在处理日期数据类型值时使用 YYYY-MM-DD 格式。\\n\\n确保过滤器考虑到属性的描述，并仅进行与存储的数据类型相符的比较。可用的属性有：['author', 'name', 'tags', 'source', 'date_end', 'date_start', 'id']。禁止擅自添加其他属性。\\n\\n除非是非确定，否则不要使用 eq 进行比较，鼓励多使用 like 或 in 的语句进行模糊匹配，并尽量多地将模糊匹配应用到所有属性中。\\n\\n仅在需要时使用过滤器。如果没有可应用的过滤器，请为过滤器值返回 \\\"NO_FILTER\\\"。\\n\\n\\n\\n<< 数据源 >>\\n\\n```json\\n\\n{\\n    \\\"content\\\": \\\"文章\\\",\\n    \\\"attributes\\\": {\\n        \\\"author\\\": {\\n            \\\"description\\\": \\\"本篇文章的作者\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"date_start\\\": {\\n            \\\"description\\\": \\\"文章被创建的时间，格式是YYYY-MM-DD\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"date_end\\\": {\\n            \\\"description\\\": \\\"文章被完成的时间，格式是YYYY-MM-DD\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"id\\\": {\\n            \\\"description\\\": \\\"文章的id\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"name\\\": {\\n            \\\"description\\\": \\\"文章的名字\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"source\\\": {\\n            \\\"description\\\": \\\"文章的来源，这里的文章取自若干不同数据库\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"tags\\\": {\\n            \\\"description\\\": \\\"文章的标签，可能代表它的风格、题材、来源，或者系列\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    }\\n}\\n\\n```\\n\\n<< Example 1. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"人生有几个不捡\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\", \\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\", \\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\", \\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"笑死\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nUser Query:\\n请从三岛由纪夫的小说找到下文相关片段：“我们之所以突然变得残暴”的后面是什么？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"我们之所以突然变得残暴\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"author\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"author\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"tags\\\\\\\", \\\\\\\"小说\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"小说\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nUser Query:\\n《天使与昆虫》是哪位作家的作品？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), like(\\\\\\\"name\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"name\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), like(\\\\\\\"source\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"source\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 4. >>\\nUser Query:\\n我在2019年到2022年间写过多少篇文章？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"source\\\\\\\", \\\\\\\"写作\\\\\\\"), gt(\\\\\\\"date_start\\\\\\\", \\\\\\\"2019-01-01\\\\\\\"), lt(\\\\\\\"date_end\\\\\\\", \\\\\\\"2022-12-31\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 5. >>\\nUser Query:\\n美国有多少个州？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 6. >>\\nUser Query:\\n“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\\n\\nStructured Request:\\n\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"你的目标是将用户的查询结构化，以匹配下面提供的请求模式。\\n\\n\\n\\n<< 结构化请求模式 >>\\n\\n在回复时，请使用一个Markdown代码片段，其中包含一个按照以下模式格式化的JSON对象：\\n\\n\\n\\n```json\\n\\n{\\n\\n    \\\"query\\\": string \\\\ 用于与文档内容进行比较的文本字符串\\n\\n    \\\"filter\\\": string \\\\ 用于过滤文档的逻辑条件语句\\n\\n}\\n\\n```\\n\\n\\n\\n查询字符串应仅包含与文档内容匹配的文本。在查询中不应提及任何过滤条件。\\n\\n\\n\\n逻辑条件语句由一个或多个比较和逻辑操作语句组成。\\n\\n\\n\\n比较语句采用以下形式：`comp(attr, val)`：\\n\\n- comp（eq | ne | gt | gte | lt | lte | contain | like | in | nin）：比较器\\n\\n- attr（字符串）：要应用比较的属性名称\\n\\n- val（字符串）：比较值\\n\\n\\n\\n逻辑操作语句采用以下形式 op(statement1, statement2, ...)：\\n\\n\\n\\n- op（and | or | not）：逻辑运算符\\n\\n- statement1，statement2，...（比较语句或逻辑操作语句）：要应用操作的一个或多个语句\\n\\n\\n\\n确保仅使用上述比较器和逻辑运算符，不使用其他任何内容。\\n\\n确保过滤器仅引用数据源中存在的属性。\\n\\n确保过滤器仅使用带有其函数名称的属性名称（如果对其应用了函数）。\\n\\n确保过滤器仅在处理日期数据类型值时使用 YYYY-MM-DD 格式。\\n\\n确保过滤器考虑到属性的描述，并仅进行与存储的数据类型相符的比较。可用的属性有：['author', 'name', 'tags', 'source', 'date_end', 'date_start', 'id']。禁止擅自添加其他属性。\\n\\n除非是非确定，否则不要使用 eq 进行比较，鼓励多使用 like 或 in 的语句进行模糊匹配，并尽量多地将模糊匹配应用到所有属性中。\\n\\n仅在需要时使用过滤器。如果没有可应用的过滤器，请为过滤器值返回 \\\"NO_FILTER\\\"。\\n\\n\\n\\n<< 数据源 >>\\n\\n```json\\n\\n{\\n    \\\"content\\\": \\\"文章\\\",\\n    \\\"attributes\\\": {\\n        \\\"author\\\": {\\n            \\\"description\\\": \\\"本篇文章的作者\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"date_start\\\": {\\n            \\\"description\\\": \\\"文章被创建的时间，格式是YYYY-MM-DD\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"date_end\\\": {\\n            \\\"description\\\": \\\"文章被完成的时间，格式是YYYY-MM-DD\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"id\\\": {\\n            \\\"description\\\": \\\"文章的id\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"name\\\": {\\n            \\\"description\\\": \\\"文章的名字\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"source\\\": {\\n            \\\"description\\\": \\\"文章的来源，这里的文章取自若干不同数据库\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        },\\n        \\\"tags\\\": {\\n            \\\"description\\\": \\\"文章的标签，可能代表它的风格、题材、来源，或者系列\\\",\\n            \\\"type\\\": \\\"string\\\"\\n        }\\n    }\\n}\\n\\n```\\n\\n<< Example 1. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"人生有几个不捡\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\", \\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\", \\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\", \\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"笑死\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nUser Query:\\n请从三岛由纪夫的小说找到下文相关片段：“我们之所以突然变得残暴”的后面是什么？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"我们之所以突然变得残暴\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"author\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"author\\\\\\\", \\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"tags\\\\\\\", \\\\\\\"小说\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"小说\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nUser Query:\\n《天使与昆虫》是哪位作家的作品？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), like(\\\\\\\"name\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"name\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), like(\\\\\\\"source\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"), in(\\\\\\\"source\\\\\\\", \\\\\\\"天使与昆虫\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 4. >>\\nUser Query:\\n我在2019年到2022年间写过多少篇文章？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"source\\\\\\\", \\\\\\\"写作\\\\\\\"), gt(\\\\\\\"date_start\\\\\\\", \\\\\\\"2019-01-01\\\\\\\"), lt(\\\\\\\"date_end\\\\\\\", \\\\\\\"2022-12-31\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 5. >>\\nUser Query:\\n美国有多少个州？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 6. >>\\nUser Query:\\n“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\\n\\nStructured Request:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:LlamaCpp] [9.64s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{ \\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\")), like(\\\\\\\"source\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"))\\\" \\n} \\n```\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StructuredQueryOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"```json\\n{ \\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\")), like(\\\\\\\"source\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"))\\\" \\n} \\n```\"\n",
      "}\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StructuredQueryOutputParser] [1ms] Parser run errored with error:\n",
      "\u001b[0m\"OutputParserException('Parsing text\\\\n```json\\\\n{ \\\\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\\\n    \\\"filter\\\": \\\"or(like(\\\\\\\\\\\"tags\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\"), in(\\\\\\\\\\\"tags\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\")), like(\\\\\\\\\\\"source\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\"))\\\" \\\\n} \\\\n```\\\\n raised following error:\\\\nUnexpected token Token(\\\\'COMMA\\\\', \\\\',\\\\') at line 1, column 53.\\\\nExpected one of: \\\\n\\\\t* $END\\\\n')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py\\\", line 77, in feed_token\\n    action, arg = states[state][token.type]\\n                  ~~~~~~~~~~~~~^^^^^^^^^^^^\\n\\n\\nKeyError: 'COMMA'\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py\\\", line 56, in parse\\n    parsed[\\\"filter\\\"] = self.ast_parse(parsed[\\\"filter\\\"])\\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/lark.py\\\", line 658, in parse\\n    return self.parser.parse(text, start=start, on_error=on_error)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parser_frontends.py\\\", line 104, in parse\\n    return self.parser.parse(stream, chosen_start, **kw)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 42, in parse\\n    return self.parser.parse(lexer, start)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 88, in parse\\n    return self.parse_from_state(parser_state)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 111, in parse_from_state\\n    raise e\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 102, in parse_from_state\\n    state.feed_token(token)\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py\\\", line 80, in feed_token\\n    raise UnexpectedToken(token, expected, state=self, interactive_parser=None)\\n\\n\\nlark.exceptions.UnexpectedToken: Unexpected token Token('COMMA', ',') at line 1, column 53.\\nExpected one of: \\n\\t* $END\\n\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 1246, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\\\", line 177, in <lambda>\\n    lambda inner_input: self.parse_result([Generation(text=inner_input)]),\\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\\\", line 219, in parse_result\\n    return self.parse(result[0].text)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py\\\", line 63, in parse\\n    raise OutputParserException(\\n\\n\\nlangchain_core.exceptions.OutputParserException: Parsing text\\n```json\\n{ \\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\")), like(\\\\\\\"source\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"))\\\" \\n} \\n```\\n raised following error:\\nUnexpected token Token('COMMA', ',') at line 1, column 53.\\nExpected one of: \\n\\t* $END\"\n",
      "\u001b[31;1m\u001b[1;3m[chain/error]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [9.65s] Chain run errored with error:\n",
      "\u001b[0m\"OutputParserException('Parsing text\\\\n```json\\\\n{ \\\\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\\\n    \\\"filter\\\": \\\"or(like(\\\\\\\\\\\"tags\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\"), in(\\\\\\\\\\\"tags\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\")), like(\\\\\\\\\\\"source\\\\\\\\\\\", \\\\\\\\\\\"读书笔记（文学）\\\\\\\\\\\"))\\\" \\\\n} \\\\n```\\\\n raised following error:\\\\nUnexpected token Token(\\\\'COMMA\\\\', \\\\',\\\\') at line 1, column 53.\\\\nExpected one of: \\\\n\\\\t* $END\\\\n')Traceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py\\\", line 77, in feed_token\\n    action, arg = states[state][token.type]\\n                  ~~~~~~~~~~~~~^^^^^^^^^^^^\\n\\n\\nKeyError: 'COMMA'\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py\\\", line 56, in parse\\n    parsed[\\\"filter\\\"] = self.ast_parse(parsed[\\\"filter\\\"])\\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/lark.py\\\", line 658, in parse\\n    return self.parser.parse(text, start=start, on_error=on_error)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parser_frontends.py\\\", line 104, in parse\\n    return self.parser.parse(stream, chosen_start, **kw)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 42, in parse\\n    return self.parser.parse(lexer, start)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 88, in parse\\n    return self.parse_from_state(parser_state)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 111, in parse_from_state\\n    raise e\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py\\\", line 102, in parse_from_state\\n    state.feed_token(token)\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py\\\", line 80, in feed_token\\n    raise UnexpectedToken(token, expected, state=self, interactive_parser=None)\\n\\n\\nlark.exceptions.UnexpectedToken: Unexpected token Token('COMMA', ',') at line 1, column 53.\\nExpected one of: \\n\\t* $END\\n\\n\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\n\\n\\nTraceback (most recent call last):\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 2053, in invoke\\n    input = step.invoke(\\n            ^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\\\", line 176, in invoke\\n    return self._call_with_config(\\n           ^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/base.py\\\", line 1246, in _call_with_config\\n    context.run(\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/config.py\\\", line 326, in call_func_with_variable_args\\n    return func(input, **kwargs)  # type: ignore[call-arg]\\n           ^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\\\", line 177, in <lambda>\\n    lambda inner_input: self.parse_result([Generation(text=inner_input)]),\\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py\\\", line 219, in parse_result\\n    return self.parse(result[0].text)\\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\\n\\n\\n  File \\\"/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py\\\", line 63, in parse\\n    raise OutputParserException(\\n\\n\\nlangchain_core.exceptions.OutputParserException: Parsing text\\n```json\\n{ \\n    \\\"query\\\": \\\"每个人都以为他自己至少有一种主要的美德。\\\", \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"), in(\\\\\\\"tags\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\")), like(\\\\\\\"source\\\\\\\", \\\\\\\"读书笔记（文学）\\\\\\\"))\\\" \\n} \\n```\\n raised following error:\\nUnexpected token Token('COMMA', ',') at line 1, column 53.\\nExpected one of: \\n\\t* $END\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    6662.11 ms\n",
      "llama_print_timings:      sample time =      26.98 ms /    70 runs   (    0.39 ms per token,  2594.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4765.89 ms /   306 tokens (   15.57 ms per token,    64.21 tokens per second)\n",
      "llama_print_timings:        eval time =    4517.55 ms /    69 runs   (   65.47 ms per token,    15.27 tokens per second)\n",
      "llama_print_timings:       total time =    9639.78 ms /   375 tokens\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Parsing text\n```json\n{ \n    \"query\": \"每个人都以为他自己至少有一种主要的美德。\", \n    \"filter\": \"or(like(\\\"tags\\\", \\\"读书笔记（文学）\\\"), in(\\\"tags\\\", \\\"读书笔记（文学）\\\")), like(\\\"source\\\", \\\"读书笔记（文学）\\\"))\" \n} \n```\n raised following error:\nUnexpected token Token('COMMA', ',') at line 1, column 53.\nExpected one of: \n\t* $END\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py:77\u001b[0m, in \u001b[0;36mParserState.feed_token\u001b[0;34m(self, token, is_end)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     action, arg \u001b[38;5;241m=\u001b[39m \u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'COMMA'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:56\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     parsed[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mast_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparsed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/lark.py:658\u001b[0m, in \u001b[0;36mLark.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse the given text, according to the options provided.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \n\u001b[1;32m    643\u001b[0m \u001b[38;5;124;03mParameters:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m \n\u001b[1;32m    657\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parser_frontends.py:104\u001b[0m, in \u001b[0;36mParsingFrontend.parse\u001b[0;34m(self, text, start, on_error)\u001b[0m\n\u001b[1;32m    103\u001b[0m stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_lexer_thread(text)\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:42\u001b[0m, in \u001b[0;36mLALR_Parser.parse\u001b[0;34m(self, lexer, start, on_error)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m UnexpectedInput \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:88\u001b[0m, in \u001b[0;36m_Parser.parse\u001b[0;34m(self, lexer, start, value_stack, state_stack, start_interactive)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InteractiveParser(\u001b[38;5;28mself\u001b[39m, parser_state, parser_state\u001b[38;5;241m.\u001b[39mlexer)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_from_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:111\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser.py:102\u001b[0m, in \u001b[0;36m_Parser.parse_from_state\u001b[0;34m(self, state, last_token)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m end_token \u001b[38;5;241m=\u001b[39m Token\u001b[38;5;241m.\u001b[39mnew_borrow_pos(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$END\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, token) \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;28;01melse\u001b[39;00m Token(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$END\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/lark/parsers/lalr_parser_state.py:80\u001b[0m, in \u001b[0;36mParserState.feed_token\u001b[0;34m(self, token, is_end)\u001b[0m\n\u001b[1;32m     79\u001b[0m     expected \u001b[38;5;241m=\u001b[39m {s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states[state]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m s\u001b[38;5;241m.\u001b[39misupper()}\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedToken(token, expected, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, interactive_parser\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m arg \u001b[38;5;241m!=\u001b[39m end_state\n",
      "\u001b[0;31mUnexpectedToken\u001b[0m: Unexpected token Token('COMMA', ',') at line 1, column 53.\nExpected one of: \n\t* $END\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:6\u001b[0m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/base.py:1246\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1242\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1243\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1244\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1245\u001b[0m         Output,\n\u001b[0;32m-> 1246\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1254\u001b[0m     )\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/output_parsers/base.py:219\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain/chains/query_constructor/base.py:63\u001b[0m, in \u001b[0;36mStructuredQueryOutputParser.parse\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m StructuredQuery(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m parsed\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m allowed_keys}\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParsing text\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m raised following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     )\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Parsing text\n```json\n{ \n    \"query\": \"每个人都以为他自己至少有一种主要的美德。\", \n    \"filter\": \"or(like(\\\"tags\\\", \\\"读书笔记（文学）\\\"), in(\\\"tags\\\", \\\"读书笔记（文学）\\\")), like(\\\"source\\\", \\\"读书笔记（文学）\\\"))\" \n} \n```\n raised following error:\nUnexpected token Token('COMMA', ',') at line 1, column 53.\nExpected one of: \n\t* $END\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "query_constructor = prompt | llm | output_parser\n",
    "\n",
    "query_constructor.invoke(\n",
    "    {\n",
    "        \"query\": \"“每个人都以为他自己至少有一种主要的美德。”是出自哪里？请从“读书笔记（文学）”中找到答案。\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "673a0513-b62e-4fb1-8572-0c9c6cf565f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author', 'date_end', 'date_start', 'id', 'name', 'source', 'tags'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e72a5-92e2-4413-8784-6ab390728c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da7741-e68b-470f-a98b-cb201f31ecb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1587f04-ea69-4c7b-96d2-204d31f2f5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba96478-59d7-41c7-a9ed-d4ed11987e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30f8021-ef6c-4f5c-9aea-42c76efd09b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49d860-1598-46f7-a669-239f049374fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0778404e-bcf4-40e9-9f7b-44b974ec6494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"人生有几个不捡？仅从“笑死”中找答案。\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"人生有几个不捡？仅从“笑死”中找答案。\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:FewShotPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"base\",\n",
      "    \"StringPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"text\": \"Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n    \\\"limit\\\": int \\\\ the number of documents to retrieve\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or | not): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\nMake sure the `limit` is always an int value. It is an optional parameter so leave it blank if it does not make sense.\\n\\n\\n<< Data Source >>\\n```json\\n{\\n    \\\"content\\\": \\\"文章\\\",\\n    \\\"attributes\\\": {\\n    \\\"author\\\": {\\n        \\\"description\\\": \\\"\\\\u672c\\\\u7bc7\\\\u6587\\\\u7ae0\\\\u7684\\\\u4f5c\\\\u8005\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_start\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u88ab\\\\u521b\\\\u5efa\\\\u7684\\\\u65f6\\\\u95f4\\\\uff0c\\\\u683c\\\\u5f0f\\\\u662fYYYY-MM-DD\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_end\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u88ab\\\\u5b8c\\\\u6210\\\\u7684\\\\u65f6\\\\u95f4\\\\uff0c\\\\u683c\\\\u5f0f\\\\u662fYYYY-MM-DD\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"id\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684id\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"name\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u540d\\\\u5b57\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"source\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u6765\\\\u6e90\\\\uff0c\\\\u8fd9\\\\u91cc\\\\u7684\\\\u6587\\\\u7ae0\\\\u53d6\\\\u81ea\\\\u82e5\\\\u5e72\\\\u4e0d\\\\u540c\\\\u6570\\\\u636e\\\\u5e93\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"tags\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u6807\\\\u7b7e\\\\uff0c\\\\u53ef\\\\u80fd\\\\u4ee3\\\\u8868\\\\u5b83\\\\u7684\\\\u98ce\\\\u683c\\\\u3001\\\\u9898\\\\u6750\\\\u3001\\\\u6765\\\\u6e90\\\\uff0c\\\\u6216\\\\u8005\\\\u7cfb\\\\u5217\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    }\\n}\\n}\\n```\\n\\n\\n<< Example 1. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"人生有几个不捡？\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nUser Query:\\n“我们之所以突然变得残暴”的后面是什么？请从三岛由纪夫的小说中寻找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"我们之所以突然变得残暴\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"author\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"author\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"小说\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"小说\\\\\\\")\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nUser Query:\\n我在2019年到2022年间写过多少篇文章？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"source\\\\\\\",\\\\\\\"写作\\\\\\\"), gt(\\\\\\\"date_start\\\\\\\", \\\\\\\"2019-01-01\\\\\\\"), lt(\\\\\\\"date_end\\\\\\\", \\\\\\\"2022-12-31\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 4. >>\\nUser Query:\\n美国有多少个州？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 5. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\\n\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n    \\\"limit\\\": int \\\\ the number of documents to retrieve\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte | contain | like | in | nin): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or | not): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\nMake sure the `limit` is always an int value. It is an optional parameter so leave it blank if it does not make sense.\\n\\n\\n<< Data Source >>\\n```json\\n{\\n    \\\"content\\\": \\\"文章\\\",\\n    \\\"attributes\\\": {\\n    \\\"author\\\": {\\n        \\\"description\\\": \\\"\\\\u672c\\\\u7bc7\\\\u6587\\\\u7ae0\\\\u7684\\\\u4f5c\\\\u8005\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_start\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u88ab\\\\u521b\\\\u5efa\\\\u7684\\\\u65f6\\\\u95f4\\\\uff0c\\\\u683c\\\\u5f0f\\\\u662fYYYY-MM-DD\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_end\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u88ab\\\\u5b8c\\\\u6210\\\\u7684\\\\u65f6\\\\u95f4\\\\uff0c\\\\u683c\\\\u5f0f\\\\u662fYYYY-MM-DD\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"id\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684id\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"name\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u540d\\\\u5b57\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"source\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u6765\\\\u6e90\\\\uff0c\\\\u8fd9\\\\u91cc\\\\u7684\\\\u6587\\\\u7ae0\\\\u53d6\\\\u81ea\\\\u82e5\\\\u5e72\\\\u4e0d\\\\u540c\\\\u6570\\\\u636e\\\\u5e93\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"tags\\\": {\\n        \\\"description\\\": \\\"\\\\u6587\\\\u7ae0\\\\u7684\\\\u6807\\\\u7b7e\\\\uff0c\\\\u53ef\\\\u80fd\\\\u4ee3\\\\u8868\\\\u5b83\\\\u7684\\\\u98ce\\\\u683c\\\\u3001\\\\u9898\\\\u6750\\\\u3001\\\\u6765\\\\u6e90\\\\uff0c\\\\u6216\\\\u8005\\\\u7cfb\\\\u5217\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    }\\n}\\n}\\n```\\n\\n\\n<< Example 1. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"人生有几个不捡？\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nUser Query:\\n“我们之所以突然变得残暴”的后面是什么？请从三岛由纪夫的小说中寻找答案。\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"我们之所以突然变得残暴\\\",\\n    \\\"filter\\\": \\\"or(like(\\\\\\\"tags\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"author\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), in(\\\\\\\"author\\\\\\\",\\\\\\\"三岛由纪夫\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"小说\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"小说\\\\\\\")\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nUser Query:\\n我在2019年到2022年间写过多少篇文章？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"and(eq(\\\\\\\"source\\\\\\\",\\\\\\\"写作\\\\\\\"), gt(\\\\\\\"date_start\\\\\\\", \\\\\\\"2019-01-01\\\\\\\"), lt(\\\\\\\"date_end\\\\\\\", \\\\\\\"2022-12-31\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 4. >>\\nUser Query:\\n美国有多少个州？\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 5. >>\\nUser Query:\\n人生有几个不捡？仅从“笑死”中找答案。\\n\\nStructured Request:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:LlamaCpp] [9.34s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{  \\n    \\\"query\\\": \\\"人生有几个不捡？\\\",  \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"))\\\"  \\n}  \\n```\\n\\nIn this example, the user query is asking for a count of items that follow a certain pattern. The structured request filters the data to only include those where the source or tags contain the string \\\"笑死\\\". This ensures that the count being returned is specifically related to the user's question.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StructuredQueryOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"```json\\n{  \\n    \\\"query\\\": \\\"人生有几个不捡？\\\",  \\n    \\\"filter\\\": \\\"or(like(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"source\\\\\\\",\\\\\\\"笑死\\\\\\\"), like(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"), in(\\\\\\\"tags\\\\\\\",\\\\\\\"笑死\\\\\\\"))\\\"  \\n}  \\n```\\n\\nIn this example, the user query is asking for a count of items that follow a certain pattern. The structured request filters the data to only include those where the source or tags contain the string \\\"笑死\\\". This ensures that the count being returned is specifically related to the user's question.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:StructuredQueryOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"not_implemented\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"chains\",\n",
      "    \"query_constructor\",\n",
      "    \"ir\",\n",
      "    \"StructuredQuery\"\n",
      "  ],\n",
      "  \"repr\": \"StructuredQuery(query='人生有几个不捡？', filter=Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.LIKE: 'like'>, attribute='source', value='笑死'), Comparison(comparator=<Comparator.IN: 'in'>, attribute='source', value='笑死'), Comparison(comparator=<Comparator.LIKE: 'like'>, attribute='tags', value='笑死'), Comparison(comparator=<Comparator.IN: 'in'>, attribute='tags', value='笑死')]), limit=None)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [9.34s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "CPU times: user 41.9 s, sys: 224 ms, total: 42.1 s\n",
      "Wall time: 9.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7569.75 ms\n",
      "llama_print_timings:      sample time =      50.45 ms /   121 runs   (    0.42 ms per token,  2398.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =     884.49 ms /    19 tokens (   46.55 ms per token,    21.48 tokens per second)\n",
      "llama_print_timings:        eval time =    7830.46 ms /   120 runs   (   65.25 ms per token,    15.32 tokens per second)\n",
      "llama_print_timings:       total time =    9330.40 ms /   139 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "StructuredQuery(query='人生有几个不捡？', filter=Operation(operator=<Operator.OR: 'or'>, arguments=[Comparison(comparator=<Comparator.LIKE: 'like'>, attribute='source', value='笑死'), Comparison(comparator=<Comparator.IN: 'in'>, attribute='source', value='笑死'), Comparison(comparator=<Comparator.LIKE: 'like'>, attribute='tags', value='笑死'), Comparison(comparator=<Comparator.IN: 'in'>, attribute='tags', value='笑死')]), limit=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "query_constructor = prompt | llm | output_parser\n",
    "\n",
    "query_constructor.invoke(\n",
    "    {\n",
    "        \"query\": \"人生有几个不捡？仅从“笑死”中找答案。\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a00f1705-af14-4a31-8da9-ad13e9b8eacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:4\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "retriever.invoke('人生有几个不捡？仅从“笑死”中找答案。')\n",
    "# retriever.invoke('什么是我国第一部编年国别史？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da01b66-8fd9-46a5-8ec6-775deeb60588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from notion_agent import chatbot\n",
    "\n",
    "# llm = chatbot(\n",
    "#     'Qwen/Qwen1.5-7B-Chat', \n",
    "#     _CONFIGS['model_path']+'/'+'qwen1_5-7b-chat-q4_0.gguf', \n",
    "#     **_CONFIGS['llm']\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
