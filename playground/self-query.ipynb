{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140baf02-574d-4a1f-adea-1c4e58364026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from langchain_core.documents.base import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2572370-bc5b-48d9-ad92-5c383595915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "\n",
    "with open('../.tokens.toml', 'rb') as f:\n",
    "    _TOKENS = tomllib.load(f)\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "183ddf4a-ae95-4d76-9bc3-a2e49dcffdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    api_key=_TOKENS['huggingface'], \n",
    "    model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    ")\n",
    "\n",
    "vs_chroma = Chroma(persist_directory='../database/vs_chroma', embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1aea029-97f6-4474-8a48-c4f4133dafc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='为我，轻物重生\\n《孟子》中说：“杨子取为我，拔一毛而利天下，不为也。”；《吕氏春秋》中说：“陌生贵己。”；《淮南子》中写：“全性保真，不以物累形：杨子所立也。”这些是同时代的著作中对杨朱思想的记录和反映\\n在道家更后期的《老子》和《庄子》中也有相同的体现。《老子》中写到：“名与身：孰亲？身与货：孰多？”《庄子》中写到：“山木自寇也。膏火自煎也，桂可食，故伐之。漆可用，故割之。”\\n无用是全生的方法。善于全生的人，一定不能多为恶，但也一定不能多为善。他一定要生活在善恶之间，力求无用。到头来，无用却对于他有大用\\n从为我到无我：先秦道家发展三阶段\\n先秦道家都是为我的，但是随着思考的深入，后来的发展使这种为我走向反面，取消了它自身\\n第一阶段杨朱，出发点是全生避害\\n第二阶段老子，开始企图揭示宇宙事物变化的规律。一个人如果懂得了这些规律，并且遵循规律而调整行动，那么他就能够使事物转向对他有利的方向。但即便一个人懂得自然规律，预料之外的因素仍然会发挥作用，并带来可能的危害（“吾所以有大患者，为吾有身，及吾无身，吾有何患！”）\\n第三阶段庄子，因为没有办法避免受到外界事物的影响，庄子转而从一种更高的观点看待事物，产生”齐生死，一物我“的理论\\n孟子：儒家的理想主义派\\n人性本善', metadata={'author': '【中】冯友兰', 'date_start': '2023-05-06', 'id': '0419517a-59be-47a2-a4b9-bb6f21630614', 'name': '中国哲学简史', 'source': '读书笔记（文学）', 'tags': '人文'}),\n",
       "  1.0759108066558838),\n",
       " (Document(page_content='忠与恕\\n实行仁的具体方法就是推己及人（“己欲立而立人，己欲达而达人……可谓仁之方也”）\\n“忠”是推己及人的肯定方面；“恕”是其否定方面\\n忠恕之道是人的道德生活的开端和终结，实行忠恕就是行仁（“夫子之道，忠恕而已矣”）\\n知命\\n“无所为而为”：儒家认为，一个人不可能“无为”，因为每个人都有他应该做的事。然而他做这些事都是“无所为”，因为做这些事的价值在于做的本身之内，而不是在于外在的结果\\n知命是承认世界本来存在的必然性，这样，对于外在的成败也就无所萦怀。在这种意义上，人就永远不会失败，因为如果我们尽了应尽的义务，那么这项义务在道德上就完成了，这与行动的外在成败并不相干\\n孔子的一生正是这种学说的写照。他周游各地，政治上的一切努力都没有得到回报，但他并不气馁（“不知命，无以为君子也。”）\\n超道德：孔子在五六十岁的时候认识“天命”并且顺乎天命，他在这时候认识到了超道德的价值，认为他所做的事是受到了天的支持（“天下之无道也久矣，天将以夫子为木铎。”）孔子的超道德价值仍然是人伦日用的，孔子的超道德价值和道家抛弃理智和目的的混沌、神秘仍然有所区别\\n墨子：孔子的第一个反对者\\n在周代，天子、诸侯、封建主都有他们的军事专家。当时军队的骨干，由世袭的武士组成。随着封建制度的解体，这些武士专家丧失了爵位，流散各地被雇佣，这种人被称为“游侠”。史记《游侠列传》说他们”其言必信，其行必果，已诺必诚，不爱其躯，赴士之厄困。“\\n和源自上层阶层的儒家思想不同，墨家思想是来自下层阶级的人。从平民观点看，礼乐一类的古代文化和社会活动完全限于贵族，毫无实用价值。墨家对这种传统制度及其辩护者（儒家）的批判，加上对本阶级职业道德的发挥，构成了墨家哲学的核心。\\n墨子及其门徒和普通游侠还是有两点不同：第一，普通游侠只要得到酬谢或者恩惠就可以打仗，但墨子强烈反对侵略战争，只参加严格限于自卫的战争；第二，普通游侠只是新手职业道德的条规，而墨子却详细阐明这种道德并论证了其正当性。\\n兼爱\\n“强之劫弱，众之暴寡，诈之谋愚，贵之傲贱：此天下之害也。”\\n兼爱是墨子哲学的中心概念，这种道德是游侠“有福同享有难同当”逻辑的延伸。以这种团体的概念为基础，墨子宣扬天下每个人都应该无差别地爱一切人\\n功利主义：天志和明鬼\\n为了诱导人们实行兼爱，墨子引进了宗教的、政治的制裁', metadata={'author': '【中】冯友兰', 'date_start': '2023-05-06', 'id': '0419517a-59be-47a2-a4b9-bb6f21630614', 'name': '中国哲学简史', 'source': '读书笔记（文学）', 'tags': '人文'}),\n",
       "  1.4103361368179321)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chroma applies filter before semantic sesarch\n",
    "vs_chroma.similarity_search_with_score(\n",
    "    '谁说过陌生贵己？', \n",
    "    filter={\n",
    "        'author': '【中】冯友兰',\n",
    "    },\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b18bcdc6-6c77-4357-b1b3-27e1e6138d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = vs_chroma.get(include=[\"metadatas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc16ea8-5d30-44c9-87c6-91ed8de9e069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author', 'date_end', 'date_start', 'id', 'name', 'source', 'tags'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_set = set()\n",
    "\n",
    "for x in metadata['metadatas']:\n",
    "    metadata_set = metadata_set.union(list(x.keys()))\n",
    "\n",
    "metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca8535b6-429d-4092-98e8-933034489267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': {'description': 'THe author of the article.', 'type': 'string'},\n",
       " 'date_start': {'description': 'The date the article was created. In YYYY-MM-DD format.',\n",
       "  'type': 'string'},\n",
       " 'date_end': {'description': 'The date the article was completed. In YYYY-MM-DD format.',\n",
       "  'type': 'string'},\n",
       " 'id': {'description': 'The id of the text.', 'type': 'string'},\n",
       " 'name': {'description': 'The name of the article.', 'type': 'string'},\n",
       " 'source': {'description': 'The source of the article, representing the name of the Notion database it was retrieved',\n",
       "  'type': 'string'},\n",
       " 'tags': {'description': 'The different tags for an article, can represent its genre, origination, or belonged series.',\n",
       "  'type': 'string'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = _CONFIGS['metadata']\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cd28f7e-1891-4218-b1dd-2aef112e848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure there's no more undocumented metadata\n",
    "assert metadata_set.union(metadata.keys()) == metadata_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60d99d07-c280-445f-97f3-7fb570f2a9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AttributeInfo(name='author', description='THe author of the article.', type='string'),\n",
       " AttributeInfo(name='date_start', description='The date the article was created. In YYYY-MM-DD format.', type='string'),\n",
       " AttributeInfo(name='date_end', description='The date the article was completed. In YYYY-MM-DD format.', type='string'),\n",
       " AttributeInfo(name='id', description='The id of the text.', type='string'),\n",
       " AttributeInfo(name='name', description='The name of the article.', type='string'),\n",
       " AttributeInfo(name='source', description='The source of the article, representing the name of the Notion database it was retrieved', type='string'),\n",
       " AttributeInfo(name='tags', description='The different tags for an article, can represent its genre, origination, or belonged series.', type='string')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "metadata_field_info = list()\n",
    "\n",
    "for k, v in metadata.items():\n",
    "    metadata_field_info.append(\n",
    "        AttributeInfo(\n",
    "            name=k,\n",
    "            description=v['description'],\n",
    "            type=v['type']\n",
    "        )\n",
    "    )\n",
    "\n",
    "metadata_field_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99c4642c-cf8c-4552-bdf6-70ebf355ee09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 259 tensors from /Users/fred/Documents/models/Qwen-7B-Chat.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
      "llama_model_loader: - kv   2:                        qwen.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                           qwen.block_count u32              = 32\n",
      "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q4_K:  113 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.56 GiB (5.07 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen\n",
      "llm_load_print_meta: BOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: EOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: UNK token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   612.59 MiB, ( 1421.62 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4666.59 MiB\n",
      "llm_load_tensors:      Metal buffer size =   612.59 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, ( 1453.62 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 1453.64 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   162.81 MiB, ( 1616.44 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   162.80 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   335.23 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151643', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen.attention.head_count': '32', 'qwen.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '151643', 'qwen.feed_forward_length': '22016', 'qwen.attention.layer_norm_rms_epsilon': '0.000001', 'qwen.embedding_length': '4096', 'qwen.rope.dimension_count': '128', 'qwen.context_length': '32768', 'qwen.block_count': '32', 'general.name': 'Qwen', 'general.architecture': 'qwen'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines.conversational import Conversation\n",
    "\n",
    "class chatbot:\n",
    "    def __init__(self, model_name, model_path, **model_params):\n",
    "\n",
    "        self.llm = LlamaCpp(model_path=model_path, name=model_name, **model_params)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "        self.model_params = dict(model_params)\n",
    "        self.conversation = Conversation()\n",
    "        self.full_history = Conversation()\n",
    "        \n",
    "        self.sys_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a helpful assistant. You only answer questions you are very sure of. \\\n",
    "When you don't know, say \"I don't know.\" Avoid not replying at all. Please answer questions in the language being asked.\\\n",
    "你是一个友好而乐于助人的AI助手。\\\n",
    "你只回答你非常确定的问题。如果你不知道，你会如实回答“我不知道。”不能拒绝回答问题。请使用提问使用的语言进行回答。\"\"\",\n",
    "        }\n",
    "\n",
    "        # add system message to the conversation history\n",
    "        self.conversation.add_message(self.sys_msg)\n",
    "        self.full_history.add_message(self.sys_msg)\n",
    "\n",
    "        self.prompt = PromptTemplate.from_template(\"{message}\")\n",
    "        \n",
    "        self.chain = self.prompt | self.llm\n",
    "\n",
    "    def convert_message_to_llm_format(self, msg):\n",
    "        # https://huggingface.co/docs/transformers/chat_templating\n",
    "        return self.tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    def invoke(self, text: str):\n",
    "\n",
    "        inputs = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": text,\n",
    "        }\n",
    "        \n",
    "        # add the message to the memeory\n",
    "        self.conversation.add_message(inputs)\n",
    "        self.full_history.add_message(inputs)\n",
    "        \n",
    "        inputs = {'message': self.convert_message_to_llm_format(self.conversation)}\n",
    "\n",
    "        # invoke chain and format to Conversation-style response\n",
    "        response = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": self.chain.invoke(inputs),\n",
    "        }\n",
    "\n",
    "        # add response to memory\n",
    "        self.conversation.add_message(response)\n",
    "        self.full_history.add_message(response)\n",
    "\n",
    "        # prevent memory overflow\n",
    "        self._keep_k_rounds_most_recent_conversation()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        # have to create a __call__ interface for SelfQueryRetriever constructor\n",
    "        # otherwise hit TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type: <class '__main__.chatbot'>\n",
    "        self.invoke(text)\n",
    "        \n",
    "    def clear_conversation(self):\n",
    "        self.conversation = Conversation()\n",
    "\n",
    "    def _keep_k_rounds_most_recent_conversation(self):\n",
    "        k = self.model_params['conversation']['k_rounds']\n",
    "        if len(self.conversation) > 2*k:\n",
    "            # keep if system input exists\n",
    "            if self.conversation[0]['role'] == 'system':\n",
    "                self.conversation = Conversation([self.conversation[0]] + self.conversation[-2*k:])\n",
    "            else:\n",
    "                self.conversation = Conversation(self.conversation[-2*k:])\n",
    "                \n",
    "    def extract_ai_responses(self):\n",
    "        return self.full_history.generated_responses\n",
    "\n",
    "llm = chatbot(\n",
    "    'Qwen/Qwen-7B-Chat', \n",
    "    _CONFIGS['model_path']+'/'+'Qwen-7B-Chat.Q4_K_M.gguf', \n",
    "    **_CONFIGS['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4782bc2-db3e-40f8-bad7-899679a934d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm.llm,\n",
    "    vectorstore=vs_chroma,\n",
    "    document_contents='Articles and excerpts.',\n",
    "    metadata_field_info=metadata_field_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a00f1705-af14-4a31-8da9-ad13e9b8eacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"什么是我国第一部编年国别史？\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 3:prompt:FewShotPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"什么是我国第一部编年国别史？\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 3:prompt:FewShotPromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"base\",\n",
      "    \"StringPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"text\": \"Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\n\\n<< Example 1. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"teenager love\\\",\\n    \\\"filter\\\": \\\"and(or(eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Taylor Swift\\\\\\\"), eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Katy Perry\\\\\\\")), lt(\\\\\\\"length\\\\\\\", 180), eq(\\\\\\\"genre\\\\\\\", \\\\\\\"pop\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs that were not published on Spotify\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Articles and excerpts.\\\",\\n    \\\"attributes\\\": {\\n    \\\"author\\\": {\\n        \\\"description\\\": \\\"THe author of the article.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_start\\\": {\\n        \\\"description\\\": \\\"The date the article was created. In YYYY-MM-DD format.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_end\\\": {\\n        \\\"description\\\": \\\"The date the article was completed. In YYYY-MM-DD format.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"id\\\": {\\n        \\\"description\\\": \\\"The id of the text.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"name\\\": {\\n        \\\"description\\\": \\\"The name of the article.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"source\\\": {\\n        \\\"description\\\": \\\"The source of the article, representing the name of the Notion database it was retrieved\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"tags\\\": {\\n        \\\"description\\\": \\\"The different tags for an article, can represent its genre, origination, or belonged series.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    }\\n}\\n}\\n```\\n\\nUser Query:\\n什么是我国第一部编年国别史？\\n\\nStructured Request:\\n\"\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 4:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Your goal is to structure the user's query to match the request schema provided below.\\n\\n<< Structured Request Schema >>\\nWhen responding use a markdown code snippet with a JSON object formatted in the following schema:\\n\\n```json\\n{\\n    \\\"query\\\": string \\\\ text string to compare to document contents\\n    \\\"filter\\\": string \\\\ logical condition statement for filtering documents\\n}\\n```\\n\\nThe query string should contain only text that is expected to match the contents of documents. Any conditions in the filter should not be mentioned in the query as well.\\n\\nA logical condition statement is composed of one or more comparison and logical operation statements.\\n\\nA comparison statement takes the form: `comp(attr, val)`:\\n- `comp` (eq | ne | gt | gte | lt | lte): comparator\\n- `attr` (string):  name of attribute to apply the comparison to\\n- `val` (string): is the comparison value\\n\\nA logical operation statement takes the form `op(statement1, statement2, ...)`:\\n- `op` (and | or): logical operator\\n- `statement1`, `statement2`, ... (comparison statements or logical operation statements): one or more statements to apply the operation to\\n\\nMake sure that you only use the comparators and logical operators listed above and no others.\\nMake sure that filters only refer to attributes that exist in the data source.\\nMake sure that filters only use the attributed names with its function names if there are functions applied on them.\\nMake sure that filters only use format `YYYY-MM-DD` when handling date data typed values.\\nMake sure that filters take into account the descriptions of attributes and only make comparisons that are feasible given the type of data being stored.\\nMake sure that filters are only used as needed. If there are no filters that should be applied return \\\"NO_FILTER\\\" for the filter value.\\n\\n<< Example 1. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs by Taylor Swift or Katy Perry about teenage romance under 3 minutes long in the dance pop genre\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"teenager love\\\",\\n    \\\"filter\\\": \\\"and(or(eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Taylor Swift\\\\\\\"), eq(\\\\\\\"artist\\\\\\\", \\\\\\\"Katy Perry\\\\\\\")), lt(\\\\\\\"length\\\\\\\", 180), eq(\\\\\\\"genre\\\\\\\", \\\\\\\"pop\\\\\\\"))\\\"\\n}\\n```\\n\\n\\n<< Example 2. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Lyrics of a song\\\",\\n    \\\"attributes\\\": {\\n        \\\"artist\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"Name of the song artist\\\"\\n        },\\n        \\\"length\\\": {\\n            \\\"type\\\": \\\"integer\\\",\\n            \\\"description\\\": \\\"Length of the song in seconds\\\"\\n        },\\n        \\\"genre\\\": {\\n            \\\"type\\\": \\\"string\\\",\\n            \\\"description\\\": \\\"The song genre, one of \\\"pop\\\", \\\"rock\\\" or \\\"rap\\\"\\\"\\n        }\\n    }\\n}\\n```\\n\\nUser Query:\\nWhat are songs that were not published on Spotify\\n\\nStructured Request:\\n```json\\n{\\n    \\\"query\\\": \\\"\\\",\\n    \\\"filter\\\": \\\"NO_FILTER\\\"\\n}\\n```\\n\\n\\n<< Example 3. >>\\nData Source:\\n```json\\n{\\n    \\\"content\\\": \\\"Articles and excerpts.\\\",\\n    \\\"attributes\\\": {\\n    \\\"author\\\": {\\n        \\\"description\\\": \\\"THe author of the article.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_start\\\": {\\n        \\\"description\\\": \\\"The date the article was created. In YYYY-MM-DD format.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"date_end\\\": {\\n        \\\"description\\\": \\\"The date the article was completed. In YYYY-MM-DD format.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"id\\\": {\\n        \\\"description\\\": \\\"The id of the text.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"name\\\": {\\n        \\\"description\\\": \\\"The name of the article.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"source\\\": {\\n        \\\"description\\\": \\\"The source of the article, representing the name of the Notion database it was retrieved\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    },\\n    \\\"tags\\\": {\\n        \\\"description\\\": \\\"The different tags for an article, can represent its genre, origination, or belonged series.\\\",\\n        \\\"type\\\": \\\"string\\\"\\n    }\\n}\\n}\\n```\\n\\nUser Query:\\n什么是我国第一部编年国别史？\\n\\nStructured Request:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 4:llm:LlamaCpp] [6.02s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{ \\n    \\\"query\\\": \\\"first Chinese history book\\\", \\n    \\\"filter\\\": \\\"eq('source', 'Notion database 1')\\\" \\n}\\n```[PAD151645]\\n[PAD151644]'t be able to solve this issue[PAD151645]\\n\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 5:parser:StructuredQueryOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"```json\\n{ \\n    \\\"query\\\": \\\"first Chinese history book\\\", \\n    \\\"filter\\\": \\\"eq('source', 'Notion database 1')\\\" \\n}\\n```[PAD151645]\\n[PAD151644]'t be able to solve this issue[PAD151645]\\n\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence > 5:parser:StructuredQueryOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"not_implemented\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"chains\",\n",
      "    \"query_constructor\",\n",
      "    \"ir\",\n",
      "    \"StructuredQuery\"\n",
      "  ],\n",
      "  \"repr\": \"StructuredQuery(query='first Chinese history book', filter=Comparison(comparator=<Comparator.EQ: 'eq'>, attribute='source', value='Notion database 1'), limit=None)\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:retriever:Retriever > 2:chain:RunnableSequence] [6.03s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "CPU times: user 20.7 s, sys: 1.33 s, total: 22.1 s\n",
      "Wall time: 6.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   14709.21 ms\n",
      "llama_print_timings:      sample time =      17.36 ms /    48 runs   (    0.36 ms per token,  2765.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2229.65 ms /    13 tokens (  171.51 ms per token,     5.83 tokens per second)\n",
      "llama_print_timings:        eval time =    3512.33 ms /    47 runs   (   74.73 ms per token,    13.38 tokens per second)\n",
      "llama_print_timings:       total time =    6002.90 ms /    60 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "# retriever.invoke('人生有几个不捡？仅从“笑死”中找答案。')\n",
    "retriever.invoke('什么是我国第一部编年国别史？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19092d-33cf-41da-9c3a-41a3e6035089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
