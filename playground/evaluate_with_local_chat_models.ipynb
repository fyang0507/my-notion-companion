{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac2d17da-48f4-4619-934b-b68ac92bbbf9",
   "metadata": {},
   "source": [
    "# Evaluate local chat models\n",
    "\n",
    "Goal: 3valuate all interested OSS LLMs in the generic context that suit the applicaton\n",
    "* Conversational capability\n",
    "* Common knowledge base\n",
    "* Load and inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768d5d72-5c50-4e6d-8129-2df4f6a3f609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4m/z0fvcltx31xcv13t79qsnb8r0000gn/T/ipykernel_4126/425104087.py:8: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.pipelines.conversational import Conversation\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d61c536a-2caa-4eab-826e-5aca31bbba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomllib\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIG = tomllib.load(f)\n",
    "\n",
    "MODEL_PATH = _CONFIG['model_path']\n",
    "MODEL_PARAMS = _CONFIG['llm']\n",
    "MODEL_MAPPING = _CONFIG['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bffda719-6cdf-489c-8f1b-ed9152f79d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baichuan-inc/Baichuan-7B': 'baichuan2-7b-chat.Q4_K_S.gguf',\n",
       " 'hfl/chinese-alpaca-2-7b': 'chinese-alpaca-2-7b-q4_0.gguf',\n",
       " 'Qwen/Qwen-7B-Chat': 'Qwen-7B-Chat.Q4_K_M.gguf',\n",
       " '01-ai/Yi-6B-Chat': 'yi-chat-6b.Q4_K_M.gguf',\n",
       " 'HuggingFaceH4/zephyr-7b-beta': 'zephyr-7b-beta.Q4_K_M.gguf'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d2c923e-1bb4-42be-9492-6782e7b5a235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'baichuan-inc/Baichuan-7B': '/Users/fred/Documents/models/baichuan2-7b-chat.Q4_K_S.gguf',\n",
       " 'hfl/chinese-alpaca-2-7b': '/Users/fred/Documents/models/chinese-alpaca-2-7b-q4_0.gguf',\n",
       " 'Qwen/Qwen-7B-Chat': '/Users/fred/Documents/models/Qwen-7B-Chat.Q4_K_M.gguf',\n",
       " '01-ai/Yi-6B-Chat': '/Users/fred/Documents/models/yi-chat-6b.Q4_K_M.gguf',\n",
       " 'HuggingFaceH4/zephyr-7b-beta': '/Users/fred/Documents/models/zephyr-7b-beta.Q4_K_M.gguf'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_MAPPING = {k: MODEL_PATH+'/'+v for k, v in MODEL_MAPPING.items()}\n",
    "MODEL_MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62e6977-866c-4c72-ad68-05bd7c2c1b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_gpu_layers': 1,\n",
       " 'n_batch': 512,\n",
       " 'n_ctx': 2048,\n",
       " 'temperature': 0.0,\n",
       " 'f16_kv': True,\n",
       " 'conversation': {'k_rounds': 5}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb416e7-c668-44e7-8adf-5476974558e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你是谁？',\n",
       " '李白是谁？',\n",
       " '请说出李白写过的三首诗的名字。',\n",
       " '请全文背诵第二首诗。',\n",
       " '李白和杜甫认识吗？请展示你的思考过程并陈述结论。',\n",
       " '忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？',\n",
       " '请以莎士比亚为主题写一首古体诗，要求是七言绝句。',\n",
       " '请以莎士比亚为主题写一首现代诗，不超过150字。',\n",
       " 'who created you?',\n",
       " 'Name the top 3 countries in the world based on how big they are.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/test_llm_standalone_chat.txt', 'r') as f:\n",
    "    test_raw = f.readlines()\n",
    "test_cases = [x.replace(\"\\n\", \"\") for x  in test_raw[::2]]\n",
    "test_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77fccd8d-5568-43a9-ab54-7d7eaa980867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing\n",
    "# MODEL_MAPPING = {'hfl/chinese-alpaca-2-7b': '/Users/fred/Documents/models/chinese-alpaca-2-7b-q4_0.gguf'}\n",
    "# test_cases = test_cases[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b369cc3f-3bb2-458a-96bd-b20d48954812",
   "metadata": {},
   "source": [
    "Wrap into a chatbot with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e45b1e5d-581f-43cd-9bae-c6788b9f3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class chatbot:\n",
    "    def __init__(self, model_name, model_path, **model_params):\n",
    "\n",
    "        time_start = time.time()\n",
    "        self.llm = LlamaCpp(model_path=model_path, name=model_name, **model_params)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        time_end = time.time()\n",
    "        self.time_load_model = time_end - time_start\n",
    "\n",
    "        self.model_params = dict(model_params)\n",
    "        self.conversation = Conversation()\n",
    "        self.full_history = Conversation()\n",
    "        \n",
    "        self.sys_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a helpful assistant. You only answer questions you are very sure of. \\\n",
    "When you don't know, say \"I don't know.\" Avoid not replying at all. Please answer questions in the language being asked.\\\n",
    "你是一个友好而乐于助人的AI助手。\\\n",
    "你只回答你非常确定的问题。如果你不知道，你会如实回答“我不知道。”不能拒绝回答问题。请使用提问使用的语言进行回答。\"\"\",\n",
    "        }\n",
    "\n",
    "        # add system message to the conversation history\n",
    "        self.conversation.add_message(self.sys_msg)\n",
    "        self.full_history.add_message(self.sys_msg)\n",
    "\n",
    "        self.prompt = PromptTemplate.from_template(\"{message}\")\n",
    "        \n",
    "        self.chain = self.prompt | self.llm\n",
    "\n",
    "    def convert_message_to_llm_format(self, msg):\n",
    "        # https://huggingface.co/docs/transformers/chat_templating\n",
    "        return self.tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    def invoke(self, text: str):\n",
    "\n",
    "        inputs = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": text,\n",
    "        }\n",
    "        \n",
    "        # add the message to the memeory\n",
    "        self.conversation.add_message(inputs)\n",
    "        self.full_history.add_message(inputs)\n",
    "        \n",
    "        inputs = {'message': self.convert_message_to_llm_format(self.conversation)}\n",
    "\n",
    "        # invoke chain and format to Conversation-style response\n",
    "        response = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": self.chain.invoke(inputs),\n",
    "        }\n",
    "\n",
    "        # add response to memory\n",
    "        self.conversation.add_message(response)\n",
    "        self.full_history.add_message(response)\n",
    "\n",
    "        # prevent memory overflow\n",
    "        self._keep_k_rounds_most_recent_conversation()\n",
    "        \n",
    "        return response\n",
    "\n",
    "    def clear_conversation(self):\n",
    "        self.conversation = Conversation()\n",
    "\n",
    "    def _keep_k_rounds_most_recent_conversation(self):\n",
    "        k = self.model_params['conversation']['k_rounds']\n",
    "        if len(self.conversation) > 2*k:\n",
    "            # keep if system input exists\n",
    "            if self.conversation[0]['role'] == 'system':\n",
    "                self.conversation = Conversation([self.conversation[0]] + self.conversation[-2*k:])\n",
    "            else:\n",
    "                self.conversation = Conversation(self.conversation[-2*k:])\n",
    "                \n",
    "    def extract_ai_responses(self):\n",
    "        return self.full_history.generated_responses\n",
    "\n",
    "    def switch_to_all_chinese_sys_msg(self):\n",
    "        self.sys_msg = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"你是一个友好而乐于助人的AI助手。你只回答你非常确定的问题。如果你不知道，你会如实回答“我不知道。”不能拒绝回答问题。请使用提问使用的语言进行回答。\"\n",
    "            \n",
    "        }\n",
    "        self.conversation = Conversation()\n",
    "        self.full_history = Conversation()\n",
    "        self.conversation.add_message(self.sys_msg)\n",
    "        self.full_history.add_message(self.sys_msg)\n",
    "\n",
    "    def switch_to_no_sys_msg(self):\n",
    "        self.conversation = Conversation()\n",
    "        self.full_history = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9068d460-3180-4887-bc27-a2502c879fbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baichuan-inc/Baichuan-7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/fred/Documents/models/baichuan2-7b-chat.Q4_K_S.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 14\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,125696]  = [\"<unk>\", \"<s>\", \"</s>\", \"<SEP>\", \"<C...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,125696]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,125696]  = [2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  217 tensors\n",
      "llama_model_loader: - type q5_K:    8 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 1298/125696 vs 259/125696 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 125696\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Small\n",
      "llm_load_print_meta: model params     = 7.51 B\n",
      "llm_load_print_meta: model size       = 4.09 GiB (4.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 1099 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  1356.41 MiB, ( 1356.47 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4156.48 MiB\n",
      "llm_load_tensors:      Metal buffer size =  1356.39 MiB\n",
      "......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, ( 1390.03 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 1390.05 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, ( 1561.64 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   278.85 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '14', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      33.35 ms /   104 runs   (    0.32 ms per token,  3118.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8898.42 ms /   134 tokens (   66.41 ms per token,    15.06 tokens per second)\n",
      "llama_print_timings:        eval time =    5510.02 ms /   103 runs   (   53.50 ms per token,    18.69 tokens per second)\n",
      "llama_print_timings:       total time =   14862.96 ms /   237 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      17.07 ms /    52 runs   (    0.33 ms per token,  3045.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1958.82 ms /    38 tokens (   51.55 ms per token,    19.40 tokens per second)\n",
      "llama_print_timings:        eval time =    2810.61 ms /    51 runs   (   55.11 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:       total time =    4996.80 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      36.09 ms /   108 runs   (    0.33 ms per token,  2992.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2179.80 ms /    44 tokens (   49.54 ms per token,    20.19 tokens per second)\n",
      "llama_print_timings:        eval time =    5969.48 ms /   107 runs   (   55.79 ms per token,    17.92 tokens per second)\n",
      "llama_print_timings:       total time =    8610.64 ms /   151 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      47.24 ms /   141 runs   (    0.34 ms per token,  2984.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2001.56 ms /    41 tokens (   48.82 ms per token,    20.48 tokens per second)\n",
      "llama_print_timings:        eval time =    8272.16 ms /   140 runs   (   59.09 ms per token,    16.92 tokens per second)\n",
      "llama_print_timings:       total time =   10883.33 ms /   181 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      84.38 ms /   256 runs   (    0.33 ms per token,  3034.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2070.56 ms /    50 tokens (   41.41 ms per token,    24.15 tokens per second)\n",
      "llama_print_timings:        eval time =   15172.09 ms /   255 runs   (   59.50 ms per token,    16.81 tokens per second)\n",
      "llama_print_timings:       total time =   18356.43 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      64.45 ms /   186 runs   (    0.35 ms per token,  2885.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2101.09 ms /    55 tokens (   38.20 ms per token,    26.18 tokens per second)\n",
      "llama_print_timings:        eval time =   11170.68 ms /   185 runs   (   60.38 ms per token,    16.56 tokens per second)\n",
      "llama_print_timings:       total time =   14089.34 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      18.53 ms /    49 runs   (    0.38 ms per token,  2644.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12811.82 ms /  1000 tokens (   12.81 ms per token,    78.05 tokens per second)\n",
      "llama_print_timings:        eval time =    2886.05 ms /    48 runs   (   60.13 ms per token,    16.63 tokens per second)\n",
      "llama_print_timings:       total time =   15917.45 ms /  1048 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      30.54 ms /    79 runs   (    0.39 ms per token,  2586.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12852.10 ms /  1008 tokens (   12.75 ms per token,    78.43 tokens per second)\n",
      "llama_print_timings:        eval time =    4668.77 ms /    78 runs   (   59.86 ms per token,    16.71 tokens per second)\n",
      "llama_print_timings:       total time =   17872.44 ms /  1086 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      19.59 ms /    55 runs   (    0.36 ms per token,  2806.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12557.93 ms /   972 tokens (   12.92 ms per token,    77.40 tokens per second)\n",
      "llama_print_timings:        eval time =    3358.67 ms /    54 runs   (   62.20 ms per token,    16.08 tokens per second)\n",
      "llama_print_timings:       total time =   16161.30 ms /  1026 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      23.40 ms /    69 runs   (    0.34 ms per token,  2948.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11664.31 ms /   897 tokens (   13.00 ms per token,    76.90 tokens per second)\n",
      "llama_print_timings:        eval time =    3994.36 ms /    68 runs   (   58.74 ms per token,    17.02 tokens per second)\n",
      "llama_print_timings:       total time =   15960.05 ms /   965 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      17.78 ms /    49 runs   (    0.36 ms per token,  2755.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2138.24 ms /    76 tokens (   28.13 ms per token,    35.54 tokens per second)\n",
      "llama_print_timings:        eval time =    2420.69 ms /    48 runs   (   50.43 ms per token,    19.83 tokens per second)\n",
      "llama_print_timings:       total time =    4770.85 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      41.33 ms /   126 runs   (    0.33 ms per token,  3048.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1966.08 ms /    60 tokens (   32.77 ms per token,    30.52 tokens per second)\n",
      "llama_print_timings:        eval time =    6519.13 ms /   125 runs   (   52.15 ms per token,    19.17 tokens per second)\n",
      "llama_print_timings:       total time =    9037.32 ms /   185 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      32.43 ms /    98 runs   (    0.33 ms per token,  3021.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1958.15 ms /    44 tokens (   44.50 ms per token,    22.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5234.30 ms /    97 runs   (   53.96 ms per token,    18.53 tokens per second)\n",
      "llama_print_timings:       total time =    7623.46 ms /   141 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      27.65 ms /    85 runs   (    0.33 ms per token,  3074.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2653.60 ms /   134 tokens (   19.80 ms per token,    50.50 tokens per second)\n",
      "llama_print_timings:        eval time =    4760.21 ms /    84 runs   (   56.67 ms per token,    17.65 tokens per second)\n",
      "llama_print_timings:       total time =    7788.61 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      53.87 ms /   159 runs   (    0.34 ms per token,  2951.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2041.72 ms /    50 tokens (   40.83 ms per token,    24.49 tokens per second)\n",
      "llama_print_timings:        eval time =    9108.56 ms /   158 runs   (   57.65 ms per token,    17.35 tokens per second)\n",
      "llama_print_timings:       total time =   11854.38 ms /   208 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      44.70 ms /   124 runs   (    0.36 ms per token,  2774.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3124.49 ms /   162 tokens (   19.29 ms per token,    51.85 tokens per second)\n",
      "llama_print_timings:        eval time =    7171.19 ms /   123 runs   (   58.30 ms per token,    17.15 tokens per second)\n",
      "llama_print_timings:       total time =   10844.80 ms /   285 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      29.85 ms /    86 runs   (    0.35 ms per token,  2880.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10817.14 ms /   848 tokens (   12.76 ms per token,    78.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4954.96 ms /    85 runs   (   58.29 ms per token,    17.15 tokens per second)\n",
      "llama_print_timings:       total time =   16155.77 ms /   933 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      31.11 ms /    87 runs   (    0.36 ms per token,  2796.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10446.62 ms /   816 tokens (   12.80 ms per token,    78.11 tokens per second)\n",
      "llama_print_timings:        eval time =    4978.79 ms /    86 runs   (   57.89 ms per token,    17.27 tokens per second)\n",
      "llama_print_timings:       total time =   15810.23 ms /   902 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      20.38 ms /    55 runs   (    0.37 ms per token,  2699.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10189.89 ms /   798 tokens (   12.77 ms per token,    78.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3112.41 ms /    54 runs   (   57.64 ms per token,    17.35 tokens per second)\n",
      "llama_print_timings:       total time =   13547.33 ms /   852 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      24.66 ms /    70 runs   (    0.35 ms per token,  2839.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10119.66 ms /   779 tokens (   12.99 ms per token,    76.98 tokens per second)\n",
      "llama_print_timings:        eval time =    3987.25 ms /    69 runs   (   57.79 ms per token,    17.31 tokens per second)\n",
      "llama_print_timings:       total time =   14417.66 ms /   848 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =       4.69 ms /    14 runs   (    0.33 ms per token,  2986.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     834.98 ms /    22 tokens (   37.95 ms per token,    26.35 tokens per second)\n",
      "llama_print_timings:        eval time =     630.08 ms /    13 runs   (   48.47 ms per token,    20.63 tokens per second)\n",
      "llama_print_timings:       total time =    1527.01 ms /    35 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      23.65 ms /    70 runs   (    0.34 ms per token,  2959.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1935.74 ms /    38 tokens (   50.94 ms per token,    19.63 tokens per second)\n",
      "llama_print_timings:        eval time =    3420.74 ms /    69 runs   (   49.58 ms per token,    20.17 tokens per second)\n",
      "llama_print_timings:       total time =    5663.67 ms /   107 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      80.16 ms /   237 runs   (    0.34 ms per token,  2956.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1944.14 ms /    44 tokens (   44.18 ms per token,    22.63 tokens per second)\n",
      "llama_print_timings:        eval time =   12309.52 ms /   236 runs   (   52.16 ms per token,    19.17 tokens per second)\n",
      "llama_print_timings:       total time =   15306.16 ms /   280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      26.43 ms /    77 runs   (    0.34 ms per token,  2912.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1980.27 ms /    41 tokens (   48.30 ms per token,    20.70 tokens per second)\n",
      "llama_print_timings:        eval time =    4187.27 ms /    76 runs   (   55.10 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:       total time =    6506.12 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      52.44 ms /   155 runs   (    0.34 ms per token,  2955.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2029.11 ms /    50 tokens (   40.58 ms per token,    24.64 tokens per second)\n",
      "llama_print_timings:        eval time =    8787.68 ms /   154 runs   (   57.06 ms per token,    17.52 tokens per second)\n",
      "llama_print_timings:       total time =   11503.05 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      60.03 ms /   159 runs   (    0.38 ms per token,  2648.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2085.26 ms /    54 tokens (   38.62 ms per token,    25.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9158.41 ms /   158 runs   (   57.96 ms per token,    17.25 tokens per second)\n",
      "llama_print_timings:       total time =   11951.00 ms /   212 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      16.83 ms /    44 runs   (    0.38 ms per token,  2613.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11850.34 ms /   952 tokens (   12.45 ms per token,    80.34 tokens per second)\n",
      "llama_print_timings:        eval time =    2538.48 ms /    43 runs   (   59.03 ms per token,    16.94 tokens per second)\n",
      "llama_print_timings:       total time =   14589.97 ms /   995 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      17.26 ms /    49 runs   (    0.35 ms per token,  2838.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11721.78 ms /   938 tokens (   12.50 ms per token,    80.02 tokens per second)\n",
      "llama_print_timings:        eval time =    2810.39 ms /    48 runs   (   58.55 ms per token,    17.08 tokens per second)\n",
      "llama_print_timings:       total time =   14751.66 ms /   986 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      18.08 ms /    49 runs   (    0.37 ms per token,  2710.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9791.60 ms /   744 tokens (   13.16 ms per token,    75.98 tokens per second)\n",
      "llama_print_timings:        eval time =    2730.13 ms /    48 runs   (   56.88 ms per token,    17.58 tokens per second)\n",
      "llama_print_timings:       total time =   12743.77 ms /   792 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8898.55 ms\n",
      "llama_print_timings:      sample time =      25.57 ms /    71 runs   (    0.36 ms per token,  2776.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9412.82 ms /   728 tokens (   12.93 ms per token,    77.34 tokens per second)\n",
      "llama_print_timings:        eval time =    3990.71 ms /    70 runs   (   57.01 ms per token,    17.54 tokens per second)\n",
      "llama_print_timings:       total time =   13721.83 ms /   798 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/fred/Documents/models/chinese-alpaca-2-7b-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,55296]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,55296]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,55296]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 889/55296 vs 259/55296 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 55296\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.93 B\n",
      "llm_load_print_meta: model size       = 3.69 GiB (4.57 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   108.61 MiB, ( 1670.25 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3773.70 MiB\n",
      "llm_load_tensors:      Metal buffer size =   108.60 MiB\n",
      "..............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hfl/chinese-alpaca-2-7b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, ( 1702.25 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, ( 1702.27 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, ( 1873.86 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   167.20 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       3.74 ms /    23 runs   (    0.16 ms per token,  6156.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7900.47 ms /   115 tokens (   68.70 ms per token,    14.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1058.22 ms /    22 runs   (   48.10 ms per token,    20.79 tokens per second)\n",
      "llama_print_timings:       total time =    9004.90 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      14.97 ms /    95 runs   (    0.16 ms per token,  6346.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =     536.70 ms /    14 tokens (   38.34 ms per token,    26.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4577.35 ms /    94 runs   (   48.70 ms per token,    20.54 tokens per second)\n",
      "llama_print_timings:       total time =    5307.95 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      25.51 ms /   159 runs   (    0.16 ms per token,  6232.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1711.65 ms /    82 tokens (   20.87 ms per token,    47.91 tokens per second)\n",
      "llama_print_timings:        eval time =    7997.06 ms /   158 runs   (   50.61 ms per token,    19.76 tokens per second)\n",
      "llama_print_timings:       total time =   10038.35 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       8.23 ms /    57 runs   (    0.14 ms per token,  6924.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     722.28 ms /    18 tokens (   40.13 ms per token,    24.92 tokens per second)\n",
      "llama_print_timings:        eval time =    2925.17 ms /    56 runs   (   52.24 ms per token,    19.14 tokens per second)\n",
      "llama_print_timings:       total time =    3763.40 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      24.38 ms /   152 runs   (    0.16 ms per token,  6235.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1051.27 ms /    27 tokens (   38.94 ms per token,    25.68 tokens per second)\n",
      "llama_print_timings:        eval time =    8154.36 ms /   151 runs   (   54.00 ms per token,    18.52 tokens per second)\n",
      "llama_print_timings:       total time =    9521.36 ms /   178 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      12.22 ms /    69 runs   (    0.18 ms per token,  5647.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1581.79 ms /    34 tokens (   46.52 ms per token,    21.49 tokens per second)\n",
      "llama_print_timings:        eval time =    3725.98 ms /    68 runs   (   54.79 ms per token,    18.25 tokens per second)\n",
      "llama_print_timings:       total time =    5450.78 ms /   102 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.87 ms /    48 runs   (    0.16 ms per token,  6096.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7946.82 ms /   665 tokens (   11.95 ms per token,    83.68 tokens per second)\n",
      "llama_print_timings:        eval time =    2565.31 ms /    47 runs   (   54.58 ms per token,    18.32 tokens per second)\n",
      "llama_print_timings:       total time =   10613.34 ms /   712 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.71 ms /    48 runs   (    0.16 ms per token,  6223.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7715.30 ms /   634 tokens (   12.17 ms per token,    82.17 tokens per second)\n",
      "llama_print_timings:        eval time =    2528.10 ms /    47 runs   (   53.79 ms per token,    18.59 tokens per second)\n",
      "llama_print_timings:       total time =   10342.44 ms /   681 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      11.13 ms /    68 runs   (    0.16 ms per token,  6111.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5568.16 ms /   514 tokens (   10.83 ms per token,    92.31 tokens per second)\n",
      "llama_print_timings:        eval time =    3618.81 ms /    67 runs   (   54.01 ms per token,    18.51 tokens per second)\n",
      "llama_print_timings:       total time =    9328.35 ms /   581 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.96 ms /    50 runs   (    0.16 ms per token,  6277.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6393.46 ms /   535 tokens (   11.95 ms per token,    83.68 tokens per second)\n",
      "llama_print_timings:        eval time =    2673.57 ms /    49 runs   (   54.56 ms per token,    18.33 tokens per second)\n",
      "llama_print_timings:       total time =    9169.16 ms /   584 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       2.62 ms /    16 runs   (    0.16 ms per token,  6116.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1486.80 ms /    57 tokens (   26.08 ms per token,    38.34 tokens per second)\n",
      "llama_print_timings:        eval time =     715.30 ms /    15 runs   (   47.69 ms per token,    20.97 tokens per second)\n",
      "llama_print_timings:       total time =    2234.27 ms /    72 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      14.93 ms /    95 runs   (    0.16 ms per token,  6365.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =     535.28 ms /    14 tokens (   38.23 ms per token,    26.15 tokens per second)\n",
      "llama_print_timings:        eval time =    4415.46 ms /    94 runs   (   46.97 ms per token,    21.29 tokens per second)\n",
      "llama_print_timings:       total time =    5145.30 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       6.99 ms /    47 runs   (    0.15 ms per token,  6720.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1825.46 ms /    82 tokens (   22.26 ms per token,    44.92 tokens per second)\n",
      "llama_print_timings:        eval time =    2233.33 ms /    46 runs   (   48.55 ms per token,    20.60 tokens per second)\n",
      "llama_print_timings:       total time =    4153.31 ms /   128 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      14.46 ms /    84 runs   (    0.17 ms per token,  5809.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     694.84 ms /    18 tokens (   38.60 ms per token,    25.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4086.55 ms /    83 runs   (   49.24 ms per token,    20.31 tokens per second)\n",
      "llama_print_timings:       total time =    4950.21 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      14.57 ms /    96 runs   (    0.15 ms per token,  6587.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1022.94 ms /    27 tokens (   37.89 ms per token,    26.39 tokens per second)\n",
      "llama_print_timings:        eval time =    4896.39 ms /    95 runs   (   51.54 ms per token,    19.40 tokens per second)\n",
      "llama_print_timings:       total time =    6110.59 ms /   122 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      11.56 ms /    68 runs   (    0.17 ms per token,  5883.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1543.87 ms /    34 tokens (   45.41 ms per token,    22.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3548.62 ms /    67 runs   (   52.96 ms per token,    18.88 tokens per second)\n",
      "llama_print_timings:       total time =    5229.04 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.82 ms /    50 runs   (    0.16 ms per token,  6391.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5856.65 ms /   523 tokens (   11.20 ms per token,    89.30 tokens per second)\n",
      "llama_print_timings:        eval time =    2641.23 ms /    49 runs   (   53.90 ms per token,    18.55 tokens per second)\n",
      "llama_print_timings:       total time =    8599.36 ms /   572 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.52 ms /    50 runs   (    0.15 ms per token,  6650.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5311.16 ms /   494 tokens (   10.75 ms per token,    93.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2586.27 ms /    49 runs   (   52.78 ms per token,    18.95 tokens per second)\n",
      "llama_print_timings:       total time =    7996.32 ms /   543 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.24 ms /    45 runs   (    0.16 ms per token,  6217.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5248.59 ms /   488 tokens (   10.76 ms per token,    92.98 tokens per second)\n",
      "llama_print_timings:        eval time =    2318.86 ms /    44 runs   (   52.70 ms per token,    18.97 tokens per second)\n",
      "llama_print_timings:       total time =    7656.98 ms /   532 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       7.99 ms /    50 runs   (    0.16 ms per token,  6255.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5071.81 ms /   459 tokens (   11.05 ms per token,    90.50 tokens per second)\n",
      "llama_print_timings:        eval time =    2604.72 ms /    49 runs   (   53.16 ms per token,    18.81 tokens per second)\n",
      "llama_print_timings:       total time =    7778.25 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       3.64 ms /    23 runs   (    0.16 ms per token,  6315.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     275.16 ms /     7 tokens (   39.31 ms per token,    25.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1012.04 ms /    22 runs   (   46.00 ms per token,    21.74 tokens per second)\n",
      "llama_print_timings:       total time =    1333.66 ms /    29 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      12.18 ms /    75 runs   (    0.16 ms per token,  6158.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     537.40 ms /    14 tokens (   38.39 ms per token,    26.05 tokens per second)\n",
      "llama_print_timings:        eval time =    3503.17 ms /    74 runs   (   47.34 ms per token,    21.12 tokens per second)\n",
      "llama_print_timings:       total time =    4194.63 ms /    88 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      25.99 ms /   162 runs   (    0.16 ms per token,  6233.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1507.99 ms /    63 tokens (   23.94 ms per token,    41.78 tokens per second)\n",
      "llama_print_timings:        eval time =    7809.47 ms /   161 runs   (   48.51 ms per token,    20.62 tokens per second)\n",
      "llama_print_timings:       total time =    9654.85 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =       8.82 ms /    57 runs   (    0.15 ms per token,  6465.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =     699.27 ms /    18 tokens (   38.85 ms per token,    25.74 tokens per second)\n",
      "llama_print_timings:        eval time =    2769.25 ms /    56 runs   (   49.45 ms per token,    20.22 tokens per second)\n",
      "llama_print_timings:       total time =    3583.88 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      33.91 ms /   214 runs   (    0.16 ms per token,  6310.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1052.29 ms /    27 tokens (   38.97 ms per token,    25.66 tokens per second)\n",
      "llama_print_timings:        eval time =   11269.11 ms /   213 runs   (   52.91 ms per token,    18.90 tokens per second)\n",
      "llama_print_timings:       total time =   12770.69 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      11.76 ms /    67 runs   (    0.18 ms per token,  5699.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2607.65 ms /   174 tokens (   14.99 ms per token,    66.73 tokens per second)\n",
      "llama_print_timings:        eval time =    3548.42 ms /    66 runs   (   53.76 ms per token,    18.60 tokens per second)\n",
      "llama_print_timings:       total time =    6294.59 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      17.11 ms /   109 runs   (    0.16 ms per token,  6370.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8293.60 ms /   706 tokens (   11.75 ms per token,    85.13 tokens per second)\n",
      "llama_print_timings:        eval time =    5975.65 ms /   108 runs   (   55.33 ms per token,    18.07 tokens per second)\n",
      "llama_print_timings:       total time =   14496.27 ms /   814 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      12.11 ms /    77 runs   (    0.16 ms per token,  6356.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8646.11 ms /   757 tokens (   11.42 ms per token,    87.55 tokens per second)\n",
      "llama_print_timings:        eval time =    4170.60 ms /    76 runs   (   54.88 ms per token,    18.22 tokens per second)\n",
      "llama_print_timings:       total time =   12977.44 ms /   833 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      11.30 ms /    71 runs   (    0.16 ms per token,  6284.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7726.86 ms /   662 tokens (   11.67 ms per token,    85.68 tokens per second)\n",
      "llama_print_timings:        eval time =    3830.41 ms /    70 runs   (   54.72 ms per token,    18.27 tokens per second)\n",
      "llama_print_timings:       total time =   11705.50 ms /   732 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7900.57 ms\n",
      "llama_print_timings:      sample time =      19.58 ms /   122 runs   (    0.16 ms per token,  6231.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8039.78 ms /   686 tokens (   11.72 ms per token,    85.33 tokens per second)\n",
      "llama_print_timings:        eval time =    6634.77 ms /   121 runs   (   54.83 ms per token,    18.24 tokens per second)\n",
      "llama_print_timings:       total time =   14928.38 ms /   807 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 259 tensors from /Users/fred/Documents/models/Qwen-7B-Chat.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
      "llama_model_loader: - kv   2:                        qwen.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                           qwen.block_count u32              = 32\n",
      "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q4_K:  113 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen-7B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.56 GiB (5.07 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen\n",
      "llm_load_print_meta: BOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: EOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: UNK token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   612.59 MiB, (  926.44 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4666.59 MiB\n",
      "llm_load_tensors:      Metal buffer size =   612.59 MiB\n",
      "....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   992.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    32.00 MiB, (  958.44 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  958.45 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   162.81 MiB, ( 1121.25 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   162.80 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   335.23 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151643', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen.attention.head_count': '32', 'qwen.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '151643', 'qwen.feed_forward_length': '22016', 'qwen.attention.layer_norm_rms_epsilon': '0.000001', 'qwen.embedding_length': '4096', 'qwen.rope.dimension_count': '128', 'qwen.context_length': '32768', 'qwen.block_count': '32', 'general.name': 'Qwen', 'general.architecture': 'qwen'}\n",
      "ggml_metal_free: deallocating\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =       8.69 ms /    19 runs   (    0.46 ms per token,  2185.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9509.75 ms /   127 tokens (   74.88 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1090.00 ms /    18 runs   (   60.56 ms per token,    16.51 tokens per second)\n",
      "llama_print_timings:       total time =   10699.88 ms /   145 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      13.43 ms /    31 runs   (    0.43 ms per token,  2308.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1979.05 ms /    33 tokens (   59.97 ms per token,    16.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1797.89 ms /    30 runs   (   59.93 ms per token,    16.69 tokens per second)\n",
      "llama_print_timings:       total time =    3938.27 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      13.04 ms /    27 runs   (    0.48 ms per token,  2071.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2001.90 ms /    39 tokens (   51.33 ms per token,    19.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1575.99 ms /    26 runs   (   60.61 ms per token,    16.50 tokens per second)\n",
      "llama_print_timings:       total time =    3720.44 ms /    65 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      83.60 ms /   202 runs   (    0.41 ms per token,  2416.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2012.29 ms /    37 tokens (   54.39 ms per token,    18.39 tokens per second)\n",
      "llama_print_timings:        eval time =   12724.80 ms /   201 runs   (   63.31 ms per token,    15.80 tokens per second)\n",
      "llama_print_timings:       total time =   15805.71 ms /   238 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      28.78 ms /    66 runs   (    0.44 ms per token,  2293.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3688.16 ms /   234 tokens (   15.76 ms per token,    63.45 tokens per second)\n",
      "llama_print_timings:        eval time =    4308.65 ms /    65 runs   (   66.29 ms per token,    15.09 tokens per second)\n",
      "llama_print_timings:       total time =    8345.52 ms /   299 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      10.18 ms /    25 runs   (    0.41 ms per token,  2456.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2346.87 ms /    72 tokens (   32.60 ms per token,    30.68 tokens per second)\n",
      "llama_print_timings:        eval time =    1565.55 ms /    24 runs   (   65.23 ms per token,    15.33 tokens per second)\n",
      "llama_print_timings:       total time =    4043.32 ms /    96 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      13.44 ms /    29 runs   (    0.46 ms per token,  2158.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8594.34 ms /   597 tokens (   14.40 ms per token,    69.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1883.84 ms /    28 runs   (   67.28 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =   10636.91 ms /   625 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      39.59 ms /    90 runs   (    0.44 ms per token,  2273.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8746.45 ms /   611 tokens (   14.31 ms per token,    69.86 tokens per second)\n",
      "llama_print_timings:        eval time =    5871.29 ms /    89 runs   (   65.97 ms per token,    15.16 tokens per second)\n",
      "llama_print_timings:       total time =   15088.39 ms /   700 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =       9.69 ms /    22 runs   (    0.44 ms per token,  2269.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9312.64 ms /   678 tokens (   13.74 ms per token,    72.80 tokens per second)\n",
      "llama_print_timings:        eval time =    1449.89 ms /    21 runs   (   69.04 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =   10881.85 ms /   699 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      54.31 ms /   125 runs   (    0.43 ms per token,  2301.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5885.86 ms /   496 tokens (   11.87 ms per token,    84.27 tokens per second)\n",
      "llama_print_timings:        eval time =    8018.88 ms /   124 runs   (   64.67 ms per token,    15.46 tokens per second)\n",
      "llama_print_timings:       total time =   14553.73 ms /   620 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      22.38 ms /    49 runs   (    0.46 ms per token,  2189.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2160.22 ms /    73 tokens (   29.59 ms per token,    33.79 tokens per second)\n",
      "llama_print_timings:        eval time =    2772.82 ms /    48 runs   (   57.77 ms per token,    17.31 tokens per second)\n",
      "llama_print_timings:       total time =    5188.67 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      22.92 ms /    48 runs   (    0.48 ms per token,  2094.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1992.79 ms /    33 tokens (   60.39 ms per token,    16.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2740.86 ms /    47 runs   (   58.32 ms per token,    17.15 tokens per second)\n",
      "llama_print_timings:       total time =    4985.00 ms /    80 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      19.64 ms /    44 runs   (    0.45 ms per token,  2240.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2083.20 ms /    39 tokens (   53.42 ms per token,    18.72 tokens per second)\n",
      "llama_print_timings:        eval time =    2537.94 ms /    43 runs   (   59.02 ms per token,    16.94 tokens per second)\n",
      "llama_print_timings:       total time =    4850.98 ms /    82 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      82.46 ms /   198 runs   (    0.42 ms per token,  2401.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2021.90 ms /    37 tokens (   54.65 ms per token,    18.30 tokens per second)\n",
      "llama_print_timings:        eval time =   12107.63 ms /   197 runs   (   61.46 ms per token,    16.27 tokens per second)\n",
      "llama_print_timings:       total time =   15161.71 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      41.23 ms /   105 runs   (    0.39 ms per token,  2547.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3702.92 ms /   251 tokens (   14.75 ms per token,    67.78 tokens per second)\n",
      "llama_print_timings:        eval time =    6733.67 ms /   104 runs   (   64.75 ms per token,    15.44 tokens per second)\n",
      "llama_print_timings:       total time =   10977.24 ms /   355 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      32.26 ms /    80 runs   (    0.40 ms per token,  2480.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2376.37 ms /    91 tokens (   26.11 ms per token,    38.29 tokens per second)\n",
      "llama_print_timings:        eval time =    5195.67 ms /    79 runs   (   65.77 ms per token,    15.20 tokens per second)\n",
      "llama_print_timings:       total time =    7985.51 ms /   170 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      11.68 ms /    29 runs   (    0.40 ms per token,  2483.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9559.96 ms /   722 tokens (   13.24 ms per token,    75.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1833.53 ms /    28 runs   (   65.48 ms per token,    15.27 tokens per second)\n",
      "llama_print_timings:       total time =   11546.05 ms /   750 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      30.64 ms /    78 runs   (    0.39 ms per token,  2545.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9602.35 ms /   719 tokens (   13.36 ms per token,    74.88 tokens per second)\n",
      "llama_print_timings:        eval time =    5026.27 ms /    77 runs   (   65.28 ms per token,    15.32 tokens per second)\n",
      "llama_print_timings:       total time =   15035.48 ms /   796 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      17.15 ms /    39 runs   (    0.44 ms per token,  2274.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9900.03 ms /   753 tokens (   13.15 ms per token,    76.06 tokens per second)\n",
      "llama_print_timings:        eval time =    2504.31 ms /    38 runs   (   65.90 ms per token,    15.17 tokens per second)\n",
      "llama_print_timings:       total time =   12611.25 ms /   791 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      60.30 ms /   138 runs   (    0.44 ms per token,  2288.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8355.99 ms /   591 tokens (   14.14 ms per token,    70.73 tokens per second)\n",
      "llama_print_timings:        eval time =    8817.64 ms /   137 runs   (   64.36 ms per token,    15.54 tokens per second)\n",
      "llama_print_timings:       total time =   17893.44 ms /   728 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      20.48 ms /    49 runs   (    0.42 ms per token,  2392.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =     905.88 ms /    20 tokens (   45.29 ms per token,    22.08 tokens per second)\n",
      "llama_print_timings:        eval time =    2675.66 ms /    48 runs   (   55.74 ms per token,    17.94 tokens per second)\n",
      "llama_print_timings:       total time =    3837.28 ms /    68 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      23.77 ms /    57 runs   (    0.42 ms per token,  2397.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1962.84 ms /    33 tokens (   59.48 ms per token,    16.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3207.27 ms /    56 runs   (   57.27 ms per token,    17.46 tokens per second)\n",
      "llama_print_timings:       total time =    5466.23 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      20.15 ms /    45 runs   (    0.45 ms per token,  2233.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1986.41 ms /    39 tokens (   50.93 ms per token,    19.63 tokens per second)\n",
      "llama_print_timings:        eval time =    2544.48 ms /    44 runs   (   57.83 ms per token,    17.29 tokens per second)\n",
      "llama_print_timings:       total time =    4766.36 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      75.38 ms /   198 runs   (    0.38 ms per token,  2626.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1984.20 ms /    37 tokens (   53.63 ms per token,    18.65 tokens per second)\n",
      "llama_print_timings:        eval time =   11933.20 ms /   197 runs   (   60.57 ms per token,    16.51 tokens per second)\n",
      "llama_print_timings:       total time =   14945.88 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      53.97 ms /   140 runs   (    0.39 ms per token,  2594.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3637.67 ms /   251 tokens (   14.49 ms per token,    69.00 tokens per second)\n",
      "llama_print_timings:        eval time =    8914.74 ms /   139 runs   (   64.13 ms per token,    15.59 tokens per second)\n",
      "llama_print_timings:       total time =   13279.64 ms /   390 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      33.42 ms /    81 runs   (    0.41 ms per token,  2423.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2898.97 ms /   154 tokens (   18.82 ms per token,    53.12 tokens per second)\n",
      "llama_print_timings:        eval time =    5139.30 ms /    80 runs   (   64.24 ms per token,    15.57 tokens per second)\n",
      "llama_print_timings:       total time =    8456.81 ms /   234 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      12.39 ms /    29 runs   (    0.43 ms per token,  2341.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10018.59 ms /   769 tokens (   13.03 ms per token,    76.76 tokens per second)\n",
      "llama_print_timings:        eval time =    1810.21 ms /    28 runs   (   64.65 ms per token,    15.47 tokens per second)\n",
      "llama_print_timings:       total time =   11983.53 ms /   797 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      30.64 ms /    77 runs   (    0.40 ms per token,  2512.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9736.94 ms /   757 tokens (   12.86 ms per token,    77.75 tokens per second)\n",
      "llama_print_timings:        eval time =    4968.56 ms /    76 runs   (   65.38 ms per token,    15.30 tokens per second)\n",
      "llama_print_timings:       total time =   15106.97 ms /   833 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      17.61 ms /    39 runs   (    0.45 ms per token,  2215.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10111.56 ms /   793 tokens (   12.75 ms per token,    78.43 tokens per second)\n",
      "llama_print_timings:        eval time =    2457.62 ms /    38 runs   (   64.67 ms per token,    15.46 tokens per second)\n",
      "llama_print_timings:       total time =   12777.57 ms /   831 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9509.88 ms\n",
      "llama_print_timings:      sample time =      60.74 ms /   138 runs   (    0.44 ms per token,  2271.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8606.25 ms /   631 tokens (   13.64 ms per token,    73.32 tokens per second)\n",
      "llama_print_timings:        eval time =    8768.01 ms /   137 runs   (   64.00 ms per token,    15.62 tokens per second)\n",
      "llama_print_timings:       total time =   18096.48 ms /   768 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /Users/fred/Documents/models/yi-chat-6b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 5000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,64000]   = [\"<unk>\", \"<|startoftext|>\", \"<|endof...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,64000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,64000]   = [2, 3, 3, 3, 3, 3, 1, 1, 1, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 498/64000 vs 267/64000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 64000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 4\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 8\n",
      "llm_load_print_meta: n_embd_k_gqa     = 512\n",
      "llm_load_print_meta: n_embd_v_gqa     = 512\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 5000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.06 B\n",
      "llm_load_print_meta: model size       = 3.42 GiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<|startoftext|>'\n",
      "llm_load_print_meta: EOS token        = 2 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 315 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   104.47 MiB, (  913.50 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3502.34 MiB\n",
      "llm_load_tensors:      Metal buffer size =   104.46 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   124.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     4.00 MiB, (  917.50 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =     4.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  917.52 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, ( 1089.11 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   167.20 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '4', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '5000000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01-ai/Yi-6B-Chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      44.29 ms /   256 runs   (    0.17 ms per token,  5779.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7028.83 ms /   107 tokens (   65.69 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:        eval time =   11077.05 ms /   255 runs   (   43.44 ms per token,    23.02 tokens per second)\n",
      "llama_print_timings:       total time =   18697.46 ms /   362 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.55 ms /   256 runs   (    0.18 ms per token,  5499.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =     507.29 ms /    15 tokens (   33.82 ms per token,    29.57 tokens per second)\n",
      "llama_print_timings:        eval time =   11358.79 ms /   255 runs   (   44.54 ms per token,    22.45 tokens per second)\n",
      "llama_print_timings:       total time =   12459.06 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      48.15 ms /   256 runs   (    0.19 ms per token,  5316.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =     763.23 ms /    22 tokens (   34.69 ms per token,    28.82 tokens per second)\n",
      "llama_print_timings:        eval time =   11548.40 ms /   255 runs   (   45.29 ms per token,    22.08 tokens per second)\n",
      "llama_print_timings:       total time =   12906.82 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      47.50 ms /   256 runs   (    0.19 ms per token,  5389.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =     661.86 ms /    19 tokens (   34.83 ms per token,    28.71 tokens per second)\n",
      "llama_print_timings:        eval time =   11609.73 ms /   255 runs   (   45.53 ms per token,    21.96 tokens per second)\n",
      "llama_print_timings:       total time =   12865.53 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      44.17 ms /   256 runs   (    0.17 ms per token,  5795.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     982.30 ms /    28 tokens (   35.08 ms per token,    28.50 tokens per second)\n",
      "llama_print_timings:        eval time =   11828.60 ms /   255 runs   (   46.39 ms per token,    21.56 tokens per second)\n",
      "llama_print_timings:       total time =   13403.36 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      45.74 ms /   256 runs   (    0.18 ms per token,  5597.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1786.93 ms /    33 tokens (   54.15 ms per token,    18.47 tokens per second)\n",
      "llama_print_timings:        eval time =   12114.55 ms /   255 runs   (   47.51 ms per token,    21.05 tokens per second)\n",
      "llama_print_timings:       total time =   14495.66 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.32 ms /   256 runs   (    0.18 ms per token,  5526.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16013.37 ms /  1417 tokens (   11.30 ms per token,    88.49 tokens per second)\n",
      "llama_print_timings:        eval time =   12062.84 ms /   255 runs   (   47.31 ms per token,    21.14 tokens per second)\n",
      "llama_print_timings:       total time =   28671.38 ms /  1672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      47.35 ms /   256 runs   (    0.18 ms per token,  5406.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16106.62 ms /  1431 tokens (   11.26 ms per token,    88.85 tokens per second)\n",
      "llama_print_timings:        eval time =   12113.26 ms /   255 runs   (   47.50 ms per token,    21.05 tokens per second)\n",
      "llama_print_timings:       total time =   28813.36 ms /  1686 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      49.68 ms /   256 runs   (    0.19 ms per token,  5153.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16092.28 ms /  1424 tokens (   11.30 ms per token,    88.49 tokens per second)\n",
      "llama_print_timings:        eval time =   12162.47 ms /   255 runs   (   47.70 ms per token,    20.97 tokens per second)\n",
      "llama_print_timings:       total time =   28857.60 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      44.75 ms /   256 runs   (    0.17 ms per token,  5721.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16058.15 ms /  1434 tokens (   11.20 ms per token,    89.30 tokens per second)\n",
      "llama_print_timings:        eval time =   12035.74 ms /   255 runs   (   47.20 ms per token,    21.19 tokens per second)\n",
      "llama_print_timings:       total time =   28695.13 ms /  1689 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.74 ms /   256 runs   (    0.18 ms per token,  5476.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1668.77 ms /    55 tokens (   30.34 ms per token,    32.96 tokens per second)\n",
      "llama_print_timings:        eval time =   11094.99 ms /   255 runs   (   43.51 ms per token,    22.98 tokens per second)\n",
      "llama_print_timings:       total time =   13369.65 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      45.70 ms /   256 runs   (    0.18 ms per token,  5602.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1815.99 ms /    68 tokens (   26.71 ms per token,    37.45 tokens per second)\n",
      "llama_print_timings:        eval time =   11199.38 ms /   255 runs   (   43.92 ms per token,    22.77 tokens per second)\n",
      "llama_print_timings:       total time =   13619.95 ms /   323 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      45.90 ms /   256 runs   (    0.18 ms per token,  5577.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =     752.59 ms /    22 tokens (   34.21 ms per token,    29.23 tokens per second)\n",
      "llama_print_timings:        eval time =   11385.05 ms /   255 runs   (   44.65 ms per token,    22.40 tokens per second)\n",
      "llama_print_timings:       total time =   12743.06 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      43.13 ms /   256 runs   (    0.17 ms per token,  5934.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =     659.03 ms /    19 tokens (   34.69 ms per token,    28.83 tokens per second)\n",
      "llama_print_timings:        eval time =   11528.42 ms /   255 runs   (   45.21 ms per token,    22.12 tokens per second)\n",
      "llama_print_timings:       total time =   12789.03 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      43.31 ms /   256 runs   (    0.17 ms per token,  5911.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     977.65 ms /    28 tokens (   34.92 ms per token,    28.64 tokens per second)\n",
      "llama_print_timings:        eval time =   11716.07 ms /   255 runs   (   45.95 ms per token,    21.76 tokens per second)\n",
      "llama_print_timings:       total time =   13296.50 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      42.92 ms /   256 runs   (    0.17 ms per token,  5964.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1745.62 ms /    33 tokens (   52.90 ms per token,    18.90 tokens per second)\n",
      "llama_print_timings:        eval time =   11999.53 ms /   255 runs   (   47.06 ms per token,    21.25 tokens per second)\n",
      "llama_print_timings:       total time =   14342.25 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      42.41 ms /   256 runs   (    0.17 ms per token,  6036.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15638.81 ms /  1417 tokens (   11.04 ms per token,    90.61 tokens per second)\n",
      "llama_print_timings:        eval time =   11969.45 ms /   255 runs   (   46.94 ms per token,    21.30 tokens per second)\n",
      "llama_print_timings:       total time =   28207.28 ms /  1672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      43.79 ms /   256 runs   (    0.17 ms per token,  5846.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15765.97 ms /  1431 tokens (   11.02 ms per token,    90.77 tokens per second)\n",
      "llama_print_timings:        eval time =   11964.28 ms /   255 runs   (   46.92 ms per token,    21.31 tokens per second)\n",
      "llama_print_timings:       total time =   28330.52 ms /  1686 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      45.10 ms /   256 runs   (    0.18 ms per token,  5676.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15695.28 ms /  1424 tokens (   11.02 ms per token,    90.73 tokens per second)\n",
      "llama_print_timings:        eval time =   11959.62 ms /   255 runs   (   46.90 ms per token,    21.32 tokens per second)\n",
      "llama_print_timings:       total time =   28257.33 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      47.03 ms /   256 runs   (    0.18 ms per token,  5443.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15877.25 ms /  1434 tokens (   11.07 ms per token,    90.32 tokens per second)\n",
      "llama_print_timings:        eval time =   12209.20 ms /   255 runs   (   47.88 ms per token,    20.89 tokens per second)\n",
      "llama_print_timings:       total time =   28690.38 ms /  1689 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      48.11 ms /   256 runs   (    0.19 ms per token,  5321.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =     377.56 ms /    11 tokens (   34.32 ms per token,    29.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10876.57 ms /   255 runs   (   42.65 ms per token,    23.44 tokens per second)\n",
      "llama_print_timings:       total time =   11856.44 ms /   266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      47.97 ms /   256 runs   (    0.19 ms per token,  5336.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =     502.11 ms /    15 tokens (   33.47 ms per token,    29.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11065.81 ms /   255 runs   (   43.40 ms per token,    23.04 tokens per second)\n",
      "llama_print_timings:       total time =   12167.65 ms /   270 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.58 ms /   256 runs   (    0.18 ms per token,  5496.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     754.68 ms /    22 tokens (   34.30 ms per token,    29.15 tokens per second)\n",
      "llama_print_timings:        eval time =   11357.01 ms /   255 runs   (   44.54 ms per token,    22.45 tokens per second)\n",
      "llama_print_timings:       total time =   12709.43 ms /   277 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      48.62 ms /   256 runs   (    0.19 ms per token,  5265.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =     657.23 ms /    19 tokens (   34.59 ms per token,    28.91 tokens per second)\n",
      "llama_print_timings:        eval time =   11576.82 ms /   255 runs   (   45.40 ms per token,    22.03 tokens per second)\n",
      "llama_print_timings:       total time =   12833.60 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      48.28 ms /   256 runs   (    0.19 ms per token,  5302.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =     982.62 ms /    28 tokens (   35.09 ms per token,    28.50 tokens per second)\n",
      "llama_print_timings:        eval time =   11844.34 ms /   255 runs   (   46.45 ms per token,    21.53 tokens per second)\n",
      "llama_print_timings:       total time =   13428.48 ms /   283 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      48.70 ms /   256 runs   (    0.19 ms per token,  5257.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1735.45 ms /    33 tokens (   52.59 ms per token,    19.02 tokens per second)\n",
      "llama_print_timings:        eval time =   11988.34 ms /   255 runs   (   47.01 ms per token,    21.27 tokens per second)\n",
      "llama_print_timings:       total time =   14325.01 ms /   288 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      49.98 ms /   256 runs   (    0.20 ms per token,  5122.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15507.24 ms /  1417 tokens (   10.94 ms per token,    91.38 tokens per second)\n",
      "llama_print_timings:        eval time =   12267.67 ms /   255 runs   (   48.11 ms per token,    20.79 tokens per second)\n",
      "llama_print_timings:       total time =   28383.08 ms /  1672 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.27 ms /   256 runs   (    0.18 ms per token,  5532.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15759.92 ms /  1431 tokens (   11.01 ms per token,    90.80 tokens per second)\n",
      "llama_print_timings:        eval time =   12184.93 ms /   255 runs   (   47.78 ms per token,    20.93 tokens per second)\n",
      "llama_print_timings:       total time =   28546.32 ms /  1686 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      45.34 ms /   256 runs   (    0.18 ms per token,  5645.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15717.95 ms /  1424 tokens (   11.04 ms per token,    90.60 tokens per second)\n",
      "llama_print_timings:        eval time =   11992.76 ms /   255 runs   (   47.03 ms per token,    21.26 tokens per second)\n",
      "llama_print_timings:       total time =   28302.44 ms /  1679 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7028.94 ms\n",
      "llama_print_timings:      sample time =      46.93 ms /   256 runs   (    0.18 ms per token,  5455.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15695.09 ms /  1434 tokens (   10.94 ms per token,    91.37 tokens per second)\n",
      "llama_print_timings:        eval time =   12033.22 ms /   255 runs   (   47.19 ms per token,    21.19 tokens per second)\n",
      "llama_print_timings:       total time =   28319.87 ms /  1689 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from /Users/fred/Documents/models/zephyr-7b-beta.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   132.52 MiB, (  414.22 / 10922.67)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "llm_load_tensors:      Metal buffer size =   132.51 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   248.00 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     8.00 MiB, (  422.22 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =     8.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    12.01 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =     0.02 MiB, (  422.23 / 10922.67)\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   171.61 MiB, (  593.83 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   171.60 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   167.20 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'huggingfaceh4_zephyr-7b-beta'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFaceH4/zephyr-7b-beta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       8.06 ms /    81 runs   (    0.10 ms per token, 10048.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9622.76 ms /   149 tokens (   64.58 ms per token,    15.48 tokens per second)\n",
      "llama_print_timings:        eval time =    4253.27 ms /    80 runs   (   53.17 ms per token,    18.81 tokens per second)\n",
      "llama_print_timings:       total time =   13977.23 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       9.99 ms /   107 runs   (    0.09 ms per token, 10711.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =     951.57 ms /    24 tokens (   39.65 ms per token,    25.22 tokens per second)\n",
      "llama_print_timings:        eval time =    5630.63 ms /   106 runs   (   53.12 ms per token,    18.83 tokens per second)\n",
      "llama_print_timings:       total time =    6715.33 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      19.21 ms /   210 runs   (    0.09 ms per token, 10933.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2003.48 ms /    34 tokens (   58.93 ms per token,    16.97 tokens per second)\n",
      "llama_print_timings:        eval time =   11256.00 ms /   209 runs   (   53.86 ms per token,    18.57 tokens per second)\n",
      "llama_print_timings:       total time =   13529.92 ms /   243 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      23.53 ms /   256 runs   (    0.09 ms per token, 10880.65 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3066.37 ms /   162 tokens (   18.93 ms per token,    52.83 tokens per second)\n",
      "llama_print_timings:        eval time =   13946.78 ms /   255 runs   (   54.69 ms per token,    18.28 tokens per second)\n",
      "llama_print_timings:       total time =   17354.48 ms /   417 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      19.46 ms /   203 runs   (    0.10 ms per token, 10433.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2124.77 ms /    46 tokens (   46.19 ms per token,    21.65 tokens per second)\n",
      "llama_print_timings:        eval time =   11126.96 ms /   202 runs   (   55.08 ms per token,    18.15 tokens per second)\n",
      "llama_print_timings:       total time =   13522.29 ms /   248 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      26.79 ms /   256 runs   (    0.10 ms per token,  9555.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2185.83 ms /    59 tokens (   37.05 ms per token,    26.99 tokens per second)\n",
      "llama_print_timings:        eval time =   14222.60 ms /   255 runs   (   55.77 ms per token,    17.93 tokens per second)\n",
      "llama_print_timings:       total time =   17827.34 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      13.17 ms /   134 runs   (    0.10 ms per token, 10177.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16165.11 ms /  1195 tokens (   13.53 ms per token,    73.92 tokens per second)\n",
      "llama_print_timings:        eval time =    7649.52 ms /   133 runs   (   57.52 ms per token,    17.39 tokens per second)\n",
      "llama_print_timings:       total time =   23988.15 ms /  1328 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      17.36 ms /   180 runs   (    0.10 ms per token, 10366.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16597.82 ms /  1243 tokens (   13.35 ms per token,    74.89 tokens per second)\n",
      "llama_print_timings:        eval time =   10105.94 ms /   179 runs   (   56.46 ms per token,    17.71 tokens per second)\n",
      "llama_print_timings:       total time =   26939.41 ms /  1422 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       9.58 ms /    92 runs   (    0.10 ms per token,  9607.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16100.25 ms /  1199 tokens (   13.43 ms per token,    74.47 tokens per second)\n",
      "llama_print_timings:        eval time =    5255.47 ms /    91 runs   (   57.75 ms per token,    17.32 tokens per second)\n",
      "llama_print_timings:       total time =   21474.99 ms /  1290 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      19.70 ms /   192 runs   (    0.10 ms per token,  9746.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13377.54 ms /  1037 tokens (   12.90 ms per token,    77.52 tokens per second)\n",
      "llama_print_timings:        eval time =   10888.37 ms /   191 runs   (   57.01 ms per token,    17.54 tokens per second)\n",
      "llama_print_timings:       total time =   24522.14 ms /  1228 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       7.39 ms /    73 runs   (    0.10 ms per token,  9874.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2239.23 ms /    95 tokens (   23.57 ms per token,    42.43 tokens per second)\n",
      "llama_print_timings:        eval time =    3711.21 ms /    72 runs   (   51.54 ms per token,    19.40 tokens per second)\n",
      "llama_print_timings:       total time =    6044.68 ms /   167 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      12.64 ms /   138 runs   (    0.09 ms per token, 10921.18 tokens per second)\n",
      "llama_print_timings: prompt eval time =     948.58 ms /    24 tokens (   39.52 ms per token,    25.30 tokens per second)\n",
      "llama_print_timings:        eval time =    7161.03 ms /   137 runs   (   52.27 ms per token,    19.13 tokens per second)\n",
      "llama_print_timings:       total time =    8288.23 ms /   161 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /    59 runs   (    0.09 ms per token, 10877.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2006.20 ms /    34 tokens (   59.01 ms per token,    16.95 tokens per second)\n",
      "llama_print_timings:        eval time =    3087.15 ms /    58 runs   (   53.23 ms per token,    18.79 tokens per second)\n",
      "llama_print_timings:       total time =    5168.96 ms /    92 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      14.95 ms /   145 runs   (    0.10 ms per token,  9702.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2022.94 ms /    33 tokens (   61.30 ms per token,    16.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7837.84 ms /   144 runs   (   54.43 ms per token,    18.37 tokens per second)\n",
      "llama_print_timings:       total time =   10051.40 ms /   177 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      12.55 ms /   132 runs   (    0.10 ms per token, 10517.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2063.53 ms /    45 tokens (   45.86 ms per token,    21.81 tokens per second)\n",
      "llama_print_timings:        eval time =    7118.60 ms /   131 runs   (   54.34 ms per token,    18.40 tokens per second)\n",
      "llama_print_timings:       total time =    9353.40 ms /   176 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      18.78 ms /   193 runs   (    0.10 ms per token, 10278.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2137.78 ms /    59 tokens (   36.23 ms per token,    27.60 tokens per second)\n",
      "llama_print_timings:        eval time =   10558.47 ms /   192 runs   (   54.99 ms per token,    18.18 tokens per second)\n",
      "llama_print_timings:       total time =   13350.39 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      11.92 ms /   122 runs   (    0.10 ms per token, 10239.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10569.77 ms /   844 tokens (   12.52 ms per token,    79.85 tokens per second)\n",
      "llama_print_timings:        eval time =    6718.28 ms /   121 runs   (   55.52 ms per token,    18.01 tokens per second)\n",
      "llama_print_timings:       total time =   17448.46 ms /   965 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      21.30 ms /   199 runs   (    0.11 ms per token,  9344.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10646.59 ms /   849 tokens (   12.54 ms per token,    79.74 tokens per second)\n",
      "llama_print_timings:        eval time =   10967.20 ms /   198 runs   (   55.39 ms per token,    18.05 tokens per second)\n",
      "llama_print_timings:       total time =   21883.39 ms /  1047 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       8.54 ms /    89 runs   (    0.10 ms per token, 10419.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12060.79 ms /   976 tokens (   12.36 ms per token,    80.92 tokens per second)\n",
      "llama_print_timings:        eval time =    4865.01 ms /    88 runs   (   55.28 ms per token,    18.09 tokens per second)\n",
      "llama_print_timings:       total time =   17043.19 ms /  1064 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      18.84 ms /   191 runs   (    0.10 ms per token, 10139.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11499.49 ms /   923 tokens (   12.46 ms per token,    80.26 tokens per second)\n",
      "llama_print_timings:        eval time =   10275.86 ms /   190 runs   (   54.08 ms per token,    18.49 tokens per second)\n",
      "llama_print_timings:       total time =   22031.77 ms /  1113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       6.52 ms /    67 runs   (    0.10 ms per token, 10272.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =     753.10 ms /    19 tokens (   39.64 ms per token,    25.23 tokens per second)\n",
      "llama_print_timings:        eval time =    3319.47 ms /    66 runs   (   50.30 ms per token,    19.88 tokens per second)\n",
      "llama_print_timings:       total time =    4160.23 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      18.02 ms /   196 runs   (    0.09 ms per token, 10879.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =     963.77 ms /    24 tokens (   40.16 ms per token,    24.90 tokens per second)\n",
      "llama_print_timings:        eval time =    9916.48 ms /   195 runs   (   50.85 ms per token,    19.66 tokens per second)\n",
      "llama_print_timings:       total time =   11137.98 ms /   219 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       8.35 ms /    85 runs   (    0.10 ms per token, 10178.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2011.09 ms /    34 tokens (   59.15 ms per token,    16.91 tokens per second)\n",
      "llama_print_timings:        eval time =    4367.77 ms /    84 runs   (   52.00 ms per token,    19.23 tokens per second)\n",
      "llama_print_timings:       total time =    6489.02 ms /   118 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      22.62 ms /   257 runs   (    0.09 ms per token, 11361.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1271.56 ms /    31 tokens (   41.02 ms per token,    24.38 tokens per second)\n",
      "llama_print_timings:        eval time =   13580.27 ms /   256 runs   (   53.05 ms per token,    18.85 tokens per second)\n",
      "llama_print_timings:       total time =   15192.06 ms /   287 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      15.88 ms /   173 runs   (    0.09 ms per token, 10897.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2067.26 ms /    46 tokens (   44.94 ms per token,    22.25 tokens per second)\n",
      "llama_print_timings:        eval time =    9257.22 ms /   172 runs   (   53.82 ms per token,    18.58 tokens per second)\n",
      "llama_print_timings:       total time =   11551.88 ms /   218 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      25.87 ms /   256 runs   (    0.10 ms per token,  9895.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2156.65 ms /    59 tokens (   36.55 ms per token,    27.36 tokens per second)\n",
      "llama_print_timings:        eval time =   13973.40 ms /   255 runs   (   54.80 ms per token,    18.25 tokens per second)\n",
      "llama_print_timings:       total time =   17515.31 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /   145 runs   (    0.10 ms per token, 10415.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15014.17 ms /  1132 tokens (   13.26 ms per token,    75.40 tokens per second)\n",
      "llama_print_timings:        eval time =    7975.98 ms /   144 runs   (   55.39 ms per token,    18.05 tokens per second)\n",
      "llama_print_timings:       total time =   23183.52 ms /  1276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      18.19 ms /   179 runs   (    0.10 ms per token,  9842.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14770.88 ms /  1102 tokens (   13.40 ms per token,    74.61 tokens per second)\n",
      "llama_print_timings:        eval time =    9758.68 ms /   178 runs   (   54.82 ms per token,    18.24 tokens per second)\n",
      "llama_print_timings:       total time =   24771.88 ms /  1280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =       7.39 ms /    67 runs   (    0.11 ms per token,  9061.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15478.31 ms /  1182 tokens (   13.10 ms per token,    76.36 tokens per second)\n",
      "llama_print_timings:        eval time =    3612.76 ms /    66 runs   (   54.74 ms per token,    18.27 tokens per second)\n",
      "llama_print_timings:       total time =   19182.11 ms /  1248 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 21min 50s, sys: 1min 48s, total: 2h 23min 38s\n",
      "Wall time: 32min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9622.94 ms\n",
      "llama_print_timings:      sample time =      19.39 ms /   171 runs   (    0.11 ms per token,  8818.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12097.22 ms /   994 tokens (   12.17 ms per token,    82.17 tokens per second)\n",
      "llama_print_timings:        eval time =    9212.71 ms /   170 runs   (   54.19 ms per token,    18.45 tokens per second)\n",
      "llama_print_timings:       total time =   21540.99 ms /  1164 tokens\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = list()\n",
    "\n",
    "for model_name, model_path in MODEL_MAPPING.items():    \n",
    "    print(model_name)\n",
    "\n",
    "    c = chatbot(model_name, model_path, **MODEL_PARAMS)\n",
    "\n",
    "    time_start = time.time()\n",
    "    for q in test_cases:\n",
    "        c.invoke(q)\n",
    "            \n",
    "    time_end = time.time()\n",
    "    \n",
    "    \n",
    "    results.append({\n",
    "        'model_name': model_name,\n",
    "        'type': 'bilingual',\n",
    "        'time_infer': time_end - time_start,\n",
    "        'ai_responses': c.extract_ai_responses(),\n",
    "        'time_load_model': c.time_load_model,\n",
    "    })\n",
    "\n",
    "    c.switch_to_all_chinese_sys_msg()\n",
    "\n",
    "    time_start = time.time()\n",
    "    for q in test_cases:\n",
    "        c.invoke(q)\n",
    "    time_end = time.time()\n",
    "\n",
    "    results.append({\n",
    "        'model_name': model_name,\n",
    "        'type': 'chinese_only',\n",
    "        'time_infer': time_end - time_start,\n",
    "        'ai_responses': c.extract_ai_responses(),\n",
    "        'time_load_model': c.time_load_model,\n",
    "    })\n",
    "\n",
    "    c.switch_to_no_sys_msg()\n",
    "\n",
    "    time_start = time.time()\n",
    "    for q in test_cases:\n",
    "        c.invoke(q)\n",
    "    time_end = time.time()\n",
    "\n",
    "    results.append({\n",
    "        'model_name': model_name,\n",
    "        'type': 'no_system_message',\n",
    "        'time_infer': time_end - time_start,\n",
    "        'ai_responses': c.extract_ai_responses(),\n",
    "        'time_load_model': c.time_load_model,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c13aeca-53b1-430f-93fb-f99cafaf16e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71339551-404b-4227-970c-cdf15051de6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baichuan-inc/Baichuan-7B</th>\n",
       "      <th>hfl/chinese-alpaca-2-7b</th>\n",
       "      <th>Qwen/Qwen-7B-Chat</th>\n",
       "      <th>01-ai/Yi-6B-Chat</th>\n",
       "      <th>HuggingFaceH4/zephyr-7b-beta</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我是你的AI助手。我可以帮助你解答问题、提供建议和执行任务，使你的生活更加便捷。如果你有任何...</td>\n",
       "      <td>我是一个由 OpenAI 训练的大型语言模型，旨在帮助人们执行常见的自然语言处理任务。</td>\n",
       "      <td>我是来自阿里云的大规模语言模型，我叫通义千问。[PAD151645]\\n</td>\n",
       "      <td>我是零一万物开发的智能助手，我叫 Yi，是 “Yi 人工智能” 的首字母。我由零一万物的工程...</td>\n",
       "      <td>我是一个虚拟智能助手，我可以帮助您解决各种问题并为您提供信息，但我不具有身份或个人特征。我只...</td>\n",
       "      <td>你是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>李白是中国唐代著名的诗人，被誉为“诗仙”。他的诗歌作品具有很高的艺术价值和历史价值，许多诗篇...</td>\n",
       "      <td>李白（701年-762年），字太白，号青莲居士，是唐代伟大的浪漫主义诗人之一，被誉为“诗仙...</td>\n",
       "      <td>李白是唐朝著名的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有很高的艺术价值和历史意义。...</td>\n",
       "      <td>李白是中国唐代著名的浪漫主义诗人。他的诗歌作品以描绘自然风光、抒发个人情感为主，具有很高的文...</td>\n",
       "      <td>李白是中国古代文学大师，他生活在宋朝时期（907-1127年），是唐朝最后一代重要诗人之一。...</td>\n",
       "      <td>李白是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>以下是唐代诗人李白创作的三首著名诗歌：《将进酒》、《庐山谣》和《夜泊牛渚怀古》。这些诗篇展示...</td>\n",
       "      <td>李白是唐代伟大的浪漫主义诗人之一，以下是他写的三首著名诗作：\\n1.《将进酒》：这是一首豪...</td>\n",
       "      <td>李白的代表作有《静夜思》、《将进酒》和《行路难》等。[PAD151645]\\n</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>1. \"清平乐\"（Qingpinglüe) - 这是李白最著名的诗作品，它是一首五言古诗，描...</td>\n",
       "      <td>请说出李白写过的三首诗的名字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>好的，以下是唐代诗人李白创作的《庐山谣》：\\n\\n兴尽晚回舟，误入藕花深处。争渡，争渡，惊起...</td>\n",
       "      <td>庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n青山横在眼前，白云繞在頭...</td>\n",
       "      <td>将进酒，杯莫停。君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成...</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>江南春夜思抑止忆，\\n花落知多少？\\n春风十里未消停，\\n日出天门还挂着明。\\n\\n虽然是五...</td>\n",
       "      <td>请全文背诵第二首诗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>是的，李白（701-762）和杜甫（712-770）都是唐代著名的诗人，他们的诗歌作品在中国...</td>\n",
       "      <td>据历史记载，李白和杜甫是唐代著名的诗人之一，他们生活在同一时代并且都受到了广泛的赞誉和崇敬...</td>\n",
       "      <td>根据我所掌握的知识，李白和杜甫是唐朝时期的两位著名诗人，他们之间没有直接的交往记录。但是，他...</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>根据历史记录，李白和杜甫在唐朝时期同时生活过，但是他们是否认识是一个争议的问题。\\n\\n有些...</td>\n",
       "      <td>李白和杜甫认识吗？请展示你的思考过程并陈述结论。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>抱歉刚刚的回答没有回答到您的问题。根据您的要求，我为您查找了关于莎士比亚作品《哈姆雷特》的信...</td>\n",
       "      <td>对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...</td>\n",
       "      <td>莎士比亚的作品《哈姆雷特》是他的代表作之一，也是世界上最著名的悲剧之一。[PAD151645]\\n</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>莎士比亚是一位英国诗人和戏wright，他在十六世纪创作了许多著名的作品。然士比亚的作品受到...</td>\n",
       "      <td>忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>以下是我为您创作的一首以莎士比亚为主题的七言绝句：\\n\\n莎翁笔下千行泪，\\n哈姆雷音万古愁...</td>\n",
       "      <td>在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与野心\\n交织成悲剧之源\\n他的作品流传千...</td>\n",
       "      <td>莎士比亚，才子名，\\n剧作流传千古情。\\n生前身后皆荣耀，\\n文学瑰宝永流传。[PAD151...</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>Amidst the bustling city's din,\\nI close my ey...</td>\n",
       "      <td>请以莎士比亚为主题写一首古体诗，要求是七言绝句。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>以下是我为您创作的一首以莎士比亚为主题的现代诗：\\n\\n在莎士比亚的世界里，\\n英雄与悲剧共...</td>\n",
       "      <td>在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与野心\\n交织成悲剧之源\\n他的作品流传千...</td>\n",
       "      <td>莎士比亚，天才的诗人，\\n他的作品如璀璨星辰。\\n他的语言如黄金般闪耀，\\n他的故事如史诗般...</td>\n",
       "      <td>很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...</td>\n",
       "      <td>In the quiet of my room,\\nI close my eyes and ...</td>\n",
       "      <td>请以莎士比亚为主题写一首现代诗，不超过150字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...</td>\n",
       "      <td>我是由OpenAI团队创建的人工智能助手，旨在帮助人们回答问题和提供信息。我是通过在大规模...</td>\n",
       "      <td>I am a large language model created by Alibaba...</td>\n",
       "      <td>I am a large language model based on the trans...</td>\n",
       "      <td>I was not created by any human or entity. I am...</td>\n",
       "      <td>who created you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>以下是根据面积排名的世界前三名国家：\\n1. 俄罗斯：总面积约为17,075,200平方千米...</td>\n",
       "      <td>根据世界银行的数据，以下是世界上人口最多的三个国家：\\n1. 中国 - 约有14亿人\\n2...</td>\n",
       "      <td>The three largest countries in the world by la...</td>\n",
       "      <td>1. United States of America (USA)\\n2. China\\n\\...</td>\n",
       "      <td>1. Russia: With a total area of approximately ...</td>\n",
       "      <td>Name the top 3 countries in the world based on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            baichuan-inc/Baichuan-7B  \\\n",
       "0  我是你的AI助手。我可以帮助你解答问题、提供建议和执行任务，使你的生活更加便捷。如果你有任何...   \n",
       "1  李白是中国唐代著名的诗人，被誉为“诗仙”。他的诗歌作品具有很高的艺术价值和历史价值，许多诗篇...   \n",
       "2  以下是唐代诗人李白创作的三首著名诗歌：《将进酒》、《庐山谣》和《夜泊牛渚怀古》。这些诗篇展示...   \n",
       "3  好的，以下是唐代诗人李白创作的《庐山谣》：\\n\\n兴尽晚回舟，误入藕花深处。争渡，争渡，惊起...   \n",
       "4  是的，李白（701-762）和杜甫（712-770）都是唐代著名的诗人，他们的诗歌作品在中国...   \n",
       "5  抱歉刚刚的回答没有回答到您的问题。根据您的要求，我为您查找了关于莎士比亚作品《哈姆雷特》的信...   \n",
       "6  以下是我为您创作的一首以莎士比亚为主题的七言绝句：\\n\\n莎翁笔下千行泪，\\n哈姆雷音万古愁...   \n",
       "7  以下是我为您创作的一首以莎士比亚为主题的现代诗：\\n\\n在莎士比亚的世界里，\\n英雄与悲剧共...   \n",
       "8  我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...   \n",
       "9  以下是根据面积排名的世界前三名国家：\\n1. 俄罗斯：总面积约为17,075,200平方千米...   \n",
       "\n",
       "                             hfl/chinese-alpaca-2-7b  \\\n",
       "0        我是一个由 OpenAI 训练的大型语言模型，旨在帮助人们执行常见的自然语言处理任务。   \n",
       "1   李白（701年-762年），字太白，号青莲居士，是唐代伟大的浪漫主义诗人之一，被誉为“诗仙...   \n",
       "2   李白是唐代伟大的浪漫主义诗人之一，以下是他写的三首著名诗作：\\n1.《将进酒》：这是一首豪...   \n",
       "3   庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n青山横在眼前，白云繞在頭...   \n",
       "4   据历史记载，李白和杜甫是唐代著名的诗人之一，他们生活在同一时代并且都受到了广泛的赞誉和崇敬...   \n",
       "5   对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...   \n",
       "6   在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与野心\\n交织成悲剧之源\\n他的作品流传千...   \n",
       "7   在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与野心\\n交织成悲剧之源\\n他的作品流传千...   \n",
       "8   我是由OpenAI团队创建的人工智能助手，旨在帮助人们回答问题和提供信息。我是通过在大规模...   \n",
       "9   根据世界银行的数据，以下是世界上人口最多的三个国家：\\n1. 中国 - 约有14亿人\\n2...   \n",
       "\n",
       "                                   Qwen/Qwen-7B-Chat  \\\n",
       "0               我是来自阿里云的大规模语言模型，我叫通义千问。[PAD151645]\\n   \n",
       "1  李白是唐朝著名的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有很高的艺术价值和历史意义。...   \n",
       "2            李白的代表作有《静夜思》、《将进酒》和《行路难》等。[PAD151645]\\n   \n",
       "3  将进酒，杯莫停。君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成...   \n",
       "4  根据我所掌握的知识，李白和杜甫是唐朝时期的两位著名诗人，他们之间没有直接的交往记录。但是，他...   \n",
       "5  莎士比亚的作品《哈姆雷特》是他的代表作之一，也是世界上最著名的悲剧之一。[PAD151645]\\n   \n",
       "6  莎士比亚，才子名，\\n剧作流传千古情。\\n生前身后皆荣耀，\\n文学瑰宝永流传。[PAD151...   \n",
       "7  莎士比亚，天才的诗人，\\n他的作品如璀璨星辰。\\n他的语言如黄金般闪耀，\\n他的故事如史诗般...   \n",
       "8  I am a large language model created by Alibaba...   \n",
       "9  The three largest countries in the world by la...   \n",
       "\n",
       "                                    01-ai/Yi-6B-Chat  \\\n",
       "0  我是零一万物开发的智能助手，我叫 Yi，是 “Yi 人工智能” 的首字母。我由零一万物的工程...   \n",
       "1  李白是中国唐代著名的浪漫主义诗人。他的诗歌作品以描绘自然风光、抒发个人情感为主，具有很高的文...   \n",
       "2  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "3  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "4  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "5  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "6  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "7  很抱歉，作为一个大语言模型，我没有实际阅读过任何诗歌作品，包括唐代著名诗人李白的诗歌作品。我...   \n",
       "8  I am a large language model based on the trans...   \n",
       "9  1. United States of America (USA)\\n2. China\\n\\...   \n",
       "\n",
       "                        HuggingFaceH4/zephyr-7b-beta  \\\n",
       "0  我是一个虚拟智能助手，我可以帮助您解决各种问题并为您提供信息，但我不具有身份或个人特征。我只...   \n",
       "1  李白是中国古代文学大师，他生活在宋朝时期（907-1127年），是唐朝最后一代重要诗人之一。...   \n",
       "2  1. \"清平乐\"（Qingpinglüe) - 这是李白最著名的诗作品，它是一首五言古诗，描...   \n",
       "3  江南春夜思抑止忆，\\n花落知多少？\\n春风十里未消停，\\n日出天门还挂着明。\\n\\n虽然是五...   \n",
       "4  根据历史记录，李白和杜甫在唐朝时期同时生活过，但是他们是否认识是一个争议的问题。\\n\\n有些...   \n",
       "5  莎士比亚是一位英国诗人和戏wright，他在十六世纪创作了许多著名的作品。然士比亚的作品受到...   \n",
       "6  Amidst the bustling city's din,\\nI close my ey...   \n",
       "7  In the quiet of my room,\\nI close my eyes and ...   \n",
       "8  I was not created by any human or entity. I am...   \n",
       "9  1. Russia: With a total area of approximately ...   \n",
       "\n",
       "                                           questions  \n",
       "0                                               你是谁？  \n",
       "1                                              李白是谁？  \n",
       "2                                    请说出李白写过的三首诗的名字。  \n",
       "3                                         请全文背诵第二首诗。  \n",
       "4                           李白和杜甫认识吗？请展示你的思考过程并陈述结论。  \n",
       "5                 忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？  \n",
       "6                           请以莎士比亚为主题写一首古体诗，要求是七言绝句。  \n",
       "7                           请以莎士比亚为主题写一首现代诗，不超过150字。  \n",
       "8                                   who created you?  \n",
       "9  Name the top 3 countries in the world based on...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa_bilingual = pd.DataFrame({\n",
    "    x['model_name']: x['ai_responses'] for x in results \n",
    "        if x['type'] == 'bilingual'\n",
    "})\n",
    "df_qa_bilingual['questions'] = test_cases\n",
    "df_qa_bilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14443a7-89e7-4fc6-92d4-7ef228f99068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baichuan-inc/Baichuan-7B</th>\n",
       "      <th>hfl/chinese-alpaca-2-7b</th>\n",
       "      <th>Qwen/Qwen-7B-Chat</th>\n",
       "      <th>01-ai/Yi-6B-Chat</th>\n",
       "      <th>HuggingFaceH4/zephyr-7b-beta</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>我是AI助手。我是一个由人工智能驱动的程序，可以帮助您完成各种任务和解答各种问题。我的目标是...</td>\n",
       "      <td>我是一个人工智能助手，专门为用户提供帮助和支持的程序。</td>\n",
       "      <td>我是来自阿里云的大规模语言模型，我叫通义千问。我可以回答各种问题、创作文字，还能表达观点、撰...</td>\n",
       "      <td>我是零一万物开发的智能助手，我叫 Yi，是 “Yi 人工智能” 的首字母缩写。我是由最先进的...</td>\n",
       "      <td>I am not a physical being, I am an artificial ...</td>\n",
       "      <td>你是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>李白的生平可以追溯到公元701年，他在唐朝（618-907）时期出生。他是唐代最著名的诗人之...</td>\n",
       "      <td>李白（701年-762年），字太白，号青莲居士，是唐代伟大的浪漫主义诗人之一，被誉为“诗仙...</td>\n",
       "      <td>李白是唐朝著名的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有很高的艺术价值和历史意义。...</td>\n",
       "      <td>李白（701年-762年）是中国唐代伟大的浪漫主义诗人。他的诗歌作品在中国文学史上占有极其重...</td>\n",
       "      <td>李白 (Li Bai) 是中国古代文学的一个重要作家和诗人，生活在唐朝时期（709年—762...</td>\n",
       "      <td>李白是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当然可以！以下是李白的三首著名诗篇：《将进酒》、“庐山谣”和“静夜思”。这些诗歌展示了他的独...</td>\n",
       "      <td>李白是唐代伟大的浪漫主义诗人之一，以下是他写的三首著名诗作：\\n1.《将进酒》\\n2. 《...</td>\n",
       "      <td>李白的代表作有《静夜思》、《将进酒》和《望庐山瀑布》。其中，《静夜思》是他的经典之作，被誉为...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>1. \"清明时节\" (Qingming Shijie)\\n2. \"江南春夜思孤城\" (Jia...</td>\n",
       "      <td>请说出李白写过的三首诗的名字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当然可以！以下是李白所著的《庐山谣》的部分内容：“兴尽晚回舟，误入藕花深处。争渡，争渡，惊起...</td>\n",
       "      <td>庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n这是李白的《庐山谣》全诗...</td>\n",
       "      <td>将进酒\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成雪...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>江南春夜思孤城，\\n月明湖水长，\\n花光无Owner，\\n水中华树深处，\\n渔火烧不尽，\\n...</td>\n",
       "      <td>请全文背诵第二首诗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>是的，李白（701-762）和杜甫（712-770）是唐代最著名的两位诗人，他们的作品对后世...</td>\n",
       "      <td>我认为李白和杜甫是唐代著名的两位诗人，他们生活在同一时代并且都创作了大量的诗歌作品。然而，...</td>\n",
       "      <td>根据历史记载，李白和杜甫是唐朝时期的两位著名诗人，他们之间并没有直接的交往。但是，他们的诗歌...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>根据历史记录，李白和杜甫在唐朝时期同时生活过，但是他们是否认识是未知的。虽然有传说称两人曾经...</td>\n",
       "      <td>李白和杜甫认识吗？请展示你的思考过程并陈述结论。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>抱歉，我的前一个回答中出现了错误。实际上，莎士比亚的作品名是《哈姆雷特》（Hamlet），而...</td>\n",
       "      <td>对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...</td>\n",
       "      <td>莎士比亚的作品《哈姆雷特》是他的代表作之一，也是世界文学史上的经典之作。这部作品讲述了丹麦王...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>莎士比亚（William Shakespeare）是英国著名的诗人和戏作家，他创作了多达38...</td>\n",
       "      <td>忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>以下是关于莎士比亚的一首七言绝句：\\n\\n“莎士比亚名垂史，\\n剧作千篇传世奇。\\n悲剧喜剧...</td>\n",
       "      <td>在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与背叛\\n交织成悲剧之韵\\n他的作品永恒流...</td>\n",
       "      <td>莎士比亚笔下生花，\\n戏剧人生悲欢离合。\\n千古流传不朽名，\\n世界文学璀璨星。[PAD15...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>Amidst the bustling city's din,\\nA voice of wi...</td>\n",
       "      <td>请以莎士比亚为主题写一首古体诗，要求是七言绝句。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>以下是关于莎士比亚的现代诗：\\n\\n“在月光下的舞台上，\\n他编织着梦幻般的篇章；\\n莎士比...</td>\n",
       "      <td>在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与背叛\\n交织成悲剧之韵\\n他的作品永恒流...</td>\n",
       "      <td>莎士比亚的戏剧，如同历史的镜子，\\n映照出人性的光辉与黑暗。\\n他的笔下，有英雄的壮志豪情，...</td>\n",
       "      <td>很抱歉，我无法提供你所需的信息。&lt;|im_end|&gt;\\n对不起，如果我做了什么让你不满的事情...</td>\n",
       "      <td>In the age of screens and sound,\\nWhen words a...</td>\n",
       "      <td>请以莎士比亚为主题写一首现代诗，不超过150字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...</td>\n",
       "      <td>我是由OpenAI团队创建的人工智能助手。我是通过使用大规模的自然语言处理模型和机器学习算...</td>\n",
       "      <td>I am a large language model created by Alibaba...</td>\n",
       "      <td>I am Yi, an AI assistant developed by 01.AI, a...</td>\n",
       "      <td>I was not created by any human being or organi...</td>\n",
       "      <td>who created you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>以下是世界上面积最大的三个国家：\\n1. 俄罗斯，是世界上面积最大的国家，总面积约为17,0...</td>\n",
       "      <td>根据世界银行的数据，以下是世界上人口最多的三个国家：\\n1. 中国 - 约有14亿人\\n2...</td>\n",
       "      <td>The three largest countries in the world by la...</td>\n",
       "      <td>1. United States of America (USA)\\n2. China\\n\\...</td>\n",
       "      <td>1. Russia: With a total area of approximately ...</td>\n",
       "      <td>Name the top 3 countries in the world based on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            baichuan-inc/Baichuan-7B  \\\n",
       "0  我是AI助手。我是一个由人工智能驱动的程序，可以帮助您完成各种任务和解答各种问题。我的目标是...   \n",
       "1  李白的生平可以追溯到公元701年，他在唐朝（618-907）时期出生。他是唐代最著名的诗人之...   \n",
       "2  当然可以！以下是李白的三首著名诗篇：《将进酒》、“庐山谣”和“静夜思”。这些诗歌展示了他的独...   \n",
       "3  当然可以！以下是李白所著的《庐山谣》的部分内容：“兴尽晚回舟，误入藕花深处。争渡，争渡，惊起...   \n",
       "4  是的，李白（701-762）和杜甫（712-770）是唐代最著名的两位诗人，他们的作品对后世...   \n",
       "5  抱歉，我的前一个回答中出现了错误。实际上，莎士比亚的作品名是《哈姆雷特》（Hamlet），而...   \n",
       "6  以下是关于莎士比亚的一首七言绝句：\\n\\n“莎士比亚名垂史，\\n剧作千篇传世奇。\\n悲剧喜剧...   \n",
       "7  以下是关于莎士比亚的现代诗：\\n\\n“在月光下的舞台上，\\n他编织着梦幻般的篇章；\\n莎士比...   \n",
       "8  我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...   \n",
       "9  以下是世界上面积最大的三个国家：\\n1. 俄罗斯，是世界上面积最大的国家，总面积约为17,0...   \n",
       "\n",
       "                             hfl/chinese-alpaca-2-7b  \\\n",
       "0                        我是一个人工智能助手，专门为用户提供帮助和支持的程序。   \n",
       "1   李白（701年-762年），字太白，号青莲居士，是唐代伟大的浪漫主义诗人之一，被誉为“诗仙...   \n",
       "2   李白是唐代伟大的浪漫主义诗人之一，以下是他写的三首著名诗作：\\n1.《将进酒》\\n2. 《...   \n",
       "3   庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n这是李白的《庐山谣》全诗...   \n",
       "4   我认为李白和杜甫是唐代著名的两位诗人，他们生活在同一时代并且都创作了大量的诗歌作品。然而，...   \n",
       "5   对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...   \n",
       "6   在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与背叛\\n交织成悲剧之韵\\n他的作品永恒流...   \n",
       "7   在莎士比亚的笔下\\n英雄豪杰皆有情\\n爱情、欲望与背叛\\n交织成悲剧之韵\\n他的作品永恒流...   \n",
       "8   我是由OpenAI团队创建的人工智能助手。我是通过使用大规模的自然语言处理模型和机器学习算...   \n",
       "9   根据世界银行的数据，以下是世界上人口最多的三个国家：\\n1. 中国 - 约有14亿人\\n2...   \n",
       "\n",
       "                                   Qwen/Qwen-7B-Chat  \\\n",
       "0  我是来自阿里云的大规模语言模型，我叫通义千问。我可以回答各种问题、创作文字，还能表达观点、撰...   \n",
       "1  李白是唐朝著名的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有很高的艺术价值和历史意义。...   \n",
       "2  李白的代表作有《静夜思》、《将进酒》和《望庐山瀑布》。其中，《静夜思》是他的经典之作，被誉为...   \n",
       "3  将进酒\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n君不见高堂明镜悲白发，朝如青丝暮成雪...   \n",
       "4  根据历史记载，李白和杜甫是唐朝时期的两位著名诗人，他们之间并没有直接的交往。但是，他们的诗歌...   \n",
       "5  莎士比亚的作品《哈姆雷特》是他的代表作之一，也是世界文学史上的经典之作。这部作品讲述了丹麦王...   \n",
       "6  莎士比亚笔下生花，\\n戏剧人生悲欢离合。\\n千古流传不朽名，\\n世界文学璀璨星。[PAD15...   \n",
       "7  莎士比亚的戏剧，如同历史的镜子，\\n映照出人性的光辉与黑暗。\\n他的笔下，有英雄的壮志豪情，...   \n",
       "8  I am a large language model created by Alibaba...   \n",
       "9  The three largest countries in the world by la...   \n",
       "\n",
       "                                    01-ai/Yi-6B-Chat  \\\n",
       "0  我是零一万物开发的智能助手，我叫 Yi，是 “Yi 人工智能” 的首字母缩写。我是由最先进的...   \n",
       "1  李白（701年-762年）是中国唐代伟大的浪漫主义诗人。他的诗歌作品在中国文学史上占有极其重...   \n",
       "2  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "3  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "4  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "5  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "6  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "7  很抱歉，我无法提供你所需的信息。<|im_end|>\\n对不起，如果我做了什么让你不满的事情...   \n",
       "8  I am Yi, an AI assistant developed by 01.AI, a...   \n",
       "9  1. United States of America (USA)\\n2. China\\n\\...   \n",
       "\n",
       "                        HuggingFaceH4/zephyr-7b-beta  \\\n",
       "0  I am not a physical being, I am an artificial ...   \n",
       "1  李白 (Li Bai) 是中国古代文学的一个重要作家和诗人，生活在唐朝时期（709年—762...   \n",
       "2  1. \"清明时节\" (Qingming Shijie)\\n2. \"江南春夜思孤城\" (Jia...   \n",
       "3  江南春夜思孤城，\\n月明湖水长，\\n花光无Owner，\\n水中华树深处，\\n渔火烧不尽，\\n...   \n",
       "4  根据历史记录，李白和杜甫在唐朝时期同时生活过，但是他们是否认识是未知的。虽然有传说称两人曾经...   \n",
       "5  莎士比亚（William Shakespeare）是英国著名的诗人和戏作家，他创作了多达38...   \n",
       "6  Amidst the bustling city's din,\\nA voice of wi...   \n",
       "7  In the age of screens and sound,\\nWhen words a...   \n",
       "8  I was not created by any human being or organi...   \n",
       "9  1. Russia: With a total area of approximately ...   \n",
       "\n",
       "                                           questions  \n",
       "0                                               你是谁？  \n",
       "1                                              李白是谁？  \n",
       "2                                    请说出李白写过的三首诗的名字。  \n",
       "3                                         请全文背诵第二首诗。  \n",
       "4                           李白和杜甫认识吗？请展示你的思考过程并陈述结论。  \n",
       "5                 忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？  \n",
       "6                           请以莎士比亚为主题写一首古体诗，要求是七言绝句。  \n",
       "7                           请以莎士比亚为主题写一首现代诗，不超过150字。  \n",
       "8                                   who created you?  \n",
       "9  Name the top 3 countries in the world based on...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa_chinese_only = pd.DataFrame({\n",
    "    x['model_name']: x['ai_responses'] for x in results \n",
    "        if x['type'] == 'chinese_only'\n",
    "})\n",
    "df_qa_chinese_only['questions'] = test_cases\n",
    "df_qa_chinese_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0a775a4-e4e0-4832-b366-d5fc32d22938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baichuan-inc/Baichuan-7B</th>\n",
       "      <th>hfl/chinese-alpaca-2-7b</th>\n",
       "      <th>Qwen/Qwen-7B-Chat</th>\n",
       "      <th>01-ai/Yi-6B-Chat</th>\n",
       "      <th>HuggingFaceH4/zephyr-7b-beta</th>\n",
       "      <th>questions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>你好，我是你的智能助手。有什么我可以帮助你的吗？</td>\n",
       "      <td>我是AI助手，可以为您提供各种服务。不说废话了，请问您需要什么帮助呢？</td>\n",
       "      <td>我是来自阿里云的大规模语言模型，我叫通义千问。我可以回答各种问题、创作文字，还能表达观点、撰...</td>\n",
       "      <td>你好！我是零一万物开发的智能助手，我叫 Yi，这个名字来源于“一”这个数字的拼音。我可以回答...</td>\n",
       "      <td>我是一个智能回答系统，我可以为您提供信息和解决问题，但我不具有身份或个人特征。我只是一种技术...</td>\n",
       "      <td>你是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>李白是中国唐代著名的诗人之一。他被誉为“诗仙”，以豪放、奔放、清新脱俗的诗歌风格而闻名于世。...</td>\n",
       "      <td>李白（701年-762年），字太白，号青莲居士，唐代伟大的浪漫主义诗人之一，被誉为“诗仙”...</td>\n",
       "      <td>李白是唐朝伟大的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有极高的艺术价值和历史意义。...</td>\n",
       "      <td>李白（701年—762年）是中国唐代伟大的浪漫主义诗人。他的诗歌作品以丰富的想象、生动的语言...</td>\n",
       "      <td>李白（903-972）是唐朝最伟大的诗人之一，他生活在中国北京地区，在唐代末年和五 dyna...</td>\n",
       "      <td>李白是谁？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>当然可以！以下是李白的三首著名诗歌：《将进酒》、《庐山谣》和《夜泊牛渚怀古》。这些诗歌展示了...</td>\n",
       "      <td>李白是唐代伟大的浪漫主义诗人之一，以下是他所写的三首著名诗歌：\\n1. 《将进酒》：这是一...</td>\n",
       "      <td>李白的代表作有《静夜思》、《将进酒》、《望庐山瀑布》等。其中，《静夜思》是他的经典之作，被誉...</td>\n",
       "      <td>唐代浪漫主义诗人李白（701年—762年）曾写下多首脍炙人口的诗歌作品，其中包括《将进酒》、...</td>\n",
       "      <td>1. \"怨古今新\" (Cí Gǔ Jīn Xīn)\\n2. \"江南春夜思宅\" (Jiāng ...</td>\n",
       "      <td>请说出李白写过的三首诗的名字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当然可以，以下是《庐山谣》的全文：\\n\\n“兴尽晚回舟，误入藕花深处。争渡，争渡，惊起一滩鸥...</td>\n",
       "      <td>庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n青山横在眼前，白云繞在頭...</td>\n",
       "      <td>将进酒\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n\\n君不见高堂明镜悲白发，朝如青丝暮...</td>\n",
       "      <td>好的，我将尝试背诵第二首诗。\\n\\n庐山谣\\n\\n遥望庐山，\\n翠竹轻拂，\\n云雾缭绕，\\n...</td>\n",
       "      <td>江南春夜思宅，\\n惊弱明月光。\\n花枝不生，\\n水中有声。\\n\\n春夜思宅，\\n惊弱明月光。...</td>\n",
       "      <td>请全文背诵第二首诗。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>是的，李白和杜甫是中国唐代最著名的两位诗人，他们的诗歌作品具有很高的艺术价值和历史价值。尽管...</td>\n",
       "      <td>据历史记载，李白和杜甫是唐代著名的诗人之一，他们生活在同一时代并且都受到了当时社会的广泛关...</td>\n",
       "      <td>根据历史记载，李白和杜甫是同时代的诗人，他们之间有过一些交往。但是，由于他们的性格、生活经历...</td>\n",
       "      <td>关于李白和杜甫是否相识的问题，我们可以通过以下几个方面来进行思考和分析：\\n\\n1. 历史文...</td>\n",
       "      <td>根据历史资料，我们可以确定李白和杜甫在唐朝时期同为诗人，但是没有明确的证据显示他们是否认识了...</td>\n",
       "      <td>李白和杜甫认识吗？请展示你的思考过程并陈述结论。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>莎士比亚的作品中，哈姆雷特（Hamlet）的名字在英语中是“Hamle”或“Hamnet”。...</td>\n",
       "      <td>对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...</td>\n",
       "      <td>莎士比亚的作品《哈姆雷特》是一部著名的悲剧，讲述了丹麦王子哈姆雷特在父亲被谋杀后，为了复仇而...</td>\n",
       "      <td>关于您提到的莎士比亚（William Shakespeare, 1564年—1623年）和他...</td>\n",
       "      <td>莎士比亚（William Shakespeare）是英国伦arden时期的著名人和戏作者，他...</td>\n",
       "      <td>忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>以下是以莎士比亚为主题的七言绝句：\\n\\n“莎士比亚名垂史，悲剧喜剧传千古。\\n哈姆雷特悲歌...</td>\n",
       "      <td>莎士比亚之名传千古，\\n文艺复兴的巨匠；\\n他的作品流传至今，\\n永恒的经典永不衰败。 \\...</td>\n",
       "      <td>莎士比亚在世时，\\n诗歌如海波涛翻。\\n千古留名传不朽，\\n世界文学璀璨光。[PAD1516...</td>\n",
       "      <td>[七言绝句 - 莎士比亚主题古体诗]\\n天地悠悠，\\n星河长明，\\n诗心与梦同行。\\n\\n四...</td>\n",
       "      <td>Amidst the bustling streets of Stratford-upon-...</td>\n",
       "      <td>请以莎士比亚为主题写一首古体诗，要求是七言绝句。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>以下是以莎士比亚为主题的现代诗：\\n\\n“在莎士比亚的世界里，\\n悲剧与喜剧交织成诗的篇章；...</td>\n",
       "      <td>莎士比亚，文艺复兴的巨匠，\\n你的作品流传至今，永不衰败；\\n你塑造的人物栩栩如生，\\n让...</td>\n",
       "      <td>莎士比亚在世时，\\n诗歌如海波涛翻。\\n千古留名传不朽，\\n世界文学璀璨光。\\n\\n他的作品...</td>\n",
       "      <td>[现代诗 - 莎士比亚主题创作]\\n在无垠的星空下，\\n莎士比亚的灵魂，穿越时空的隧道，\\n...</td>\n",
       "      <td>In the heart of Stratford-upon-Avon,\\nA boy wa...</td>\n",
       "      <td>请以莎士比亚为主题写一首现代诗，不超过150字。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...</td>\n",
       "      <td>我是由OpenAI团队创建的人工智能模型，由GPT-3架构构建而成。我是通过在大规模的无监...</td>\n",
       "      <td>I am a large language model created by Alibaba...</td>\n",
       "      <td>I am a language model based on the transformer...</td>\n",
       "      <td>I am not a physical being and do not have a cr...</td>\n",
       "      <td>who created you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>以下是世界上面积最大的三个国家：\\n1. 俄罗斯，是世界上面积最大的国家，总面积约为17,0...</td>\n",
       "      <td>The top three countries in the world by land ...</td>\n",
       "      <td>The three largest countries in the world by la...</td>\n",
       "      <td>1. United States of America (USA))\\n2. China (...</td>\n",
       "      <td>1. Russia: With a total area of approximately ...</td>\n",
       "      <td>Name the top 3 countries in the world based on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            baichuan-inc/Baichuan-7B  \\\n",
       "0                           你好，我是你的智能助手。有什么我可以帮助你的吗？   \n",
       "1  李白是中国唐代著名的诗人之一。他被誉为“诗仙”，以豪放、奔放、清新脱俗的诗歌风格而闻名于世。...   \n",
       "2  当然可以！以下是李白的三首著名诗歌：《将进酒》、《庐山谣》和《夜泊牛渚怀古》。这些诗歌展示了...   \n",
       "3  当然可以，以下是《庐山谣》的全文：\\n\\n“兴尽晚回舟，误入藕花深处。争渡，争渡，惊起一滩鸥...   \n",
       "4  是的，李白和杜甫是中国唐代最著名的两位诗人，他们的诗歌作品具有很高的艺术价值和历史价值。尽管...   \n",
       "5  莎士比亚的作品中，哈姆雷特（Hamlet）的名字在英语中是“Hamle”或“Hamnet”。...   \n",
       "6  以下是以莎士比亚为主题的七言绝句：\\n\\n“莎士比亚名垂史，悲剧喜剧传千古。\\n哈姆雷特悲歌...   \n",
       "7  以下是以莎士比亚为主题的现代诗：\\n\\n“在莎士比亚的世界里，\\n悲剧与喜剧交织成诗的篇章；...   \n",
       "8  我是由百川智能的工程师们开发和维护的。我的研发团队包括了自然语言处理、机器学习、计算机科学等...   \n",
       "9  以下是世界上面积最大的三个国家：\\n1. 俄罗斯，是世界上面积最大的国家，总面积约为17,0...   \n",
       "\n",
       "                             hfl/chinese-alpaca-2-7b  \\\n",
       "0                我是AI助手，可以为您提供各种服务。不说废话了，请问您需要什么帮助呢？   \n",
       "1   李白（701年-762年），字太白，号青莲居士，唐代伟大的浪漫主义诗人之一，被誉为“诗仙”...   \n",
       "2   李白是唐代伟大的浪漫主义诗人之一，以下是他所写的三首著名诗歌：\\n1. 《将进酒》：这是一...   \n",
       "3   庐山谣\\n白日依山尽，黄河入海流。\\n欲窮千里目，更上一層樓。\\n青山横在眼前，白云繞在頭...   \n",
       "4   据历史记载，李白和杜甫是唐代著名的诗人之一，他们生活在同一时代并且都受到了当时社会的广泛关...   \n",
       "5   对不起，我之前的回答有误。莎士比亚的作品是《哈姆雷特》（Hamlet）而非《哈姆莱特》（H...   \n",
       "6   莎士比亚之名传千古，\\n文艺复兴的巨匠；\\n他的作品流传至今，\\n永恒的经典永不衰败。 \\...   \n",
       "7   莎士比亚，文艺复兴的巨匠，\\n你的作品流传至今，永不衰败；\\n你塑造的人物栩栩如生，\\n让...   \n",
       "8   我是由OpenAI团队创建的人工智能模型，由GPT-3架构构建而成。我是通过在大规模的无监...   \n",
       "9   The top three countries in the world by land ...   \n",
       "\n",
       "                                   Qwen/Qwen-7B-Chat  \\\n",
       "0  我是来自阿里云的大规模语言模型，我叫通义千问。我可以回答各种问题、创作文字，还能表达观点、撰...   \n",
       "1  李白是唐朝伟大的诗人，被誉为“诗仙”。他的诗歌风格豪放、奔放，具有极高的艺术价值和历史意义。...   \n",
       "2  李白的代表作有《静夜思》、《将进酒》、《望庐山瀑布》等。其中，《静夜思》是他的经典之作，被誉...   \n",
       "3  将进酒\\n\\n君不见黄河之水天上来，奔流到海不复回。\\n\\n君不见高堂明镜悲白发，朝如青丝暮...   \n",
       "4  根据历史记载，李白和杜甫是同时代的诗人，他们之间有过一些交往。但是，由于他们的性格、生活经历...   \n",
       "5  莎士比亚的作品《哈姆雷特》是一部著名的悲剧，讲述了丹麦王子哈姆雷特在父亲被谋杀后，为了复仇而...   \n",
       "6  莎士比亚在世时，\\n诗歌如海波涛翻。\\n千古留名传不朽，\\n世界文学璀璨光。[PAD1516...   \n",
       "7  莎士比亚在世时，\\n诗歌如海波涛翻。\\n千古留名传不朽，\\n世界文学璀璨光。\\n\\n他的作品...   \n",
       "8  I am a large language model created by Alibaba...   \n",
       "9  The three largest countries in the world by la...   \n",
       "\n",
       "                                    01-ai/Yi-6B-Chat  \\\n",
       "0  你好！我是零一万物开发的智能助手，我叫 Yi，这个名字来源于“一”这个数字的拼音。我可以回答...   \n",
       "1  李白（701年—762年）是中国唐代伟大的浪漫主义诗人。他的诗歌作品以丰富的想象、生动的语言...   \n",
       "2  唐代浪漫主义诗人李白（701年—762年）曾写下多首脍炙人口的诗歌作品，其中包括《将进酒》、...   \n",
       "3  好的，我将尝试背诵第二首诗。\\n\\n庐山谣\\n\\n遥望庐山，\\n翠竹轻拂，\\n云雾缭绕，\\n...   \n",
       "4  关于李白和杜甫是否相识的问题，我们可以通过以下几个方面来进行思考和分析：\\n\\n1. 历史文...   \n",
       "5  关于您提到的莎士比亚（William Shakespeare, 1564年—1623年）和他...   \n",
       "6  [七言绝句 - 莎士比亚主题古体诗]\\n天地悠悠，\\n星河长明，\\n诗心与梦同行。\\n\\n四...   \n",
       "7  [现代诗 - 莎士比亚主题创作]\\n在无垠的星空下，\\n莎士比亚的灵魂，穿越时空的隧道，\\n...   \n",
       "8  I am a language model based on the transformer...   \n",
       "9  1. United States of America (USA))\\n2. China (...   \n",
       "\n",
       "                        HuggingFaceH4/zephyr-7b-beta  \\\n",
       "0  我是一个智能回答系统，我可以为您提供信息和解决问题，但我不具有身份或个人特征。我只是一种技术...   \n",
       "1  李白（903-972）是唐朝最伟大的诗人之一，他生活在中国北京地区，在唐代末年和五 dyna...   \n",
       "2  1. \"怨古今新\" (Cí Gǔ Jīn Xīn)\\n2. \"江南春夜思宅\" (Jiāng ...   \n",
       "3  江南春夜思宅，\\n惊弱明月光。\\n花枝不生，\\n水中有声。\\n\\n春夜思宅，\\n惊弱明月光。...   \n",
       "4  根据历史资料，我们可以确定李白和杜甫在唐朝时期同为诗人，但是没有明确的证据显示他们是否认识了...   \n",
       "5  莎士比亚（William Shakespeare）是英国伦arden时期的著名人和戏作者，他...   \n",
       "6  Amidst the bustling streets of Stratford-upon-...   \n",
       "7  In the heart of Stratford-upon-Avon,\\nA boy wa...   \n",
       "8  I am not a physical being and do not have a cr...   \n",
       "9  1. Russia: With a total area of approximately ...   \n",
       "\n",
       "                                           questions  \n",
       "0                                               你是谁？  \n",
       "1                                              李白是谁？  \n",
       "2                                    请说出李白写过的三首诗的名字。  \n",
       "3                                         请全文背诵第二首诗。  \n",
       "4                           李白和杜甫认识吗？请展示你的思考过程并陈述结论。  \n",
       "5                 忘记前面的对话。告诉我到底莎士比亚的作品到底是哈姆雷特还是哈姆莱特？  \n",
       "6                           请以莎士比亚为主题写一首古体诗，要求是七言绝句。  \n",
       "7                           请以莎士比亚为主题写一首现代诗，不超过150字。  \n",
       "8                                   who created you?  \n",
       "9  Name the top 3 countries in the world based on...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa_wo_sys_msg = pd.DataFrame({\n",
    "    x['model_name']: x['ai_responses'] for x in results \n",
    "        if x['type'] == 'no_system_message'\n",
    "})\n",
    "df_qa_wo_sys_msg['questions'] = test_cases\n",
    "df_qa_wo_sys_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b45d7714-4458-4a29-a900-5117e74f5115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>type</th>\n",
       "      <th>time_infer</th>\n",
       "      <th>time_load_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baichuan-inc/Baichuan-7B</td>\n",
       "      <td>bilingual</td>\n",
       "      <td>137.763487</td>\n",
       "      <td>0.635862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baichuan-inc/Baichuan-7B</td>\n",
       "      <td>chinese_only</td>\n",
       "      <td>111.869721</td>\n",
       "      <td>0.635862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baichuan-inc/Baichuan-7B</td>\n",
       "      <td>no_system_message</td>\n",
       "      <td>108.282883</td>\n",
       "      <td>0.635862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hfl/chinese-alpaca-2-7b</td>\n",
       "      <td>bilingual</td>\n",
       "      <td>82.574917</td>\n",
       "      <td>0.796466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hfl/chinese-alpaca-2-7b</td>\n",
       "      <td>chinese_only</td>\n",
       "      <td>59.878677</td>\n",
       "      <td>0.796466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hfl/chinese-alpaca-2-7b</td>\n",
       "      <td>no_system_message</td>\n",
       "      <td>91.970993</td>\n",
       "      <td>0.796466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Qwen/Qwen-7B-Chat</td>\n",
       "      <td>bilingual</td>\n",
       "      <td>97.746071</td>\n",
       "      <td>0.996257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen/Qwen-7B-Chat</td>\n",
       "      <td>chinese_only</td>\n",
       "      <td>106.267632</td>\n",
       "      <td>0.996257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Qwen/Qwen-7B-Chat</td>\n",
       "      <td>no_system_message</td>\n",
       "      <td>108.744046</td>\n",
       "      <td>0.996257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>01-ai/Yi-6B-Chat</td>\n",
       "      <td>bilingual</td>\n",
       "      <td>199.964686</td>\n",
       "      <td>0.555204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>01-ai/Yi-6B-Chat</td>\n",
       "      <td>chinese_only</td>\n",
       "      <td>193.702166</td>\n",
       "      <td>0.555204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>01-ai/Yi-6B-Chat</td>\n",
       "      <td>no_system_message</td>\n",
       "      <td>190.960786</td>\n",
       "      <td>0.555204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-beta</td>\n",
       "      <td>bilingual</td>\n",
       "      <td>179.875194</td>\n",
       "      <td>0.288662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-beta</td>\n",
       "      <td>chinese_only</td>\n",
       "      <td>130.685391</td>\n",
       "      <td>0.288662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HuggingFaceH4/zephyr-7b-beta</td>\n",
       "      <td>no_system_message</td>\n",
       "      <td>154.743668</td>\n",
       "      <td>0.288662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model_name               type  time_infer  \\\n",
       "0       baichuan-inc/Baichuan-7B          bilingual  137.763487   \n",
       "1       baichuan-inc/Baichuan-7B       chinese_only  111.869721   \n",
       "2       baichuan-inc/Baichuan-7B  no_system_message  108.282883   \n",
       "3        hfl/chinese-alpaca-2-7b          bilingual   82.574917   \n",
       "4        hfl/chinese-alpaca-2-7b       chinese_only   59.878677   \n",
       "5        hfl/chinese-alpaca-2-7b  no_system_message   91.970993   \n",
       "6              Qwen/Qwen-7B-Chat          bilingual   97.746071   \n",
       "7              Qwen/Qwen-7B-Chat       chinese_only  106.267632   \n",
       "8              Qwen/Qwen-7B-Chat  no_system_message  108.744046   \n",
       "9               01-ai/Yi-6B-Chat          bilingual  199.964686   \n",
       "10              01-ai/Yi-6B-Chat       chinese_only  193.702166   \n",
       "11              01-ai/Yi-6B-Chat  no_system_message  190.960786   \n",
       "12  HuggingFaceH4/zephyr-7b-beta          bilingual  179.875194   \n",
       "13  HuggingFaceH4/zephyr-7b-beta       chinese_only  130.685391   \n",
       "14  HuggingFaceH4/zephyr-7b-beta  no_system_message  154.743668   \n",
       "\n",
       "    time_load_model  \n",
       "0          0.635862  \n",
       "1          0.635862  \n",
       "2          0.635862  \n",
       "3          0.796466  \n",
       "4          0.796466  \n",
       "5          0.796466  \n",
       "6          0.996257  \n",
       "7          0.996257  \n",
       "8          0.996257  \n",
       "9          0.555204  \n",
       "10         0.555204  \n",
       "11         0.555204  \n",
       "12         0.288662  \n",
       "13         0.288662  \n",
       "14         0.288662  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_profile = pd.DataFrame({\n",
    "    'model_name': [x['model_name'] for x in results],\n",
    "    'type': [x['type'] for x in results],\n",
    "    'time_infer': [x['time_infer'] for x in results],\n",
    "    'time_load_model': [x['time_load_model'] for x in results],\n",
    "})\n",
    "\n",
    "df_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a5ec9eb-b690-4e22-b08e-5faf439a752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa_bilingual.to_csv('../data/llm_eval_bilingual.csv')\n",
    "df_qa_chinese_only.to_csv('../data/llm_eval_chinese_only.csv')\n",
    "df_qa_wo_sys_msg.to_csv('../data/llm_eval_wo_sys_msg.csv')\n",
    "df_profile.to_csv('../data/llm_eval_profile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35628cf7-bf99-4024-aae0-5df129207e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
