{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a17b2a-7d5d-4ad7-8f94-2551f861739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb44bba-3a01-433b-a95a-5d1b88d718f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be83402f-791b-4446-9742-ede58aa7d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_notion_companion.chatbot import ChatBot\n",
    "from my_notion_companion.self_query import SelfQueryAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a9661ad-e7c4-473d-b04d-ea3e033d7f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain_core/utils/utils.py:159: UserWarning: WARNING! conversation is not default parameter.\n",
      "                conversation was transferred to model_kwargs.\n",
      "                Please confirm that conversation is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 259 tensors from /Users/fred/Documents/models/Qwen-7B-Chat.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen\n",
      "llama_model_loader: - kv   1:                               general.name str              = Qwen\n",
      "llama_model_loader: - kv   2:                        qwen.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                           qwen.block_count u32              = 32\n",
      "llama_model_loader: - kv   4:                      qwen.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                   qwen.feed_forward_length u32              = 22016\n",
      "llama_model_loader: - kv   6:                        qwen.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv   7:                  qwen.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                  qwen.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:      qwen.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  11:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  12:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  14:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.eos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  16:            tokenizer.ggml.unknown_token_id u32              = 151643\n",
      "llama_model_loader: - kv  17:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - type  f32:   97 tensors\n",
      "llama_model_loader: - type q4_K:  113 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 293/151936 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 22016\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.72 B\n",
      "llm_load_print_meta: model size       = 4.56 GiB (5.07 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen\n",
      "llm_load_print_meta: BOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: EOS token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: UNK token        = 151643 '[PAD151643]'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_tensors: ggml ctx size =    0.20 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  4666.59 MiB, ( 4666.66 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 32/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4450.41 MiB\n",
      "llm_load_tensors:      Metal buffer size =  4666.59 MiB\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M2 Pro\n",
      "ggml_metal_init: picking default device: Apple M2 Pro\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M2 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple8  (1008)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  2048.00 MiB, ( 6716.22 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  2048.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    80.04 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1248.02 MiB, ( 7964.23 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =  1248.01 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  1251.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 4\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.file_type': '15', 'tokenizer.ggml.unknown_token_id': '151643', 'tokenizer.ggml.eos_token_id': '151643', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'qwen.attention.head_count': '32', 'qwen.rope.freq_base': '10000.000000', 'tokenizer.ggml.bos_token_id': '151643', 'qwen.feed_forward_length': '22016', 'qwen.attention.layer_norm_rms_epsilon': '0.000001', 'qwen.embedding_length': '4096', 'qwen.rope.dimension_count': '128', 'qwen.context_length': '32768', 'qwen.block_count': '32', 'general.name': 'Qwen', 'general.architecture': 'qwen'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import tomllib\n",
    "\n",
    "with open('../.config.toml', 'rb') as f:\n",
    "    _CONFIGS = tomllib.load(f)\n",
    "\n",
    "with open('../.tokens.toml', 'rb') as f:\n",
    "    _TOKENS = tomllib.load(f)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=_CONFIGS['model_path']+'/'+'Qwen-7B-Chat.Q4_K_M.gguf',\n",
    "    name='Qwen/Qwen-7B-Chat', \n",
    "    **_CONFIGS['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83519d7f-6fcf-458d-862d-47a6c30d57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ChatBot(llm, _CONFIGS)\n",
    "s = SelfQueryAgent(llm, _CONFIGS, _TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de6dd91e-68b4-477c-94eb-35ec13a051ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen-7B-Chat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.llm.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "344c3315-236f-424c-beab-caf7e0eb492d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =      10.65 ms /    29 runs   (    0.37 ms per token,  2723.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =     980.30 ms /    20 tokens (   49.02 ms per token,    20.40 tokens per second)\n",
      "llama_print_timings:        eval time =    1089.71 ms /    28 runs   (   38.92 ms per token,    25.69 tokens per second)\n",
      "llama_print_timings:       total time =    2272.25 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 277 ms, total: 1.85 s\n",
      "Wall time: 2.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'I am a large language model created by Alibaba Cloud. I am called QianWen. How can I help you today?\\n'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "c.invoke(\"who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1bf71f9-70eb-4137-acd0-14d02f653c4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =     103.03 ms /   256 runs   (    0.40 ms per token,  2484.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8859.76 ms /  1134 tokens (    7.81 ms per token,   127.99 tokens per second)\n",
      "llama_print_timings:        eval time =   13259.58 ms /   255 runs   (   52.00 ms per token,    19.23 tokens per second)\n",
      "llama_print_timings:       total time =   23870.95 ms /  1389 tokens\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "Metadata key date_end not found in metadata. Setting to None. \n",
      "Metadata fields defined for this instance: ['author', 'id', 'name', 'source', 'tags', 'date_start', 'date_end']\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =     112.10 ms /   256 runs   (    0.44 ms per token,  2283.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4924.61 ms /   871 tokens (    5.65 ms per token,   176.87 tokens per second)\n",
      "llama_print_timings:        eval time =   11363.46 ms /   255 runs   (   44.56 ms per token,    22.44 tokens per second)\n",
      "llama_print_timings:       total time =   18187.84 ms /  1126 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =     103.99 ms /   256 runs   (    0.41 ms per token,  2461.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3646.30 ms /   635 tokens (    5.74 ms per token,   174.15 tokens per second)\n",
      "llama_print_timings:        eval time =   11002.53 ms /   255 runs   (   43.15 ms per token,    23.18 tokens per second)\n",
      "llama_print_timings:       total time =   16339.15 ms /   890 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =     103.20 ms /   256 runs   (    0.40 ms per token,  2480.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3714.77 ms /   617 tokens (    6.02 ms per token,   166.09 tokens per second)\n",
      "llama_print_timings:        eval time =   11312.32 ms /   255 runs   (   44.36 ms per token,    22.54 tokens per second)\n",
      "llama_print_timings:       total time =   16829.55 ms /   872 tokens\n",
      "/Users/fred/micromamba/envs/my-notion-companion/lib/python3.12/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =      94.68 ms /   256 runs   (    0.37 ms per token,  2703.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =     310.88 ms /    29 tokens (   10.72 ms per token,    93.28 tokens per second)\n",
      "llama_print_timings:        eval time =   10199.52 ms /   255 runs   (   40.00 ms per token,    25.00 tokens per second)\n",
      "llama_print_timings:       total time =   12138.56 ms /   284 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.6 s, sys: 7.57 s, total: 1min 5s\n",
      "Wall time: 1min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='前言\\n本书分为四个部分。\\n第一部分题为《从伊甸园到卡哈马卡》，它由三章组成。\\n\\t第一章提供了一次惯于人类进化和历史的旋风式旅行，从大约700万年前我们刚从类人猿分化出来开始，一直延续到大约13,000年前上一次冰期结束为止。结果表明，某些大陆上 的人类发展经过一段时间后取得了对其他大陆上的人类的领先优势。\\n\\t第二章简明地考察了岛屿环境在较小的时空范围内对历史的影响。在大约3200年前波利尼西亚人祖先向太平洋迁徙的时候，他们碰到了一些和他们原来的环境大不相同的岛屿；于是它们在这个地区建立起了一系列形形色色的子社会，从狩猎采集部落到原始帝国。\\n\\t第三章讲述了历史上最具戏剧性的遭遇，即来自不同大陆的各民族之间的冲突。这些因素（病菌、马匹、文化、政治组织和技术）正是造成这种必然性的\\n\\t直接原因\\n\\t。\\n第二部分题为《粮食生产的出现和传播》，包括从第四章到第十章。这一部分专门讨论我认为是最重要的一组\\n终极原因\\n。\\n\\t第四至六章概述了粮食生产及其影响\\n\\t第七至九章说明驯化动植物和粮食传播\\n\\t第十章则用大陆轴线阐释传播速度的差异。\\n第三部分《从粮食生产到枪炮、病菌和钢铁》\\n\\t第十一章从密集人口所特有的病菌的演化开始，对终极原因到近似原因进行了考察\\n\\t第十二章阐述另一条因果链，即粮食和文字的传播\\n\\t第十三、四章则推广到技术、政治组织的层面。\\n第四部分《在5章中环游世界》把第二和第三部分所讲的内容应用于每个大陆和一些重要的岛屿。\\n\\t第十五章研究澳大利亚及其邻近的新几内亚这个大岛的历史\\n\\t第十六、七章将前述发展扩大并结合整个地区，包括东亚大陆和太平台诸岛\\n\\t第十八章则阐释欧洲民族和美洲印第安人之间的冲突\\n\\t最后，第十九章讲述非洲撒哈拉沙漠以南地区欧洲人与非洲人间的冲突，以及非洲内部班图人的扩张造成的大迁徙。\\n后记题为《人类史作为一门科学的未来》，列出了剩下来的几个问题，包括欧亚大陆不同地区之间的差异问题，与环境无关的文 化因素的作用，以及个人的作用。\\n我试图使读者相信，历史上的确存在着适用于解释其发展必然性的广泛模式。', metadata={'id': 'doc:notiondb:9a5eabb6d1d04e90979b0dbcacd20ffd', 'author': '贾雷德·戴蒙德', 'name': '枪炮、病菌和钢铁', 'source': '笔记（非文学）', 'tags': '历史, 人文', 'date_start': '20170113', 'date_end': None}),\n",
       " Document(page_content='【第一部】失落的信\\n在历史依然缓慢前行的时代，不多的事件很容易铭刻在记忆之中，编织成一个无人不晓的背景，其前台上演着令人牵肠挂肚的私人生活的诸多传奇。今天，时间在大步前进。历史事件一夜之间即被遗忘，晨光降临便如闪烁的朝露般飘逝，因此也就不再是叙述者故事中的背景，而是过于稀松平常的私人生活背景上演的一幕出人意外的传奇。\\n\\n\\n知识分子这个词，在当时的政治用语中，是一种辱骂。它指的是不懂得生活又与人民脱离的人。当时，所有被其他共产党人绞死的共产党人，都被赐予这一骂名。与所有那些脚踏实地的人们相反，据说，知识分子们是飘荡在空中的什么地方的。因此，从某种意义上讲，为了惩罚他们，大地从此彻底拒绝让他们落足，而他们就被吊在离地面稍高一点儿的地方了。\\n是的，不管人们怎么说，共产党人都是更聪明的。他们有一个宏伟蓝图，一个全新世界的蓝图，在那个世界里所有人各得其所。反对他们的人没有伟大的梦想，只有一些陈腐的令人生厌的道德准则，用来补缀既成秩序那破旧的短裤。因此，也就难怪那些热情澎湃的人、那些勇往直前的人，轻而易举地战胜那些不冷不热的人、那些畏首畏尾的人了；也就难怪这些人很快就开始把自己的梦想付诸实践，为所有人谱写正义的牧歌了。\\n我强调这两个词：牧歌和所有人，因为古往今来，人类都一直向往着牧歌，向往这个夜莺歌唱的田园，这个和谐的王国，在那一王国里，世界不是作为局外人反对人类，人类之间也不互相对立，而是相反，世界与所有人都糅合到唯一的、同一的物质里。在那里，每个人都是巴赫壮丽的赋格曲中的一个音符，凡不愿做其中一个音符的人则成为一个无用、毫无意义的黑点，只需抓在手里并用指甲碾死它，就像碾死一只跳蚤一样。\\n有些人很快明白他们并不具备牧歌所需要的性情，因而他们动了去国外的心思。然而，既然牧歌就其本质而言是所有人的世界，想要移居他乡的人显然就是在否定牧歌，结果他们国外没去成，而是去了监狱。\\n……（略）\\n就在这个时候，这些人中一些聪明又激进的青年，突然奇怪地感觉到，他们广阔天地里所展开的事业开始有了自己的生命，与他们的理想背道而驰，并且不再理会那些赋予其生命的人们。这些年轻而且聪明的人开始在他们的事业后面呐喊，他们开始呼唤它，责难它，追捕它，对它进行逐猎。如果我要就这一代聪明且激进的青年写一部小说的话，我会把这部小说定名为《逐猎失落的事业》。', metadata={'id': 'doc:notiondb:a38effb076ad45df94caef5a9e331c0b', 'author': '【捷】米兰·昆德拉', 'name': '笑忘录 【捷】米兰·昆德拉', 'source': '读书笔记（文学）', 'tags': '小说', 'date_start': '20140114', 'date_end': None}),\n",
       " Document(page_content='“那无疑就是一种成就吧。”\\n“那么是谁的呢？”\\n“是你的意志呗。”\\n“别开玩笑，那时候我早已归天，刚才我也说过了嘛。那是在和我毫无关系中发展起来的。”\\n“你不认为那是历史的意志的成就吗？”\\n“历史有意志吗？把历史拟人化总是危险的啊。依我的想法，历史是没有意志的，同我的意志也是毫无关系。因此，结果并非是从任何意志产生的。这种结果绝不能说是‘成就’。证据就是，历史的虚假成就，从转眼的瞬间就开始崩溃了。\\n“历史总是要崩溃的。同时它又在准备下一个徒有虚名的结晶。\\n历史的形成和崩溃，仿佛只具有同一的意义。\\n“这种事我很清楚。虽然很清楚，但我和你不同，我不能不做一个有意义的人。就说意志吧，它也许就是我的被强制的性格的一部分。这确实的事，对谁也不能说。但是似乎可以说，人的意志本质上是‘企图与历史发生关系的意志’。我并不是说它是‘与历史发生关系的意志’。意志与历史发生关系几乎是不可能的事，它只是‘企图发生关系’而已。这又是所有意志都具备的宿命。当然，意志是不愿意承认一切的宿命。\\n“以长远的目光来看，所有人的意志都将遭到挫折。不能如愿以偿，这是人之常情。这种时候，西方人是这样考虑的呢？他们认为‘我的意志就是意志，失败是偶然的’。所谓偶然就是排除所有因果规律、自由意志所能承认的唯一非合目的性。\\n“因此，\\n西方的意志哲学，不承认‘偶然’就不能成立。所谓偶然，就是意志最后逃遁的场所，是赌博的胜败\\n……没有这个，就无法说明西方人的意志一再受挫折和失败。我认为这种偶然、这种赌博，才是西方的神的本质。既然意志哲学最后逃遁的场所是作为偶然的神的话，那么就只有这样的神才能鼓舞人的意志。\\n“倘若这个偶然都全被否定了，将会怎么样呢？假若任何胜利和任何失败都被认为没有任何偶然的作用余地，又将会怎么样呢？这样一来，所有自由意志逃遁的场所就会丧失殆尽。没有偶然的存在，意志就将失去赖以支撑自己身体站立的支柱。”\\n“你试想想这种局面好啰。”\\n“那里是白昼的广场，意志独立站立，佯装着依靠自己力量站立，且其自身也有这样的错觉。在洒满阳光的没有花草树木的大广场上，它所拥有的只是它自己的影子。\\n“这时候，万里无云的晴空，不知从哪里传来了震耳的轰鸣。\\n“‘偶然死了，偶然这个玩意儿时没有的。意志啊！从此你就永远失去自我辩解了吧。’', metadata={'id': 'doc:notiondb:5e663f1f9fcf4ebfa536f31505cdc5e1', 'author': '【日】三岛由纪夫', 'name': '春雪 【日】三岛由纪夫', 'source': '读书笔记（文学）', 'tags': '小说', 'date_start': '20140304', 'date_end': None})]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "s.invoke(\"'什么是我国第一部编年史著作？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e2e97f-e8a8-40e4-a25f-48597f122117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['什么是我国第一部编年史著作？', '什么是我国第一部编年国别史？', '“寡人之于国也”下一句是什么？来自哪里？']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open('../data/test_cases.txt') as f:\n",
    "    test_cases_raw = f.readlines()\n",
    "\n",
    "test_cases_raw = \"\".join(test_cases_raw[0:-1:2]).split('问：')[1:]\n",
    "test_cases_raw = [re.split(r'\\n答：|\\n资料：', x) for x in test_cases_raw]\n",
    "test_cases = [{'q': x[0], 'a': x[1], 'docs': x[2:]} for x in test_cases_raw]\n",
    "\n",
    "retrival_sample_questions = [x['q'] for x in test_cases]\n",
    "retrival_sample_questions[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0707d515-c148-44f8-9ec7-6432a37ee276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 ms, sys: 3.98 ms, total: 30.6 ms\n",
      "Wall time: 730 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "retrival_sample_embeddings = s.embedding_model.embed_documents(retrival_sample_questions, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e73ada86-6a36-4f2f-9498-fcd69422f935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27933269930385096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.utils.math import cosine_similarity\n",
    "\n",
    "query_embedding = s.embedding_model.embed_query(\"who are you?\")\n",
    "cosine_similarity([query_embedding], retrival_sample_embeddings)[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9129b7a-74e3-4407-893c-ae097de422b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.5\n",
    "\n",
    "def route(input):\n",
    "\n",
    "    query_embedding = s.embedding_model.embed_query(input)\n",
    "    similarity = cosine_similarity([query_embedding], retrival_sample_embeddings)[0].max()\n",
    "\n",
    "    if similarity >= THRESHOLD:\n",
    "        chain = s\n",
    "    else:\n",
    "        chain = c\n",
    "\n",
    "    return chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8a1764ad-af4a-42b1-b7bf-f45ca43626a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1924.62 ms\n",
      "llama_print_timings:      sample time =      86.26 ms /   256 runs   (    0.34 ms per token,  2967.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2372.19 ms /    34 tokens (   69.77 ms per token,    14.33 tokens per second)\n",
      "llama_print_timings:        eval time =   10591.00 ms /   255 runs   (   41.53 ms per token,    24.08 tokens per second)\n",
      "llama_print_timings:       total time =   14632.52 ms /   289 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': 'I am QianWen, a large language model created by Alibaba Cloud. How can I assist you today?'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"who are you?\"\n",
    "\n",
    "route(question).invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "138e3019-0a87-4ea2-9361-787b68544152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am QianWen, a large language model created by Alibaba Cloud. How can I assist you today?'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = \"I am QianWen, a large language model created by Alibaba Cloud. How can I assist you today?[PAD151645]\\n[PAD151644]'t be able to[PAD151645]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n[PAD151644]\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdc333fc-780c-463c-8960-faefbc9087f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<my_notion_companion.self_query.SelfQueryAgent at 0x175f68710>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "question = '什么是我国第一部编年史著作？'\n",
    "\n",
    "route(question).invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e7e5cd-b4a6-442e-b732-bca9f6761dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
